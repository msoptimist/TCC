<!DOCTYPE html>
<html class="js" lang="en-gb"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
<meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=2.5,user-scalable=yes">
    <meta name="citation_publisher" content="Springer Vienna">
    <meta name="citation_title" content="A medical image retrieval scheme with relevance feedback through a medical social network">
    <meta name="citation_doi" content="10.1007/s13278-016-0362-9">
    <meta name="citation_language" content="en">
    <meta name="citation_abstract_html_url" content="https://link.springer.com/article/10.1007/s13278-016-0362-9">
    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s13278-016-0362-9">
    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007%2Fs13278-016-0362-9.pdf">
    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s13278-016-0362-9&amp;api_key=">
    <meta name="citation_firstpage" content="53">
    <meta name="citation_author" content="Mouhamed Gaith Ayadi">
    <meta name="citation_author_institution" content="ISG, BESTMOD">
    <meta name="citation_author_email" content="mouhamed.gaith.ayadi@gmail.com">
    <meta name="citation_author" content="Riadh Bouslimi">
    <meta name="citation_author_institution" content="ISG, BESTMOD">
    <meta name="citation_author" content="Jalel Akaichi">
    <meta name="citation_author_institution" content="ISG, BESTMOD">
    <meta name="dc.identifier" content="10.1007/s13278-016-0362-9">
    <meta name="format-detection" content="telephone=no">
    <meta name="description" content="Medical social networking sites enabled multimedia content sharing in large volumes, by allowing physicians and patients to upload their medical images. Moreover, it is necessary to employ new...">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="A medical image retrieval scheme with relevance feedback through a med">
    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/13278/6/1.jpg">
    <meta name="twitter:image:alt" content="Content cover image">
    <meta name="twitter:site" content="SpringerLink">
    <meta name="twitter:description" content="Medical social networking sites enabled multimedia content sharing in large volumes, by allowing physicians and patients to upload their medical images. Moreover, it is necessary to employ new...">
    <meta name="citation_journal_title" content="Social Network Analysis and Mining">
    <meta name="citation_journal_abbrev" content="Soc. Netw. Anal. Min.">
    <meta name="citation_volume" content="6">
    <meta name="citation_issue" content="1">
    <meta name="citation_issn" content="1869-5450">
    <meta name="citation_issn" content="1869-5469">
    <meta name="citation_online_date" content="2016/07/29">
    <meta name="citation_cover_date" content="2016/12/01">
    <meta name="citation_article_type" content="Original Article">
    <meta property="og:title" content="A medical image retrieval scheme with relevance feedback through a medical social network">
    <meta property="og:type" content="Article">
    <meta property="og:url" content="https://link.springer.com/article/10.1007/s13278-016-0362-9">
    <meta property="og:image" content="https://static-content.springer.com/cover/journal/13278/6/1.jpg">
    <meta property="og:site_name" content="SpringerLink">
    <meta property="og:description" content="Medical social networking sites enabled multimedia content sharing in large volumes, by allowing physicians and patients to upload their medical images. Moreover, it is necessary to employ new...">

        <title>A medical image retrieval scheme with relevance feedback through a medical social network | SpringerLink</title>
        <link rel="canonical" href="https://link.springer.com/article/10.1007/s13278-016-0362-9">
        <link rel="shortcut icon" href="https://link.springer.com/springerlink-static/481091012/images/favicon/favicon.ico">
<link rel="icon" sizes="16x16 32x32 48x48" href="https://link.springer.com/springerlink-static/481091012/images/favicon/favicon.ico">
<link rel="icon" sizes="16x16" type="image/png" href="https://link.springer.com/springerlink-static/481091012/images/favicon/favicon-16x16.png">
<link rel="icon" sizes="32x32" type="image/png" href="https://link.springer.com/springerlink-static/481091012/images/favicon/favicon-32x32.png">
<link rel="icon" sizes="48x48" type="image/png" href="https://link.springer.com/springerlink-static/481091012/images/favicon/favicon-48x48.png">
<link rel="apple-touch-icon" href="https://link.springer.com/springerlink-static/481091012/images/favicon/app-icon-iphone@3x.png">
<link rel="apple-touch-icon" sizes="72x72" href="https://link.springer.com/springerlink-static/481091012/images/favicon/ic_launcher_hdpi.png">
<link rel="apple-touch-icon" sizes="76x76" href="https://link.springer.com/springerlink-static/481091012/images/favicon/app-icon-ipad.png">
<link rel="apple-touch-icon" sizes="114x114" href="https://link.springer.com/springerlink-static/481091012/images/favicon/app-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="https://link.springer.com/springerlink-static/481091012/images/favicon/app-icon-iphone@2x.png">
<link rel="apple-touch-icon" sizes="144x144" href="https://link.springer.com/springerlink-static/481091012/images/favicon/ic_launcher_xxhdpi.png">
<link rel="apple-touch-icon" sizes="152x152" href="https://link.springer.com/springerlink-static/481091012/images/favicon/app-icon-ipad@2x.png">
<link rel="apple-touch-icon" sizes="180x180" href="https://link.springer.com/springerlink-static/481091012/images/favicon/app-icon-iphone@3x.png">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/springerlink-static/481091012/images/favicon/ic_launcher_xxhdpi.png">
        <link rel="dns-prefetch" href="https://fonts.gstatic.com/">
<link rel="dns-prefetch" href="https://fonts.googleapis.com/">
<link rel="dns-prefetch" href="https://google-analytics.com/">
<link rel="dns-prefetch" href="https://www.google-analytics.com/">
<link rel="dns-prefetch" href="https://www.googletagservices.com/">
<link rel="dns-prefetch" href="https://www.googletagmanager.com/">
<link rel="dns-prefetch" href="https://static-content.springer.com/">
        <link rel="stylesheet" href="basic.css" media="screen">
<link rel="stylesheet" href="styles.css" class="js-ctm" media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
<link rel="stylesheet" href="print.css" media="print">


            <script type="text/javascript" src="https://beacon.krxd.net/optout_check?request_id=62589734-7933-48e5-8cb4-34e0ac93178a&amp;callback=Krux.ns._default.kxjsonp_optOutCheck"></script><script type="text/javascript" src="https://consumer.krxd.net/consent/set/bd339c69-af54-4a21-b4f1-654bcfcd83ca?request_id=96ef851e-6461-4912-9930-648d4b4f3e36&amp;idt=device&amp;dt=kxcookie&amp;dc=1&amp;al=1&amp;tg=1&amp;cd=0&amp;sh=0&amp;re=0&amp;callback=Krux.ns._default.kxjsonp_consent_set_1"></script><script type="text/javascript" src="https://consumer.krxd.net/consent/get/bd339c69-af54-4a21-b4f1-654bcfcd83ca?idt=device&amp;dt=kxcookie&amp;callback=Krux.ns._default.kxjsonp_consent_get_0"></script><script type="text/javascript" async="" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async="" src="//cdn.krxd.net/ctjs/controltag.js.8f9c5605187855d5a137991abae6f700"></script><script type="text/javascript" async="" src="KDqyaFZ_.js"></script><script type="text/javascript" async="" src="analytics.js"></script><script async="" src="mathJax.js"></script><script src="pubads_impl_rendering_216.js"></script><script async="" src="gtm.js"></script><script>
        window.Krux||((Krux=function(){Krux.q.push(arguments);}).q=[]);
        var dataLayer = [{
                'GA Key':'UA-26408784-1',
                'Features':["leaderboardadverts","abtesting","reprintsandpermissionsabtest"],
                'Event Category':'Article',
                'Open Access':'N',
                'Labs':'Y',
                'DOI':'10.1007\/s13278-016-0362-9',
                'VG Wort Identifier':'pw-vgzm.415900-10.1007-s13278-016-0362-9',
                'HasAccess':'Y',
                'Full HTML':'Y',
                'Has Body':'Y',
                'Static Hash':'481091012',
                'Has Preview':'N',
                'user':{'license': {'businessPartnerID': ['3000197460', '3000201946'], 'businessPartnerIDString': '3000197460|3000201946'}},
                'content':{'type': 'article', 'serial': {'eissn': '1869-5469', 'pissn': '1869-5450'}, 'category': {'pmc': {'primarySubject': 'Computer Science', 'primarySubjectCode': 'I', 'secondarySubjects': {'4': 'Statistics for Social Science, Behavorial Science, Education, Public Policy, and Law', '5': 'Methodology of the Social Sciences', '1': 'Data Mining and Knowledge Discovery', '2': 'Applications of Graph Theory and Complex Networks', '3': 'Game Theory, Economics, Social and Behav. Sciences'}, 'secondarySubjectCodes': {'4': 'S16003', '5': 'X17000', '1': 'I18030', '2': 'P33010', '3': 'M13011'}}, 'sucode': 'Computer Science'}},
                'Access Type':'subscription',
                'Page':'article',
                'Bpids':'3000197460, 3000201946',
                'Bpnames':'CAPES MEC, Universidade Tecnologica Federal do Parana',
                'SubjectCodes':'SCI, SCI18030, SCP33010, SCM13011, SCS16003, SCX17000',
                'session':{'authentication': {'loginStatus': 'N'}, 'attributes': {'edition': 'academic'}},
                'Keywords':'Medical social network, Content-based medical image retrieval (CBMIR), Feature extraction, Relevance feedback',
                'Country':'BR',
                'Journal Id':'13278',
                'Journal Title':'Social Network Analysis and Mining',

                'doi': "10.1007-s13278-016-0362-9",
                'kwrd': ["Medical_social_network","Content-based_medical_image_retrieval_(CBMIR)","Feature_extraction","Relevance_feedback"],
                'pmc': ["I","I18030","P33010","M13011","S16003","X17000"],
                'BPID': ["3000197460","3000201946"],
                'ksg': Krux.segments,
                'kuid': Krux.uid,

        }];
    </script>

    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
            'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-WCF9Z9');</script>

    <style>@font-face{font-family:'Source Sans Pro';font-weight:400;src:url(data:application/font-woff;base64,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) format('woff')}@font-face{font-family:'Source Sans Pro';font-weight:600;src:url(data:application/font-woff;base64,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) format('woff')}</style><link type="text/css" href="optanon.css" rel="stylesheet"><style>#optanon ul#optanon-menu li { background-color: #F7FBFE !important }#optanon ul#optanon-menu li.menu-item-selected { background-color: #FFFFFF !important }#optanon #optanon-popup-wrapper .optanon-white-button-middle { background-color: #3365A4 !important }.optanon-alert-box-wrapper .optanon-alert-box-button-middle { background-color: #3365A4 !important; border-color: #3365A4 !important; }#optanon #optanon-popup-wrapper .optanon-white-button-middle a { color: #ffffff !important }.optanon-alert-box-wrapper .optanon-alert-box-button-middle a { color: #ffffff !important }#optanon #optanon-popup-bottom { background-color: #F7FBFE !important }#optanon.modern #optanon-popup-top, #optanon.modern #optanon-popup-body-left-shading { background-color: #F7FBFE !important }.optanon-alert-box-wrapper { background-color:#F7FBFE !important }.optanon-alert-box-wrapper .optanon-alert-box-bg p { color:#333333 !important }</style><link rel="preload" href="integrator_002.js"><script type="text/javascript" src="integrator_002.js"></script><link rel="preload" href="integrator.js"><script type="text/javascript" src="integrator.js"></script><script src="pubads_impl_216.js" async=""></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/config/TeX-AMS-MML_HTMLorMML.js?V=2.7.4"></script></head>
    <body><button href="javascript:;" title="focus catcher" class="js-focus-catcher u-screenreader-only" tabindex="-1"></button><div id="popup-references" class="u-composite-layer popup-base-theme" style="top:-60px;left:-60px;visibility:hidden;opacity:0;" aria-hidden="true" aria-label="popup"><svg class="popup-arrow" xmlns="http://www.w3.org/2000/svg" width="33.7" height="18.4" viewBox="0 0 33.7 18.4"><path class="fill" fill="#F7FBFE" d="M1.4 18.4h30.9l-15.5-16.9z"></path><path class="stroke" fill="#98BED7" d="M0 18.4h1.4l15.4-16.9 15.5 16.9h1.4l-16.9-18.4z"></path></svg><div class="popup-arrow popup-arrow-shadow icon--popup-arrow-shadow"></div><div class="popup__references"><div class="popup-base-theme__inner" data-component="SpringerLink-Popup-inner"></div></div><button tabindex="-1" class="popup-close icon--close-btn" data-component="SpringerLink-Popup-close">close</button></div><div id="popup-article-dates" class="u-composite-layer popup-base-theme" style="top:-60px;left:-60px;visibility:hidden;opacity:0;" aria-hidden="true" aria-label="popup"><svg class="popup-arrow" xmlns="http://www.w3.org/2000/svg" width="33.7" height="18.4" viewBox="0 0 33.7 18.4"><path class="fill" fill="#F7FBFE" d="M1.4 18.4h30.9l-15.5-16.9z"></path><path class="stroke" fill="#98BED7" d="M0 18.4h1.4l15.4-16.9 15.5 16.9h1.4l-16.9-18.4z"></path></svg><div class="popup-arrow popup-arrow-shadow icon--popup-arrow-shadow"></div><div class="popup__article-dates"><div class="popup-base-theme__inner" data-component="SpringerLink-Popup-inner"></div></div><button tabindex="-1" class="popup-close icon--close-btn" data-component="SpringerLink-Popup-close">close</button></div><div id="popup-search" class="u-composite-layer popup-search-theme" style="top:-60px;left:-60px;visibility:hidden;opacity:0;" aria-hidden="true" aria-label="popup"><svg class="popup-arrow" xmlns="http://www.w3.org/2000/svg" width="33.7" height="18.4" viewBox="0 0 33.7 18.4"><path class="fill" fill="#F7FBFE" d="M1.4 18.4h30.9l-15.5-16.9z"></path><path class="stroke" fill="#98BED7" d="M0 18.4h1.4l15.4-16.9 15.5 16.9h1.4l-16.9-18.4z"></path></svg><div class="popup-arrow popup-arrow-shadow icon--popup-arrow-shadow"></div><div class="popup__search"><div class="popup-base-theme__inner" data-component="SpringerLink-Popup-inner"></div></div><button tabindex="-1" class="popup-close icon--close-btn" data-component="SpringerLink-Popup-close">close</button></div>
        <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
                      height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

    <nav class="skip-to">
    <a class="skip-to__link skip-to__link--article" href="#main-content">Skip to main content</a>
        <a class="skip-to__link skip-to__link--contents" href="#article-contents">Skip to sections</a>
</nav>
        <div class="page-wrapper">
            <noscript>
    <div class="nojs-banner u-interface">
        <p>This service is more advanced with JavaScript available, learn more at <a
                href="http://activatejavascript.org" target="_blank" rel="noopener">http://activatejavascript.org</a>
        </p>
    </div>
</noscript>
                    <div id="leaderboard" class="leaderboard u-hide" data-component="SpringerLink.GoogleAds" data-namespace="leaderboard"><div class="leaderboard__wrapper"><p class="leaderboard__label">Advertisement</p><button class="leaderboard__hide" title="Hide this advertisement">Hide</button><div id="doubleclick-leaderboard-ad" class="leaderboard__ad"><div id="google_ads_iframe_270604982/springerlink/13278/article_0__container__" style="border: 0pt none;"><iframe id="google_ads_iframe_270604982/springerlink/13278/article_0" title="3rd party ad content" name="google_ads_iframe_270604982/springerlink/13278/article_0" scrolling="no" marginwidth="0" marginheight="0" style="border: 0px none; vertical-align: bottom;" srcdoc="" width="728" frameborder="0" height="90"></iframe></div></div></div></div>

                <header id="header" class="header u-interface">
        <div class="header__content">
            <div class="header__menu-container">
                    <a id="logo" class="site-logo" href="https://link.springer.com/" title="Go to homepage">
                <div class="u-screenreader-only">SpringerLink</div>
    <svg class="site-logo__springer" width="148" height="30" role="img" focusable="false" aria-hidden="true">
        <image width="148" height="30" alt="" src="/springerlink-static/481091012/images/png/springerlink.png" xlink="http://www.w3.org/1999/xlink" xlink:href="/springerlink-static/481091012/images/svg/springerlink.svg"></image>
    </svg>

    </a>


                    <nav id="search-container" class="u-inline-block">
                        <div class="search">
                            <div class="search__content">
                                <form class="u-form-single-input" action="/search" method="get" role="search">
    <input aria-label="Search SpringerLink" name="query" autocomplete="off" placeholder="Search SpringerLink" type="text">
    <input class="u-hide-text" value="Submit" title="Submit" type="submit">
    <svg class="u-vertical-align-absolute" width="13" height="13" viewBox="222 151 13 13" version="1.1" xmlns="http://www.w3.org/2000/svg" focusable="false" aria-hidden="true" role="presentation">
        <path d="M227 159C228.7 159 230 157.7 230 156 230 154.3 228.7 153 227 153 225.3 153 224 154.3 224 156 224 157.7 225.3 159 227 159L227 159 227 159 227 159ZM230 160.1L231.1 159 233.9 161.7C234.2 162.1 234.2 162.6 233.9 162.9 233.6 163.2 233.1 163.2 232.7 162.9L230 160.1 230 160.1 230 160.1 230 160.1ZM227 161L227 161C224.2 161 222 158.8 222 156 222 153.2 224.2 151 227 151 229.8 151 232 153.2 232 156 232 158.8 229.8 161 227 161L227 161 227 161 227 161 227 161Z" stroke="none" fill-rule="evenodd"></path>
    </svg>
</form>
                            </div>
                        </div>
                    </nav>

                    <nav class="nav-container u-interface">
    <div class="global-nav__wrapper">
        <div class="search-button">
            <a class="search-button__label" href="#search-container" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-search">
                <span class="search-button__title">Search</span><svg width="12" height="12" viewBox="222 151 12 12" version="1.1" xmlns="http://www.w3.org/2000/svg" focusable="false" aria-hidden="true" role="presentation">
                    <path d="M227 159C228.7 159 230 157.7 230 156 230 154.3 228.7 153 227 153 225.3 153 224 154.3 224 156 224 157.7 225.3 159 227 159L227 159 227 159 227 159ZM230 160.1L231.1 159 233.9 161.7C234.2 162.1 234.2 162.6 233.9 162.9 233.6 163.2 233.1 163.2 232.7 162.9L230 160.1 230 160.1 230 160.1 230 160.1ZM227 161L227 161C224.2 161 222 158.8 222 156 222 153.2 224.2 151 227 151 229.8 151 232 153.2 232 156 232 158.8 229.8 161 227 161L227 161 227 161 227 161 227 161Z" stroke="none" fill-rule="evenodd"></path>
                </svg>
            </a>
        </div>

        <ul class="global-nav" data-component="SpringerLink.Menu" data-title="Navigation menu" data-text="Menu">
            <li>
                <a href="https://link.springer.com/">
                    <span class="u-overflow-ellipsis">Home</span>
                </a>
            </li>
            <li>
                <a href="https://link.springer.com/contactus">
                    <span class="u-overflow-ellipsis">Contact us</span>
                </a>
            </li>

                <li class="global-nav__logged-out">
                    <a class="test-login-link" href="https://link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs13278-016-0362-9">
                        <span class="u-overflow-ellipsis">Log in</span>
                    </a>
                </li>

        </ul><div class="main-menu u-composite-layer c-button-dropdown c-button-dropdown--ghost" data-component="SV.Dropdown" data-namespace="Menu" aria-label="button with dropdown options" style="width: 75.3333px;"><button type="button" title="Navigation menu" class="c-button-dropdown__button" data-role="button-dropdown__control" aria-pressed="false" aria-expanded="false" aria-controls="Dropdown.Menu-dropdown"><span class="u-overflow-ellipsis c-button-dropdown__button-title">Menu</span><span class="c-button-dropdown__icon"></span></button><div class="u-composite-layer c-button-dropdown__container" aria-hidden="true" aria-label="dropdown" id="Dropdown.Menu-dropdown"><ul class="main-menu__content" data-role="button-dropdown__content"><li>
                <a href="https://link.springer.com/" tabindex="-1">
                    <span class="u-overflow-ellipsis">Home</span>
                </a>
            </li><li>
                <a href="https://link.springer.com/contactus" tabindex="-1">
                    <span class="u-overflow-ellipsis">Contact us</span>
                </a>
            </li><li class="global-nav__logged-out">
                    <a class="test-login-link" href="https://link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs13278-016-0362-9" tabindex="-1">
                        <span class="u-overflow-ellipsis">Log in</span>
                    </a>
                </li></ul></div></div>
    </div> 
</nav> 
            </div>

        </div>
    </header>

            

            <main id="main-content" class="main-wrapper" tabindex="-1">
                <div class="main-container uptodate-recommendations-off">
                    <aside class="main-sidebar-left">
                        <div class="main-sidebar-left__content">
                            <div class="test-cover cover-image" itemscope="">
        <a class="test-cover-link" href="https://link.springer.com/journal/13278" title="Social Network Analysis and Mining">
            <img class="test-cover-image" src="1.jpg" alt="Social Network Analysis and Mining" itemprop="image">
        </a>

</div>
                        </div>
                    </aside>
                    <div class="main-body" data-role="NavigationContainer">
                                <div class="cta-button-container cta-button-container--top cta-button-container--stacked u-mb-16 u-hide-two-col">
                    <a href="https://link.springer.com/content/pdf/10.1007%2Fs13278-016-0362-9.pdf" target="_blank" class="c-button c-button--blue c-button__icon-right gtm-pdf-link" title="Download this article in PDF format" rel="noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" version="1.1"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill="#fff"><g transform="translate(12.000000, 5.000000)"><path d="M7 7.3L7 1C7 0.4 6.6 0 6 0 5.4 0 5 0.4 5 1L5 7.3 3.5 5.7C3.1 5.3 2.5 5.3 2.1 5.7L2.1 5.7C1.7 6.1 1.7 6.7 2.1 7.1L5.3 10.3C5.7 10.7 6.3 10.7 6.7 10.3L9.9 7.1C10.3 6.7 10.3 6.1 9.9 5.7L9.9 5.7C9.5 5.3 8.9 5.3 8.5 5.7L7 7.3 7 7.3ZM0 13C0 12.4 0.5 12 1 12L11 12C11.6 12 12 12.4 12 13 12 13.6 11.5 14 11 14L1 14C0.4 14 0 13.6 0 13L0 13Z"></path></g></g></g></svg>
            <span class="hide-text-small">Download</span>
            <span>PDF</span>
        </a>

        </div>



                        <article class="main-body__content">
                            <div xmlns="http://www.w3.org/1999/xhtml" class="FulltextWrapper"><div class="ArticleHeader main-context"><div id="enumeration" class="enumeration"><p><a href="https://link.springer.com/journal/13278" title="Social Network Analysis and Mining"><span class="JournalTitle">Social Network Analysis and Mining</span></a></p><p class="icon--meta-keyline-before"><span class="ArticleCitation_Year"><time datetime="2016-12">December 2016</time>, </span><span class="ArticleCitation_Volume">6:53</span><span class="u-inline-block u-ml-4"> | <a class="gtm-cite-link" href="#citeas">Cite as</a></span></p></div><div class="MainTitleSection"><h1 class="ArticleTitle" lang="en">A medical image retrieval scheme with relevance feedback through a medical social network</h1></div><div class="authors u-clearfix authors--enhanced" data-component="SpringerLink.Authors"><ul class="u-interface u-inline-list authors__title" data-role="AuthorsNavigation"><li><a href="#authors" class="gtm-tab-authors selected">Authors</a></li><li><a href="#authorsandaffiliations" class="gtm-tab-authorsandaffiliations">Authors and affiliations</a></li></ul><span class="marker" style="width: 160px;"></span><div class="authors__list" data-role="AuthorsList" id="authors"><ul class="test-contributor-names"><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Mouhamed&nbsp;Gaith&nbsp;Ayadi</span><span class="author-information"><span class="authors__contact"><a href="mailto:mouhamed.gaith.ayadi@gmail.com" title="mouhamed.gaith.ayadi@gmail.com" itemprop="email" class="gtm-email-author">Email author</a></span></span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Riadh&nbsp;Bouslimi</span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">Jalel&nbsp;Akaichi</span></li></ul></div><div class="authors__affiliations" id="authorsandaffiliations"><div class="authors-affiliations u-interface"><ul class="test-contributor-names"><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Mouhamed&nbsp;Gaith&nbsp;Ayadi</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul><span class="author-information"><span class="author-information__contact u-icon-before icon--email-before"><a href="mailto:mouhamed.gaith.ayadi@gmail.com" title="mouhamed.gaith.ayadi@gmail.com" itemprop="email" class="gtm-email-author">Email author</a></span></span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Riadh&nbsp;Bouslimi</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">Jalel&nbsp;Akaichi</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul></li></ul><ol class="test-affiliations"><li class="affiliation" data-test="affiliation-1" data-affiliation-highlight="affiliation-1" itemscope="" itemtype="http://schema.org/Organization"><span class="affiliation__count">1.</span><span class="affiliation__item"><span itemprop="department" class="affiliation__department">Department of Computer Sciences</span><span itemprop="name" class="affiliation__name">ISG, BESTMOD</span><span itemprop="address" itemscope="" itemtype="http://schema.org/PostalAddress" class="affiliation__address"><span itemprop="addressRegion" class="affiliation__city">Tunis</span><span itemprop="addressCountry" class="affiliation__country">Tunisia</span></span></span></li></ol></div></div></div><div class="main-context__container" data-component="SpringerLink.ArticleMetrics"><div class="main-context__column"><span><span class="test-render-category">Original Article</span></span><div class="article-dates article-dates--enhanced" data-component="SpringerLink.ArticleDates"><div class="article-dates__entry"><span class="article-dates__label">First Online: </span><span class="article-dates__first-online"><a href="#article-dates-history" class="gtm-first-online" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-article-dates">29 July 2016</a></span></div><div class="article-dates__history" id="article-dates-history"><div class="article-dates__entry"><span class="article-dates__label">Received: </span><span><time datetime="2015-12-05">05 December 2015</time></span></div><div class="article-dates__entry"><span class="article-dates__label">Revised: </span><span><time datetime="2016-07-12">12 July 2016</time></span></div><div class="article-dates__entry"><span class="article-dates__label">Accepted: </span><span><time datetime="2016-07-17">17 July 2016</time></span></div></div></div></div><div class="main-context__column">    <ul id="book-metrics" class="article-metrics u-sansSerif">
            <li class="article-metrics__item">
                     <span class="article-metrics__views">353</span>
                     <span class="article-metrics__label">Downloads</span>
            </li>
            <li class="article-metrics__item">
                    <a class="article-metrics__link gtm-citations-count" href="https://citations.springer.com/item?doi=10.1007/s13278-016-0362-9" target="_blank" rel="noopener" title="Visit Springer Citations for full citation details" id="citations-link">
                            <span id="citations-count-number" class="test-metric-count c-button-circle gtm-citations-count">1</span>
                       <span class="test-metric-name article-metrics__label gtm-citations-count">Citations</span>
                    </a>
            </li>
    </ul>
</div></div></div><section class="Abstract" id="Abs1" tabindex="-1" lang="en"><h2 class="Heading">Abstract</h2><p id="Par1" class="Para">Medical
 social networking sites enabled multimedia content sharing in large 
volumes, by allowing physicians and patients to upload their medical 
images. Moreover, it is necessary to employ new techniques in order to 
effectively handle and benefit from them. This huge volume of images 
needs to formulate new types of queries that pose complex questions to 
medical social network databases. Content-based image retrieval (CBIR) 
stills an active and efficient research topic to manipulate medical 
images. In order to palliate this situation, we propose in this paper 
the integration of a content-based medical image retrieval method 
through a medical social network, based on an efficient fusion of 
low-level visual image features (color, shape and texture features), 
which offers an efficient and flexible precision. A clear application of
 our CBIR system consists of providing stored images that are visually 
similar to a new (undiagnosed) one, allowing specialist and patients to 
check past examination diagnoses from comments and other physicians’ 
annotations, and to establish, therefore, a new diagnostic or to prepare
 a new report of an image’s examination. To scale up the performance of 
the integrated CBIR system, we implement a relevance feedback method. It
 is an effective method to bridge the semantic gap between low-level 
visual features and high-level semantic meanings. Experiments show that 
the proposed medical image retrieval scheme achieves better performance 
and accuracy in retrieving images. However, we need also to verify 
whether our approach is considered by the specialists as a potential aid
 in a real environment. To do so, we evaluate our methodology’s impact 
in the user’s decision, inquiring the specialists about the degree of 
confidence in the retrieval system. By analyzing the obtained results, 
we can argue that the proposed methodology presented a high acceptance 
regarding the specialists’ interests in the clinical practice domain and
 can improve the decision-making process during analysis.</p></section><div class="KeywordGroup" lang="en"><h2 class="Heading">Keywords</h2><span class="Keyword">Medical social network&nbsp;</span><span class="Keyword">Content-based medical image retrieval (CBMIR)&nbsp;</span><span class="Keyword">Feature extraction&nbsp;</span><span class="Keyword">Relevance feedback&nbsp;</span></div><div class="article-actions--inline" id="article-actions--inline" data-component="article-actions--inline"><div class="citations u-interface c-button-dropdown" data-component="SV.Dropdown" data-namespace="citations--inline" aria-label="button with dropdown options">
        <button type="button" class="c-button-dropdown__button" data-role="button-dropdown__control" aria-pressed="false" aria-expanded="false" aria-controls="Dropdown.citations--inline-dropdown"><span class="u-overflow-ellipsis c-button-dropdown__button-title">
    <span>Cite</span>
    <span class="hide-text-small">article</span>
</span><span class="c-button-dropdown__icon"></span></button>
<div class="u-composite-layer c-button-dropdown__container" aria-hidden="true" aria-label="dropdown" id="Dropdown.citations--inline-dropdown"><ul class="citations__content" data-role="button-dropdown__content">
    <li>
        <a href="#citeas" class="gtm-cite-dropdown" tabindex="-1">How to cite?</a>
    </li>
        <li>
            <a href="https://citation-needed.springer.com/v2/references/10.1007/s13278-016-0362-9?format=refman&amp;flavour=citation" title="Download this article's citation as a .RIS file" class="gtm-export-citation" data-gtmlabel="RIS" tabindex="-1">
                <span class="citations__extension" data-gtmlabel="RIS">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"></path></svg>
                    .RIS
                </span>
                <span class="citations__types">
                        <span>
                            Papers
                        </span>
                        <span>
                            Reference Manager
                        </span>
                        <span>
                            RefWorks
                        </span>
                        <span>
                            Zotero
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="https://citation-needed.springer.com/v2/references/10.1007/s13278-016-0362-9?format=endnote&amp;flavour=citation" title="Download this article's citation as a .ENW file" class="gtm-export-citation" data-gtmlabel="ENW" tabindex="-1">
                <span class="citations__extension" data-gtmlabel="ENW">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"></path></svg>
                    .ENW
                </span>
                <span class="citations__types">
                        <span>
                            EndNote
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="https://citation-needed.springer.com/v2/references/10.1007/s13278-016-0362-9?format=bibtex&amp;flavour=citation" title="Download this article's citation as a .BIB file" class="gtm-export-citation" data-gtmlabel="BIB" tabindex="-1">
                <span class="citations__extension" data-gtmlabel="BIB">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"></path></svg>
                    .BIB
                </span>
                <span class="citations__types">
                        <span>
                            BibTeX
                        </span>
                        <span>
                            JabRef
                        </span>
                        <span>
                            Mendeley
                        </span>
                </span>
            </a>
        </li>
</ul></div>
    </div></div><div id="body"><section id="Sec1" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading" data-role="collapsible-handle" tabindex="0"><span class="HeadingNumber">1 </span>Introduction<span class="section-icon"></span></h2><div class="content"><p id="Par2" class="Para">Online
 social networking is attracting more and more people in today’s 
Internet, where users can share and consume all kinds of multimedia 
contents (Zhi et al. <span class="CitationRef"><a href="#CR90" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2013</a></span>).
 Like most people, health care professionals use mainstream social media
 networks to connect with friends and family. But, almost one-third of 
them also join social networks focused exclusively on health care, where
 health care professionals can collaborate and share resources online, 
and patients can access more than information (Doganay <span class="CitationRef"><a href="#CR20" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2014</a></span>). According to Doganay (<span class="CitationRef"><a href="#CR20" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2014</a></span>),
 patient-focused networks, often built around a particular condition or 
disease, give individuals and their families’ supportive communities 
where they receive comfort, insights, and potential leads on new 
treatments. The data mining practices of sites like Facebook and Twitter
 make some patients and providers leery of posting questions or 
comments, while many health care organizations use Facebook, Twitter, 
LinkedIn, Instagram, and other social tools to communicate with 
constituents, individuals often worry about posting information in the 
wrong place. By sharing data on specialized sites, health care, 
professionals and other users can feel safer about expressing their 
thoughts (Doganay <span class="CitationRef"><a href="#CR20" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2014</a></span>). Franklin and Greene (<span class="CitationRef"><a href="#CR25" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2007</a></span>) consider that participation in the health care management can render patients longer health conscious. According to Grenier (<span class="CitationRef"><a href="#CR29" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2003</a></span>),
 the main objective behind medical networks is to foster collaboration 
between medical actors and to place the patient at the heart of the 
health system. In reality, the fact of making important decisions, 
related to medical images, individually, can lead the physician to 
commit errors leading to malpractices and consequently to unexpected 
damages. This fact is justified by a study done by The Institute of 
Medicine of the National Sciences Academy<sup><a href="#Fn1" id="Fn1_source">1</a></sup>
 (IMNAS) in USA. This institute published a study estimating that up to 
98,000 hospital deaths each year can be attributed to medical 
malpractice (Messaoudi et al. <span class="CitationRef"><a href="#CR48" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2013</a></span>).
 In order to minimize medical errors, a medical social network, as a 
first contribution, destined to present patients’ medical images and 
physicians’ interpretations expressing their medical reviews and advices
 presents the solution to support collaboration between physicians and 
patients. This will, obviously, help to save time and better serve the 
patients about their situations.</p><p id="Par4" class="Para">But, the 
need, to index and retrieve medical images content shared through a 
social network, becomes obviously a requirement due to the large volume 
of images uploaded by physicians and patients. It is necessary to employ
 new techniques in order to effectively handle and benefit from images 
and to perform similarity queries and support decision making. The 
diversity of the available content inspired users to demand and 
formulate more complicated queries. Generally, image databases are 
text-annotated, whereby image retrieval is based on keyword searching. 
Such an approach has many disadvantages, keyword annotation is very 
subjective. Moreover, keyword-based image retrieval is not appropriate 
because there is no fixed set of words that describes image content. The
 content-based image retrieval (CBIR) was presented to address the 
difficulties related to the text-based image retrieval (Zeyad et al. <span class="CitationRef"><a href="#CR85" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2014</a></span>).
 The CBIR system practices image content to search and retrieve digital 
images. CBIR is a method, in which various visual contents have been 
considered to search and retrieve images on the basis of automatically 
derived features such as color, texture, and shape (Singh et al. <span class="CitationRef"><a href="#CR67" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>), from large scale of image databases based on the user’s requests in the form of a query image (Nandagopalan et al. <span class="CitationRef"><a href="#CR51" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2008</a></span>).
 A major problem with CBIR systems is the so-called semantic gap, which 
refers to the difficulty of translation of user’s intentions into 
similarities among low-level features (Arevalillo-Herráez et al. <span class="CitationRef"><a href="#CR8" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2013</a></span>).
 To avoid the semantic gap, it is important to have the user interacting
 and telling what are the truly relevant images from those retrieved. 
This way of interaction between the user and the system is called 
relevance feedback (RF). Relevance feedback is a framework inherited 
from traditional information retrieval that has extensively been used to
 increase the efficiency of CBIR systems. Relevance feedback is one of 
the mechanisms, of increasing the accuracy of the retrievals (Bugatti et
 al. <span class="CitationRef"><a href="#CR14" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2011</a></span>).</p><p id="Par5" class="Para">In
 this paper, we propose the integration of a medical CBIR system with 
different stages of the relevance feedback process through the 
implemented medical social network site based on an efficient fusion of 
color, shape, and texture features. We, first, introduce the 
architecture of the integrated CBIR system, and then describe in detail 
each component of the system. The RF approach was proposed to learn the 
user’s intentions by asking the specialist to quantify the relevance of 
each medical image retrieved by a query, allowing the system to 
automatically adjust future query results (Bugatti et al. <span class="CitationRef"><a href="#CR14" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2011</a></span>).
 The idea is that the system executes a similarity query using the 
undiagnosed image as the query center and the system displays the 
retrieved images that are similar to the given one, allowing the 
specialist to take advantage of previous analyses of them from user’s 
comments, helping the improvement of the decision-making process and 
whose goal also is learning among medical residents and medical students
 that are currently, in the course of their training.</p><p id="Par6" class="Para">However,
 the study of the social network site effect and our methodology impact,
 in the decision-making process, become very important task. Indeed, the
 great majority of medical social network communities do not verify 
whether the methodology of existing CBIR system in a medical social 
network site is actually considered by the specialists and also patients
 as a potential aid in a real environment and to analyze his 
effectiveness for supporting physicians and patients in clinical 
routine. For this reason, we propose an additional contribution 
consisting of the evaluation of our method’s impact in the user’s 
decision, inquiring the specialists about their degree of confidence in 
the integrated CBIR system and in our social network, in generally.</p><p id="Par7" class="Para">The remainder of this paper is structured as follows. Section&nbsp;<span class="InternalRef"><a href="#Sec2">2</a></span> presents the background needed to follow our methodology. Section&nbsp;<span class="InternalRef"><a href="#Sec8">3</a></span> describes the design of our social network. Section&nbsp;<span class="InternalRef"><a href="#Sec9">4</a></span>
 details the proposed methodology, shows experimental measurements, 
discusses the experiments, and analyzes the achieved results. 
Section&nbsp;<span class="InternalRef"><a href="#Sec25">5</a></span> presents the evaluation of our methodology and details the experiments and results achieved. Finally, Sect.&nbsp;<span class="InternalRef"><a href="#Sec28">6</a></span> draws some conclusions and highlights future research directions.</p></div></section><section id="Sec2" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading" data-role="collapsible-handle" tabindex="0"><span class="HeadingNumber">2 </span>Related work<span class="section-icon"></span></h2><div class="content"><section id="Sec3" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">2.1 </span>Medical social networks</h3><p id="Par8" class="Para">Today,
 social networks have the ability to connect people with just about 
everything. The influence of social network and those using social 
networks grows and changes daily, generating a profound impact on 
society. Furthermore, a growing majority of modern patients, 
particularly those with chronic conditions are seeking out social 
network and other online sources to acquire health information, connect 
with others affected by similar conditions, and play a more active role 
in their health care decisions (Daniel et al. <span class="CitationRef"><a href="#CR17" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2013</a></span>).
 In 2011, more than 80&nbsp;% of adults reported using the Internet as a
 resource for health care quality information and more than half of 
patients (57&nbsp;%) said they were more likely to select hospitals 
based on their social media presence (Feldman <span class="CitationRef"><a href="#CR23" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>).
 Indeed, research shows that 81&nbsp;% of consumers believe that if 
hospitals have a strong media presence, they are likely to be more 
innovative than other hospitals. According to the Centers for Disease 
Control and Prevention (CDC), “Using social network tools has become an 
effective way to expand reach, foster engagement and increase access to 
credible, science-based health messages.” (Feldman <span class="CitationRef"><a href="#CR23" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>)
 Medicals networks have several forms which can be networks of hospital 
management (internal coordination and fragmentation within the hospital 
by specialty), resource networks (shared resources such as scanners), 
information networks (data collection to adjust policy information), and
 among many others care networks (Grenier <span class="CitationRef"><a href="#CR29" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2003</a></span>),
 which offers the members suffering from diseases an opportunity to 
change their lives, connect with others, and share problems. Indeed, 
research shows that 81&nbsp;% of patients believe that if hospitals have
 a strong media presence, they are likely to be more innovative than 
other hospitals.<sup><a href="#Fn2" id="Fn2_source">2</a></sup> </p><p id="Par10" class="Para">According to Feldman (<span class="CitationRef"><a href="#CR23" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>),
 hospitals are increasingly adopting the use of social network for a 
variety of key tasks, including: education and wellness programs, crisis
 communication, staff and volunteer training, employee and volunteer 
recruitment, information sharing, clinical trial recruitment and other 
research, public relations, and marketing. Since the beginning of 2011 
alone, the growth in social network use for hospitals has been 
staggering. Ed Bennett, the manager of web operations at the University 
Medical Center in Baltimore, has been tracking hospital social media on 
his private site since 2008. He reports that as of October 2011, nearly 
4000 social media sites were owned by US hospitals (Feldman <span class="CitationRef"><a href="#CR23" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>). Many examples of medical social networks were presented in relation to different medical activities: SoberCircle<sup><a href="#Fn3" id="Fn3_source">3</a></sup> is intended for alcoholics and drug addicts in need for support and encouragement by others. SparkPeople,<sup><a href="#Fn4" id="Fn4_source">4</a></sup> Fitocracy,<sup><a href="#Fn5" id="Fn5_source">5</a></sup> and Dacadoo<sup><a href="#Fn6" id="Fn6_source">6</a></sup> do share workouts and exercises in order to sustain them during weight loss. Asklepios,<sup><a href="#Fn7" id="Fn7_source">7</a></sup> exclusively for Canadian doctors, is meant to exchange the best practices ever and to learn from each other. CardioSource,<sup><a href="#Fn8" id="Fn8_source">8</a></sup> Cardiothoracic Surgery Network, concerns cardiothoracic surgery. Diabspace<sup><a href="#Fn9" id="Fn9_source">9</a></sup> is intended for diabetics. “Parlons Cancer”<sup><a href="#Fn10" id="Fn10_source">10</a></sup> is dedicated to cancer patients and their families. Renaloo<sup><a href="#Fn11" id="Fn11_source">11</a></sup> deals with kidney disease, dialysis, and transplantation. RxSpace<sup><a href="#Fn12" id="Fn12_source">12</a></sup> is dedicated to pharmacy students, pharmacists, pharmacy owners, and academia to interact with each other.</p><p id="Par21" class="Para">Besides,
 we can summarize that the top 25 health and medical social network 
sites are organized in three. Firstly, social networks for doctors which
 offer great opportunities to confer find support and provide their own 
expertise, such as, Sermo (<span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.sermo.com/"><span class="RefSource">http://www.sermo.com/</span></a></span>), Ozmosis (<span class="ExternalRef"><a target="_blank" rel="noopener" href="https://ozmosis.org/"><span class="RefSource">https://ozmosis.org/</span></a></span>), MomMD (<span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.mommd.com/"><span class="RefSource">http://www.mommd.com/</span></a></span>), Doctor Network (<span class="ExternalRef"><a target="_blank" rel="noopener" href="http://doctor-network.com/"><span class="RefSource">http://doctor-network.com/</span></a></span>), etc.</p><p id="Par22" class="Para">Secondly,
 social networks for nurses which can help them to connect with others 
who understand what is happening in the field by asking and answering 
questions and learning more about their profession, such as, Nursing 
Link (<span class="ExternalRef"><a target="_blank" rel="noopener" href="http://nursinglink.monster.com/"><span class="RefSource">http://nursinglink.monster.com/</span></a></span>), Ultimate Nurse (<span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.ultimatenurse.com/"><span class="RefSource">http://www.ultimatenurse.com/</span></a></span>), Nurse Zone (<span class="ExternalRef"><a target="_blank" rel="noopener" href="http://nursezone.com/"><span class="RefSource">http://nursezone.com/</span></a></span>), etc.</p><p id="Par23" class="Para">Finally, social networks for all health and medical careers, such as, MedicalMingle (<span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.medicalmingle.com/"><span class="RefSource">http://www.medicalmingle.com/</span></a></span>), Clinically Psyched (<span class="ExternalRef"><a target="_blank" rel="noopener" href="http://clinicallypsyched.com/"><span class="RefSource">http://clinicallypsyched.com/</span></a></span>), PatientsLikeMe (<span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.patientslikeme.com/"><span class="RefSource">http://www.patientslikeme.com/</span></a></span>), Radiolopolis (<span class="ExternalRef"><a target="_blank" rel="noopener" href="http://radiolopolis.com/"><span class="RefSource">http://radiolopolis.com/</span></a></span>), Docadoc (<span class="ExternalRef"><a target="_blank" rel="noopener" href="http://docadoc.com/"><span class="RefSource">http://docadoc.com/</span></a></span>), Carenity (<span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.carenity.com/"><span class="RefSource">http://www.carenity.com/</span></a></span>),
 etc. These different kinds of social networking sites offer to their 
members an opportunity to be connected with others and share experiences
 and knowledge.</p><div id="Par24" class="Para">Our first goal is to 
design a social network dedicated to physicians and patients. The basic 
model of the targeted social media should take into account the 
management of a:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li> <p id="Par25" class="Para">Set of patients which provide personal information on their health care profile.</p> </li><li> <p id="Par26" class="Para">Set of physicians providing information enabling their identifications.</p> </li><li> <p id="Par27" class="Para">Set of mechanisms permitting to patients to upload the medical images related to their diseases.</p> </li><li> <p id="Par28" class="Para">Set of mechanisms permitting to physicians to comment the uploaded medical images.</p> </li><li> <p id="Par29" class="Para">Set of search functions by which patients and physicians can locate easy and efficient information about medical images.</p> </li><li> <p id="Par30" class="Para">Site
 operator who controls the site and triggers a set of mechanisms 
permitting to collect medical images in order to process them for 
various purposes such as medical images’ indexation.</p> </li></ul></div> </div><div id="Par31" class="Para">Like in (Feldman <span class="CitationRef"><a href="#CR23" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>), our social network, addressed to physicians and patients, can connect millions of voices to:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li> <p id="Par32" class="Para">Increase the timely dissemination and potential impact of health and safety information;</p> </li><li> <p id="Par33" class="Para">Leverage audience networks to facilitate information sharing;</p> </li><li> <p id="Par34" class="Para">Personalize and reinforce health messages that can be more easily tailored or targeted to particular audiences;</p> </li><li> <p id="Par35" class="Para">Empower people to make safer and healthier decisions;</p> </li><li> <p id="Par36" class="Para">Facilitate interactive communication, connection, and public engagement;</p> </li><li> <p id="Par37" class="Para">Updates patients about changes in physician’s practice;</p> </li><li> <p id="Par38" class="Para">Keeps patients informed about upcoming appointments, tests, and immunizations;</p> </li><li> <p id="Par39" class="Para">Engages patients in discussions about key health issues;</p> </li><li> <p id="Par40" class="Para">Answers patients’ medical questions;</p> </li><li> <p id="Par41" class="Para">Communicates with family members, other caregivers;</p> </li><li> <p id="Par42" class="Para">Grows physician’s practice;</p> </li><li> <p id="Par43" class="Para">etc.</p> </li></ul></div> </div><p id="Par44" class="Para">In our work, we respect that our social network needs to contain all of these features, situated above.</p></section><section id="Sec4" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">2.2 </span>Content-based image retrieval</h3><section id="Sec5" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.2.1 </span>An overview</h4><p id="Par45" class="Para">Content-based
 image retrieval (CBIR) is an image search technique designed to find 
images that are most similar to a given query. It complements text-based
 retrieval by using quantifiable and objective image features as the 
search criteria (Smeulders et al. <span class="CitationRef"><a href="#CR68" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2000</a></span>).
 Essentially, CBIR measures the similarity of two images based on the 
similarity of the properties of their visual components, which can 
include the color, texture, shape, and spatial arrangement of regions of
 interest (ROIs) (Kumar et al. <span class="CitationRef"><a href="#CR40" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2013</a></span>).</p><p id="Par46" class="Para">The
 color is one of the most reliable visual features that are also easier 
to implement in image retrieval systems. Color is a property, i.e., 
light reflection aids information processing from medical image. Color 
is extensively a visual feature used in CBIR that moderates the robust 
and simple processing. Color sensitivity and color space were, 
respectively, proposed in finding color similar to how humans perceive 
color (Zeyad et al. <span class="CitationRef"><a href="#CR85" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2014</a></span>; Komali et al. <span class="CitationRef"><a href="#CR39" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>).
 Texture features are also intended to capture the granularity and 
repetitive patterns of surfaces within an image. Their role in 
domain-specific image retrieval is particularly vital due to their close
 relation to the underlying semantics in these cases, such as in aerial 
imagery and medical imaging. Texture features, such as gray-level 
co-occurrence matrix (GLCM), Markov random field (MRF) model, 
simultaneous auto-regressive (SAR) model, edge histogram descriptor 
(EHD), etc., have long been studied in image processing, computer 
vision, and computer graphics (Xiang-Yang et al. <span class="CitationRef"><a href="#CR79" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>).</p><p id="Par47" class="Para">The
 shape is also a visual feature that describes the contours of objects 
in an image, which are usually extracted from segmenting the image into 
meaningful regions or objects. It is known to be an important cue for 
human to identify and recognize real-world objects. However, since it is
 difficult to achieve such image segmentation for natural images, the 
use of shape features in image retrieval has been limited to special 
applications where the extraction of object contours is readily 
available such as in trademark images (Zhang and Ye <span class="CitationRef"><a href="#CR87" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2010</a></span>). Zhang and Lu (<span class="CitationRef"><a href="#CR86" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2004</a></span>)
 broadly classify shape extraction techniques into two major groups: 
contour-based and region-based methods. Contour-based methods calculate 
shape features only from the boundary of the shape, while region-based 
methods extract features from the entire region. Because contour-based 
techniques use only a portion of the region, they are more sensitive to 
noise than region-based techniques, as small changes in the shape 
significantly affect the shape contour. Therefore, color image retrieval
 usually employs region-based shape features (Zhang et al. <span class="CitationRef"><a href="#CR89" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>).
 The choice of features is a critical task when designing a CBIR system 
because it is closely related to the definition of similarity. Features 
fall into several categories. Application-specific features are tuned to
 a particular problem and describe characteristics unique to a 
particular problem domain; they are semantic features intended to encode
 a specific meaning (Smeulders et al. <span class="CitationRef"><a href="#CR68" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2000</a></span>).</p><p id="Par48" class="Para">The
 major challenges for CBIR include the application-specific definition 
of similarity (based on users’ criterion), extraction of image features 
that are relevant to this definition of similarity, and organizing these
 features into indices for fast retrieval from large repositories 
(Smeulders et al. <span class="CitationRef"><a href="#CR68" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2000</a></span>; Lew et al. <span class="CitationRef"><a href="#CR43" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2006</a></span>; Rui et al. <span class="CitationRef"><a href="#CR60" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">1999</a></span>; Datta et al. <span class="CitationRef"><a href="#CR18" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2008</a></span>). According to Kumar et al. (<span class="CitationRef"><a href="#CR40" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2013</a></span>),
 the similarity of image features can be measured in a number of ways. 
When the features are represented as a vector, distance metrics such as 
the Euclidean distance can be used. The notion of elastic deformation 
can be used to define similarity when subtle geometric differences 
between images are important. Graph matching enables the comparison of 
images based upon a combination of image features and the arrangement of
 objects in the images (or the relationships between them). Finally, 
statistical classifiers can be trained to categorize the query image 
into known classes (Kumar et al. <span class="CitationRef"><a href="#CR40" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2013</a></span>). A detailed discussion of various similarity measures can be found in (Akgül et al. <span class="CitationRef"><a href="#CR4" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2011</a></span>).</p><p id="Par49" class="Para">An
 underlying assumption of most CBIR systems is that the chosen image 
features used are sufficient to describe the image accurately. The 
choice of image features must, therefore, be made to minimize two major 
limitations: the sensory gap and the semantic gap (Smeulders et al. <span class="CitationRef"><a href="#CR68" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2000</a></span>).
 The sensory gap is the difference between the object in the world and 
the features derived from the image. It arises when an image is noisy, 
has low illumination, or includes objects that are partially occluded by
 other objects. The sensory gap is further compounded when 2D images of 
physical 3D objects are considered; some information is lost as the 
choice of viewpoint means an object may occlude part of itself (Kumar et
 al. <span class="CitationRef"><a href="#CR40" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2013</a></span>).
 The semantic gap is the conflict between the intent of the user and the
 images retrieved by the algorithm. It occurs because CBIR systems are 
unable to interpret images; they do not understand the “meaning” in the 
images in the same way that a human does. Retrieval is performed on the 
basis of image features not image interpretations (Kumar et al. <span class="CitationRef"><a href="#CR40" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2013</a></span>).</p><p id="Par50" class="Para">In
 order to overcome the semantic gap problem and to improve CBIR systems 
performance, relevance feedback (RF) is the process of automatically 
adjusting an existing query using the information feedback by the user 
about the relevance of previously retrieved objects such that the 
adjusted query (Shanmugapriya and Nallusamy <span class="CitationRef"><a href="#CR63" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2014</a></span>).
 The key issue in relevance feedback is how to effectively utilize the 
feedback information to improve the retrieval performance (Xin and Jin <span class="CitationRef"><a href="#CR81" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2004</a></span>).
 After obtaining the retrieval results, user provides the feedback as to
 whether the results are relevant or non-relevant. If the results are 
non-relevant, the feedback loop is repeated many times until the user is
 satisfied. Patil and Kokare (<span class="CitationRef"><a href="#CR53" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2011</a></span>)
 provide an overview of the technical achievements in the research area 
of relevance feedback (RF) in content-based image retrieval (CBIR). It 
also covers the current state of the art of the research of relevance 
feedback in CBIR, various relevance feedback techniques, and issues in 
relevance feedback. Hence many classical machine learning schemes may be
 applied to the RF, which include decision tree learning (MacArthur et 
al. <span class="CitationRef"><a href="#CR46" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2000</a></span>), Bayesian learning (Cox et al. <span class="CitationRef"><a href="#CR16" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2000</a></span>; Su et al. <span class="CitationRef"><a href="#CR72" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2003</a></span>), support vector machines (Wang and Hua <span class="CitationRef"><a href="#CR75" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2011</a></span>; Hoi et al. <span class="CitationRef"><a href="#CR32" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2009</a></span>; Wang et al. <span class="CitationRef"><a href="#CR77" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2014</a></span>), boosting (Tieu and Viola <span class="CitationRef"><a href="#CR74" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2003</a></span>)
 and so on. According to the strategy employed, RF techniques can be 
divided into two main categories: query point movement, and re-weighting
 schemes of the similarity measure. The query point movement techniques 
consider that a query is represented by a single query center. 
Therefore, at each user interaction cycle, the strategy estimates an 
ideal query center in the query space, moving the query center toward 
the relevant examples and away from the irrelevant ones. On the other 
hand, the re-weighting techniques usually focus on adjusting weights to 
each dimension of the feature vector emphasizing some dimensions and 
diminishing the influence of others (Doulamis and Doulamis <span class="CitationRef"><a href="#CR21" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2006</a></span>). In the proposed method, relevance feedback technique can be done using decision trees.</p><p id="Par51" class="Para">Many CBIR systems have been proposed in recent decades, including IBM’s query by image content (QBIC) system (Flickner et al. <span class="CitationRef"><a href="#CR24" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">1995</a></span>), which was used to search for famous artworks; others include the Virage framework (Bach et al. <span class="CitationRef"><a href="#CR10" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">1996</a></span>), Photobook (Pentland et al. <span class="CitationRef"><a href="#CR54" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">1996</a></span>).
 More recently, google search by image used the points, colors, lines, 
and textures in images uploaded by users to find similar images (Chechik
 et al. <span class="CitationRef"><a href="#CR15" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2010</a></span>).
 These recent developments mean that CBIR is a technology that is 
available to the masses. Other systems have been also presented like for
 examples: MARS, NeTra, PicHunter, Blobworld, VisualSEEK and SIMPLIcity 
(Wang et al. <span class="CitationRef"><a href="#CR76" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>; Hu et al. <span class="CitationRef"><a href="#CR34" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2011</a></span>).</p><p id="Par52" class="Para">To
 evaluate CBIR systems, precision and recall are generally two quality 
measures defined to calculate the accuracy of an approximate search 
paradigm. Precision refers to the proportion of retrieved images that 
are relevant, i.e., the proportion of all retrieved images that the user
 was expecting. Recall is the proportion of all relevant images that 
were retrieved, i.e., the proportion of similar images in the dataset 
that were actually retrieved. The ideal case would be a retrieval system
 that achieves 100&nbsp;% precision and 100&nbsp;% recall (Kumar et al. <span class="CitationRef"><a href="#CR40" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2013</a></span>).
 The reality is that most current algorithms fail to find all similar 
images, and many of the retrieved images contain dissimilar images 
(false positives).</p></section><section id="Sec6" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.2.2 </span>Content-based image retrieval in medical applications</h4><p id="Par53" class="Para">Our
 focus in this section is on techniques adapted to the medical field. 
Several studies have already reported on the potential clinical benefits
 of CBIR in clinical applications. The ASSERT CBIR system used for 
high-resolution CT (HRCT) lung images (Shyu et al. <span class="CitationRef"><a href="#CR64" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">1999</a></span>) showed an improvement in the accuracy of the diagnosis made by physicians (Aisen et al. <span class="CitationRef"><a href="#CR3" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2003</a></span>). Another study for liver CT concluded that CBIR could provide real-time decision support (Napel et al. <span class="CitationRef"><a href="#CR52" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2010</a></span>). CBIR was also shown to have benefits when used as part of a radiology teaching system (Müller et al. <span class="CitationRef"><a href="#CR50" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2005</a></span>).</p><p id="Par54" class="Para">The image retrieval in medical applications (IRMA)<sup><a href="#Fn13" id="Fn13_source">13</a></sup>
 project has been a sustained effort in the CBIR of X-ray images for 
medical diagnosis systems. The IRMA approach is divided into seven 
interdependent steps (Keysers et al. <span class="CitationRef"><a href="#CR38" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2003</a></span>):
 (1) categorization based on global features, (2) registration using 
geometry and contrast, (3) local feature extraction, (4) 
category-dependent and query-dependent feature selection, (5) 
multi-scale indexing, (6) identification of semantic knowledge, and (7) 
retrieval on the basis of the previous steps. The IRMA method classifies
 images into anatomical areas, modalities, and viewpoints and provides a
 generic framework (Güld et al. <span class="CitationRef"><a href="#CR30" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2007</a></span>) that allows the derivation of flexible implementations that are optimized for specific applications.</p><p id="Par56" class="Para">A number of papers (Antani et al. <span class="CitationRef"><a href="#CR7" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2004</a></span>; Antani et al. <span class="CitationRef"><a href="#CR6" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2003</a></span>; Lee et al. <span class="CitationRef"><a href="#CR41" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2003</a></span>; Xu et al. <span class="CitationRef"><a href="#CR82" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2008</a></span>; Hsu et al. <span class="CitationRef"><a href="#CR33" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2009</a></span>; Lee et al. <span class="CitationRef"><a href="#CR42" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2009</a></span>; Qian et al. <span class="CitationRef"><a href="#CR55" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2010</a></span>)
 have described investigations into every component of CBIR for spine 
X-ray retrieval, including feature extraction (Antani et al. <span class="CitationRef"><a href="#CR6" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2003</a></span>; Lee et al. <span class="CitationRef"><a href="#CR41" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2003</a></span>; Qian et al. <span class="CitationRef"><a href="#CR55" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2010</a></span>), indexing (Qian et al. <span class="CitationRef"><a href="#CR55" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2010</a></span>), similarity measurement (Qian et al. <span class="CitationRef"><a href="#CR55" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2010</a></span>), and visualization and refinement.</p><p id="Par57" class="Para">For the purpose of this study, it is essential to integrate or benefit from the use of the following works such as Agma et al. (<span class="CitationRef"><a href="#CR2" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2007</a></span>), Singh et al. (<span class="CitationRef"><a href="#CR66" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2009</a></span>), Bugatti et al. (<span class="CitationRef"><a href="#CR14" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2011</a></span>), and Harishchandra et al. (<span class="CitationRef"><a href="#CR31" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2014</a></span>). Starting from the proposal of Agma et al. (<span class="CitationRef"><a href="#CR2" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2007</a></span>),
 this study introduced the extraction of texture features, by 
statistical features and color features, by the gray-level histogram, 
for the indexation and retrieval of images, in the context of a 
comparative study between descriptors. The first step of the method is a
 segmentation process based on Markov random fields. After the 
segmentation process, features are extracted from each cluster. 
Euclidean distance was applied to measure the similarity between images’
 features.</p><p id="Par58" class="Para">Greenspan (<span class="CitationRef"><a href="#CR28" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2007</a></span>)
 indexed the images by the Gaussian mixture modeling (GMM) as a texture 
descriptor to construct vectors of features. Kullback–Leibler (KL) 
measure was used to calculate the similarity between features. In 
Selvarani and Annadurai (<span class="CitationRef"><a href="#CR61" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2007</a></span>),
 an image retrieval system is provided to retrieve the images using as 
texture representation, statistics measures (mean and variance). They 
presented a method of texture analysis based on the use of Gabor 
filters. Euclidean distance was used to measure the similarity between 
images index. Zhang et al. (<span class="CitationRef"><a href="#CR88" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2008</a></span>)
 showed also good performance with a new approach based on directional 
texture extracted from images. Direction histogram is computed from the 
enhanced Fourier spectrum. Gray-level co-occurrence matrix extracts 
texture characteristics based on the set. Euclidean distance is used, 
also, to evaluate the probability of similarity between images.</p><p id="Par59" class="Para">In Wu and Tai (<span class="CitationRef"><a href="#CR78" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2009</a></span>),
 the texture features were extracted from images by using the gray-level
 variation histogram. Earth mover’s distance was used to measure the 
image similarity. Singh et al. (<span class="CitationRef"><a href="#CR66" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2009</a></span>)
 proposed also a medical CBIR retrieval framework, a method of texture 
analysis for image retrieval, based on the use of Gabor filters. The 
similarity used for comparing images index was the Euclidean distance. 
Moreover, Seng and Mirisaee (<span class="CitationRef"><a href="#CR62" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2009</a></span>)
 have experimented various similarity measures comparison. This study 
was based on the extraction of texture features by the wavelet transform
 and color features by global color histogram. In the same context, 
Bugatti et al. (<span class="CitationRef"><a href="#CR13" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2009</a></span>)
 have extracted texture features by the gray-level co-occurrence matrix 
and color features by the gray-level histogram for the image indexing 
and retrieval. As similarity measure, they used Minkowski and Canberra 
distances. Other approaches for radiograph retrieval have tried to group
 features into semantically meaningful patterns. In one such study 
(Iakovidis et al. <span class="CitationRef"><a href="#CR35" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2009</a></span>),
 multi-scale statistical features were extracted from images by a 2D 
discrete wavelet transform. These features were then clustered into 
small patterns; images were represented as complex patterns consisting 
of sets of these smaller patterns. Experimental results revealed that 
the method had significantly higher precision and recall compared to two
 conventional approaches: local and global gray-level histograms.</p><p id="Par60" class="Para">Furthermore, Sidong et al. (<span class="CitationRef"><a href="#CR65" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2010</a></span>)
 have extracted, for their medical CBIR system, texture feature based on
 curvelet transform, Gabor filters, and wavelet transform. Euclidean 
distance is used to measure similarity between the images. Bueno et al. (<span class="CitationRef"><a href="#CR12" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2010</a></span>)
 have proposed a texture retrieval algorithm for images, based on the 
extraction of the Haralick’s texture features and Zernike moments. The 
measure used for similarity between images index is Manhattan distance. 
De Oliveira et al. (<span class="CitationRef"><a href="#CR19" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2010</a></span>)
 have proposed a breast density feature extraction for the image 
retrieval, to help the radiologists in their diagnosis. Bugatti et al. (<span class="CitationRef"><a href="#CR14" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2011</a></span>)
 have proposed also a method based on k-nearest neighbor classification 
algorithm for MRI medical image classification with color descriptors, 
by the gray-level histogram, as extracted features. In the same context,
 Bhattacharjee and Parekh (<span class="CitationRef"><a href="#CR11" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2011</a></span>)
 have experimented various similarity measures (Manhattan distance and 
Euclidean Distance) based on the extraction of texture features by the 
gray-level co-occurrence matrix.</p><p id="Par61" class="Para">In order to present a comparative study between features, Ashish and Manpreet (<span class="CitationRef"><a href="#CR9" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>)
 indexed the images by embedding the shapes features extracted by the 
Fourier descriptors and the Haar wavelet transform. John and Kazunori (<span class="CitationRef"><a href="#CR37" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>)
 used other kinds of features: SIFT scale invariant feature transform 
with various similarity measures (Minkowski and standard measures, 
statistical measures and divergence measures). Ramamurthy and Chandran (<span class="CitationRef"><a href="#CR57" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>)
 have proposed an efficient image retrieval tool namely, “Content Based 
Medical Image Retrieval with Texture Content using gray-level 
co-occurrence matrix (GLCM) and <em class="EmphasisTypeItalic ">k</em>-Means
 Clustering algorithms.” This image retrieval tool is capable of 
retrieving images based on the texture feature of the image, and it 
takes into account the preprocessing, feature extraction, classification
 and retrieval steps in order to construct an efficient retrieval tool. 
The main feature of this tool is used of GLCM of the extracting texture 
pattern of the image and <em class="EmphasisTypeItalic ">k</em>-means 
clustering algorithm for image classification in order to improve 
retrieval efficiency. The proposed image retrieval system consists of 
three stages, i.e., segmentation, texture feature extraction and 
clustering process. The result indicates that the tool gives better 
performance in terms of percentage for all the 1000 real-time medical 
images from which the scalable performance of the system has been 
proved. In other side, Ramamurthy et al. (<span class="CitationRef"><a href="#CR58" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>)
 develop an efficient visual-content-based technique by the extraction 
of texture feature by the local binary pattern algorithm (LBP) and 
intensity feature. The extracted feature images are combined to form a 
single feature vector value, and it is then compared using Euclidean 
distance method to retrieve similar images from the database.</p><p id="Par62" class="Para">As a part of a comparative study, a number of statistic texture features have been proposed in (Swarnambiga and Vasuki <span class="CitationRef"><a href="#CR73" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2013</a></span>) with various similarity measures, in order to have higher retrieval accuracy. Rajakumar and Muttan (<span class="CitationRef"><a href="#CR56" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2013</a></span>)
 presented a CBIR system based on wavelet transform. They used the 
wavelet-based texture features. For retrieval phase, they used the 
Mahalanobis distance. Yogapriya and Ila (<span class="CitationRef"><a href="#CR83" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2013</a></span>)
 have introduced another medical CBIR system based on texture features 
by using local binary patterns (LBP) and the Euclidean distance.</p><p id="Par63" class="Para">Harishchandra et al. (<span class="CitationRef"><a href="#CR31" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2014</a></span>)
 proposed a new technique of the CBIR using the combination between 
color descriptors, extracted by color auto-correlogram, and texture 
descriptors, extracted by co-occurrence matrix. A modified Minkowski 
distance was used to measure the similarity between the query image and 
the database images.</p></section></section><section id="Sec7" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">2.3 </span>Discussion</h3><p id="Par64" class="Para">We
 have made a comprehensive review on the state of the art of medical and
 non-medical CBIR techniques. Most approaches found, in the literature, 
focused on two major aspects of CBIR: feature extraction and similarity 
measures. In terms of features, several types of features have been 
used. These features can be extracted either locally or globally. Most 
of these systems have a very similar architecture for browsing and 
archiving/indexing images comprising tools for the extraction of visual 
features, for the storage and efficient retrieval of these features, for
 distance measurements or similarity calculation and a type of graphical
 user interface (GUI). However, no contribution is at the present time 
about the integration of CBIR systems through medical social networks. 
An attempt to index and retrieve medical image content shared through 
the social network, using techniques of the content-based image 
retrieval community, becomes an important task due the huge volumes of 
uploaded images, in order to analyze and to benefit from our medical 
social network content. But, the main question we address is: Which 
features are suitable for the task of medical image retrieval? 
Currently, all existing features have limitations of describing medical 
images, according to our study, and none of existing features is 
powerful enough to represent the large variety of images. Common 
practice is to combine several types of features to represent as many 
images as possible. Indeed, it is hard to attain satisfactory retrieval 
results by using a single feature because, in general, an image contains
 various visual characteristics. Moreover, user interaction and 
relevance feedback are two other techniques that need to be more 
integrated into retrieval systems as this can help to lead to much 
better results. Image retrieval needs to be interactive and all the 
interaction needs to be exploited for delivering the best possible 
results. Our present study’s approach is about the integration of a 
robust medical image retrieval scheme with relevance feedback through a 
social network for collaboration, whose goals are teaching, learning 
(among medical residents and students), and diagnostics. In what 
follows, it is vital to explicitly describe this approach.</p></section></div></section><section id="Sec8" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading" data-role="collapsible-handle" tabindex="0"><span class="HeadingNumber">3 </span>Social network description and implementation<span class="section-icon"></span></h2><div class="content"><p id="Par65" class="Para">Social
 network is the study of social entities, as a group of people linked to
 one another by one or more common attributes, and their interactions 
and relationships. The interactions and relationships can be represented
 with a network or graph, where each node represents an actor and each 
link represents a relationship. A social network plays an important role
 as a support for the dissemination of information, ideas, and impact 
among its members. It may be a major selling point for patients looking 
for physicians. In fact, social networking has been proven effective in 
sharing knowledge and establishing communication among patients and 
physicians.</p><p id="Par66" class="Para">Our proposed social network, as many others described in the literature Gong and Sun (<span class="CitationRef"><a href="#CR27" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2011</a></span>) and Almansoori et al. (<span class="CitationRef"><a href="#CR5" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2011</a></span>),
 aims to allow patients and physicians to connect with each other by 
eliminating all geographical and time frontiers. Both users exploit it 
to seek advices and to share experiences related to medical images’ 
interpretations and diseases’ analysis.</p><p id="Par67" class="Para">Like
 in Facebook, patients and physicians need to be registered by creating 
profiles about themselves (detailed information). This step is very 
important in order to use our medical social network. Our social network
 site is a virtual place where registered patients upload their medical 
images to be commented by various registered physicians. Not only 
registered patients can upload their medical images, but also registered
 physicians have this right, especially medical students, in the 
objective to learn from this collaboration. Uploading and sharing images
 are not the only functionalities. Users can also share status, medical 
events, and any information related to medical activities.</p><p id="Par68" class="Para">The
 main objective, behind using our medical social network, is to enable 
patients and physicians to exchange information, share knowledge and 
experiences. It, also, gives to its members the opportunity to connect 
and communicate with others in need of support and encouragement related
 to different situations and health care problems. Consequently, the 
patient’s condition represents the heart of the health system justified 
by experts’ interpretations and analysis expressed by comments. Patients
 frequently place trust in peer recommendations on social network site 
making them key platforms to influence change. We used PHP (Hypertext 
Preprocessor), in the development of our social network, and MySQL for 
the database management. PHP is a server-side scripting language 
designed for web development but also used as a general-purpose 
programming language. It ensures many features that allow creating and 
managing an entire social network website. Pages were written in HTML, 
PHP, Ajax, and JavaScript, and they were designed using Dreamweaver.</p><div id="Par69" class="Para">In
 order to better explain our medical social functionalities, we describe
 in the following some interfaces of the social network:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li> <p id="Par70" class="Para">The
 first interface to use is the identification interface. Patients and 
physicians need to provide the registration process, as new users. The 
registration interface dedicated to physicians has more specificity 
because profiles have different structures.</p> </li><li> <p id="Par71" class="Para">After
 the identification, the basic functions and services of the social 
network are displayed in the main interface. Patients and physicians can
 perform different activities, especially, get access to posting, 
commenting functions, uploading images, etc.</p> </li><li> <p id="Par72" class="Para">In
 the image retrieval interface (the user interface components), patients
 and/or physicians try make a diagnosis for a new medical image, by 
searching to most similar image to a given query, according to the 
visual extracted features. Both need to complete a form for the visual 
features, in relation by a link to an online tool for features 
extraction. The same process will be treated by back-office functions 
implementing our approach.</p> </li><li> <p id="Par73" class="Para">To 
answer urgent questions and/or advices, synchronous communications 
between members (patients and physicians, physicians and physicians) are
 performed in the communication interface. Mixed posts performed by 
different physicians and patients, according to their opinions and 
questions, are displayed in the posting interface.</p> </li></ul></div> </div><p id="Par74" class="Para">The
 different interfaces of our social network allow an easier mapping for 
users, their intent and targeted functions. The most important that 
these interfaces describe the way of interaction between patients and 
physicians with the social network site and the way of interaction 
between patients and physicians with each other’s by exploiting existing
 functions. The social network content blocks are visually separated. 
This separation will help to organize pages content, and each element is
 well defined and presented separately, to succeed the interaction 
between patients and physicians. In fact, this separation makes the 
content understandable, easy to recognize and makes the content more 
reachable.</p><p id="Par75" class="Para">In order to improve the 
interaction between users (patients and physicians, physicians and 
physicians), our social network contains an advanced search function, 
allowing the organization of the connections between patients and 
physicians having common interests related to diseases and their 
analysis, interpretations, and treatments. This function supports users 
to rapidly find the content and contacts they are searching for.</p><p id="Par76" class="Para">Considering
 the recommendations of a design expert, the created interfaces are 
simple in terms of color scheme and graphics, for example in Facebook. 
The idea consists of using a few colors and the background is generally 
white. This management of color scheme helps the exploitation of our 
social network site by physicians and patients, makes the content well 
presented, and comments better seen and readable. Therefore, buttons and
 links are placed almost on every page of the social network. Some links
 are related to the navigation processes, and some others permit to 
users to regulate specific functions. Buttons are used to associate 
users to actions and to navigate among different pages; they are clear 
and more remarkable.</p><p id="Par77" class="Para">When incoming 
comments or messages appear on the social network sites, physicians and 
patients need to react to them in real time. For this reason, they 
should consider establishing their own social network presence. So, we 
assure our social network with a synchronous communication, in order to 
establishing interaction and sharing information. We, also, equipped our
 social network with a real-time update feature ensuring the delivery of
 updates (medical images uploading, physicians’ annotations, etc.) as 
soon as they are submitted. In order to encourage patients and 
physicians to exploit our social network site, many actions could be 
performed such as suggesting new friends, preferably in relation to 
medical activities, interests, events, and groups. This is performed to 
extend their social circles allowing implicitly the extension of the 
shared knowledge about medical images and the associated diseases.</p><p id="Par78" class="Para">The
 extraction of more knowledge, from various posts or comments, is 
possible by using mining tools. For this reason, our social network is 
equipped by mining tools such those presented in Xie et al. (<span class="CitationRef"><a href="#CR80" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2013</a></span>),
 where authors consider the emergence and pervasivity of online social 
networks that have enhanced web data with developing interactions and 
communities both at large scale and in real time.</p><p id="Par79" class="Para">The
 successful building of our social network needs to respect security 
aspects and to protect users’ privacy. Physicians must protect their own
 privacy, as well as that of their patients. To do it, we use privacy 
settings on our social network site to keep out everyone except patients
 or fellow physicians. We will also consider security aspects such those
 presented in Li (<span class="CitationRef"><a href="#CR44" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2014</a></span>),
 where the author proposed methods to secure health care social 
networking sites providing users with tools and services to easily 
establish contact with each other around shared problems and utilize the
 wisdom of masses to outbreak disease.</p></div></section><section id="Sec9" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading" data-role="collapsible-handle" tabindex="0"><span class="HeadingNumber">4 </span>The proposed methodology<span class="section-icon"></span></h2><div class="content"><section id="Sec10" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">4.1 </span>An overview</h3><div id="Par80" class="Para">Figure&nbsp;<span class="InternalRef"><a href="#Fig1">1</a></span> shows a structure of the conceptual design for the CBMIR method through the social network site. From Fig.&nbsp;<span class="InternalRef"><a href="#Fig1">1</a></span>,
 we can see that our methodology mechanism has five main components: 
query unit, extraction unit, retrieval, labeling, and learning unit. 
Compared with the traditional CBMIR without feedback, there are two more
 components which are labeling and learning, and they are the key 
contributions in the relevance feedback system. During the offline 
phase, it first extracts three kinds of visual features for each image 
in the image social network database <em class="EmphasisTypeItalic ">SNDB</em>,
 which are color, texture and shape features, then, stores these 
features into the feature database (the extraction component). Note that
 there are a large amount of methods for these three kinds of features’ 
extractions. Accordingly, it is a very important to select efficient 
features and to choose effective methods to extract them, in order to 
perform image retrieval. Indeed, the combination between visual 
features, in order to construct a single feature’s vector, will give 
more precision to the retrieval process.<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO1"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs13278-016-0362-9/MediaObjects/13278_2016_362_Fig1_HTML.gif" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="13278_2016_362_Fig1_HTML.gif" alt=""></a> </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig.&nbsp;1</span> <p class="SimplePara">Proposed methodology pipeline</p> </div></figcaption></figure> </div><p id="Par81" class="Para">During the image retrieval, as shown in Fig.&nbsp;<span class="InternalRef"><a href="#Fig1">1</a></span>,
 three visual features of a query image are first extracted by using 
three feature extraction methods which are determined in the offline 
phase (the query component). The query image represents the undiagnosed 
image proposed by physicians, specialists, or patients. They decide 
whether they want to perform a similarity query to support their 
diagnosis or to improve their certainty about an image analysis.</p><p id="Par82" class="Para">Then,
 in the matching process, the system executes a similarity query using 
the undiagnosed image as the query center. The similarity, between the 
query image and each image in <em class="EmphasisTypeItalic ">SNDB</em>,
 can be computed by using a corresponding distance formula which is 
decided among those existing in the literature (in our case the 
Euclidian distance). Finally, the CBMIR method outputs several most 
similar images to the given one according to the above similarity (the 
retrieval component), allowing physicians or patients to take advantage 
of previous analysis of them based on existing comments and posts.</p><div id="Par83" class="Para">In
 order to satisfy user, a strategy employed, in our CBMIR system, to 
obtain a better approximation of the user’s expectations and preferences
 is the relevance feedback (RF) (labeling and learning components). 
Relevance feedback technique allows the user to judge the returned 
answers, informing which images are relevant according to the image 
query. Thus, it is a real-time learning strategy that adapts the answer 
from a retrieval system exploring the user interaction, leading to 
higher precision and improved query refinement. The user (physician or 
patient) indicates the irrelevant images, which are eliminated from the 
interface, from the relevant ones. If he is not satisfied with the 
results (the initial images retrieved) and wants to refine the search, 
it can be done through a relevance feedback iteration (RF loop). The 
users were stimulated to try a relevance feedback method [the Rocchio 
algorithm in this experiment (Rocchio <span class="CitationRef"><a href="#CR59" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">1971</a></span>)]
 to perform a refined query and analyze the new result set. They could 
use as many relevance feedback cycles, until they have satisfactory 
results, as needed to improve the accuracy and the certainty of their 
diagnosis. Finally, users finalize search. Users have, also, the choice 
to finalize search from the beginning if they are really satisfied. The 
algorithm, describing the steps of the proposed method in generally, is 
as follows: <figure class="Figure" id="Figa"><div class="MediaObject" id="MO2"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs13278-016-0362-9/MediaObjects/13278_2016_362_Figa_HTML.gif" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="13278_2016_362_Figa_HTML.gif" alt=""></a> </div></figure> </div></section><section id="Sec11" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">4.2 </span>Feature extraction</h3><p id="Par84" class="Para">The
 first issue, in image retrieval scheme, is to extract the features of 
the image efficiently and, then, represent them in a particular form to 
be used effectively in the matching of images. As an unstructured array 
of pixels, the first step in semantic understanding of images is to 
extract efficient and effective visual features from these pixels. 
Appropriate feature representation significantly improves the 
performance of the semantic learning techniques (Zhang et al. <span class="CitationRef"><a href="#CR89" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>).
 In the following, we present, in detail, various feature extraction 
techniques used in this paper. Both global and region-based image 
representations are used in the existing image retrieval scheme.</p><section id="Sec12" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">4.2.1 </span>Color features</h4><div id="Par85" class="Para">Color-based
 image retrieval is the most basic and most important method for CBIR. 
Color features are the most intuitive and most obvious image features. 
It is also an important feature of perception. Comparing with other 
image features such as texture and shape, etc., color features are very 
stable and robust. It is not sensitive to rotation, translation, and 
scale changes (Yue et al. <span class="CitationRef"><a href="#CR84" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2010</a></span>).
 Moreover, the color features calculation is relatively simple. In the 
proposed image retrieval method, we capture the characteristics of an 
image’s color contents by using the global gray-level histogram and 
color moments. These features are calculated by converting each pixel to
 gray scale using the following formula (Yue et al. <span class="CitationRef"><a href="#CR84" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2010</a></span>):<div id="Equ1" class="Equation EquationMathjax"><div class="EquationContent">$${\text{Grey}} = 0.299\,{\text{Red}} + 0.587\,{\text{Green}} + 0.114\,{\text{Blue}}$$</div> <div class="EquationNumber">(1)</div></div> </div><section id="Sec13" tabindex="-1" class="Section4 RenderAsSection4"><h5 class="Heading"><span class="HeadingNumber">4.2.1.1 </span>Global gray-level histogram</h5><div id="Par86" class="Para">The gray-level histogram describes the gray-level distribution of an image (Goh et al. <span class="CitationRef"><a href="#CR26" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2005</a></span>).
 The gray-level space needs to be divided into several small ranges in 
order to calculate the gray-level histogram. Each interval is regarded 
as a bin. Thus, the gray-level is quantized. The gray-level histogram 
can be calculated through counting pixels where the gray-level fall into
 each interval (Yue et al. <span class="CitationRef"><a href="#CR84" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2010</a></span>).
 It quantizes a gray-level space into different bins and counts the 
frequency of pixels belonging to each gray-level bin. According to the 
study proposed by Zhang et al. (<span class="CitationRef"><a href="#CR89" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>),
 this feature is robust to translation and rotation changes. However, a 
gray-level histogram does not tell pixels’ spatial information. In 
addition, the dimension of a histogram is usually very high. The 
gray-level histogram defined the mathematical equations given in 
Eqs.&nbsp;<span class="InternalRef"><a href="#Equ1">1</a></span> and <span class="InternalRef"><a href="#Equ2">2</a></span> (Zeyad et al. <span class="CitationRef"><a href="#CR85" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2014</a></span>), where <em class="EmphasisTypeItalic ">I (x, y)</em> represents the probability of pixel falling into the bin:<div id="Equ2" class="Equation EquationMathjax"><div class="EquationContent">$$hc\left(
 {\text{m}} \right) = \frac{1}{XY} \mathop \sum \limits_{x = 0}^{X - 1} 
\mathop \sum \limits_{y = 0}^{Y - 1} I\left( {x,y} \right)$$</div> <div class="EquationNumber">(2)</div></div> <div id="Equ3" class="Equation EquationMathjax"><div class="EquationContent">$$I\left(
 {x, \, y} \right) = \left\{ {\begin{array}{*{20}l} {1\quad 
{\text{if}}\,I \left( {{\text{x}}, {\text{y}}} 
\right)\,{\text{in}}\,{\text{bin}}\,m} \hfill \\ {0\quad 
{\text{otherwise}}} \hfill \\ \end{array} } \right.$$</div> <div class="EquationNumber">(3)</div></div> </div></section><section id="Sec14" tabindex="-1" class="Section4 RenderAsSection4"><h5 class="Heading"><span class="HeadingNumber">4.2.1.2 </span>Color moments</h5><div id="Par87" class="Para">Color moments are one of the simplest features. They are used in many retrieval systems (Fazal and Baharum <span class="CitationRef"><a href="#CR22" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2013</a></span>; Afifi and Ashour <span class="CitationRef"><a href="#CR1" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>).
 The common moments are mean, standard deviation, skewness, and 
kurtosis. They have been proved to be efficient and effective in 
representing color distributions of images. In our case, we are 
interested in representing gray-value distributions of images. These 
features are useful when they are calculated for region or object. 
According to Zhang et al. (<span class="CitationRef"><a href="#CR89" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>),
 these features are compact and robust. However, the moments are not 
enough to represent all the color information of an image. If the value 
of the <em class="EmphasisTypeItalic ">i</em>th gray-value channel at the <em class="EmphasisTypeItalic ">j</em>th image pixel is <em class="EmphasisTypeItalic ">p</em> <sub> <em class="EmphasisTypeItalic ">i j</em> </sub>, then the gray-value moments are as follows:<div id="Equ4" class="Equation EquationMathjax"><div class="EquationContent">$${\text{Mean}}:m_{i} = \frac{1}{N}\mathop \sum \limits_{j = 1}^{N} p_{ij}$$</div> <div class="EquationNumber">(4)</div></div> <div id="Equ5" class="Equation EquationMathjax"><div class="EquationContent">$${\text{Standard}}\,{\text{deviation}}:\sigma_{i}
 = \sqrt[2]{{\frac{1}{N} \mathop \sum \limits_{j = 1}^{N} \left( {p_{ij}
 - m_{i} } \right)^{2} }}$$</div> <div class="EquationNumber">(5)</div></div> <div id="Equ6" class="Equation EquationMathjax"><div class="EquationContent">$${\text{Skewness}}:s_{i} = \sqrt[3]{{\frac{1}{N} \mathop \sum \limits_{j = 1}^{N} \left( {p_{ij} - m_{i} } \right)^{3} }}$$</div> <div class="EquationNumber">(6)</div></div> <div id="Equ7" class="Equation EquationMathjax"><div class="EquationContent">$${\text{Kurtosis}}:k_{i} = \sqrt[4]{{\frac{1}{N} \mathop \sum \limits_{j = 1}^{N} \left( {p_{ij} - m_{i} } \right)^{4} }} .$$</div> <div class="EquationNumber">(7)</div></div> </div></section></section><section id="Sec15" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">4.2.2 </span>Texture features</h4><p id="Par88" class="Para">Texture
 is another important image feature. While color is usually a pixel 
property, texture can only be measured from a group of pixels. Texture 
has been well studied in image processing and computer vision area 
(Zhang et al. <span class="CitationRef"><a href="#CR89" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>).
 A number of techniques have been proposed to extract texture features. 
For our work, the texture feature is extracted by using spatial texture 
feature extraction method. Statistical texture features (Haralick 
features) characterize texture as a measure of low-level statistics of 
gray-level images. Haralick features are energy, entropy, contrast, 
correlation, and inverse difference moment. These features are derived 
from the gray-level co-occurrence matrix (GLCM). According to the study 
of Zhang et al. (<span class="CitationRef"><a href="#CR89" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>),
 statistical features are intuitive, compact, and robust because they 
are derived from large support. However, they are not sufficient to 
describe the large variety of textures.</p><div id="Par89" class="Para">According to Zeyad et al. (<span class="CitationRef"><a href="#CR85" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2014</a></span>), the co-occurrence matrix <em class="EmphasisTypeItalic ">C</em> (<em class="EmphasisTypeItalic ">i,j</em>) is used to compute the co-occurrence of pixels with gray values <em class="EmphasisTypeItalic ">i</em> and <em class="EmphasisTypeItalic ">j</em> as a given distance that is the polar coordinates (<em class="EmphasisTypeItalic ">d,)</em> with discrete length and orientation. In practice, <em class="EmphasisTypeItalic ">θ</em> takes the values 0°, 45°, 90°, 135°, 180°, 225°, 270°, and 315°, respectively. The co-occurrence matrix <em class="EmphasisTypeItalic ">C</em> (<em class="EmphasisTypeItalic ">i, j</em>) is defined as follows according to Eq.&nbsp;<span class="InternalRef"><a href="#Equ7">7</a></span>.
<div id="Equ8" class="Equation EquationMathjax"><div class="EquationContent">$$C_{{\theta^{
 \circ } d}} \left( {i, \, j} \right) = \# \left\{ 
{\begin{array}{*{20}l} {1,\quad {\text{if}}\;I\left( {x_{1} , \, x_{2} }
 \right) = i,\;{\text{and }}I(x_{1} + d\cos \theta ,y_{1} + d\sin \theta
 ) = i} \hfill \\ {0,\quad {\text{otherwise}}} \hfill \\ \end{array} } 
\right\}$$</div> <div class="EquationNumber">(8)</div></div>where # denotes the number of elements in the set, d represents the distance between gray-level value <em class="EmphasisTypeItalic ">i</em> and <em class="EmphasisTypeItalic ">j</em> of an image, <em class="EmphasisTypeItalic ">i, j</em>&nbsp;=&nbsp;0.255 the number of possible gray levels in the image, while the co-occurrence matrix <em class="EmphasisTypeItalic ">C</em>(<em class="EmphasisTypeItalic ">i, j</em>) dimension is <em class="EmphasisTypeItalic ">M</em>&nbsp;×&nbsp;<em class="EmphasisTypeItalic ">N</em>. The co-occurrence matrix measures can be calculated as follows:<div id="Equ9" class="Equation EquationMathjax"><div class="EquationContent">$${\text{Energy}}:E_{\text{ne}}
 = \sum\nolimits_{i} {\sum\nolimits_{j} {C^{2}_{{\theta^{ \circ } ,d}} 
\left( {i, \, j} \right)} } \,$$</div> <div class="EquationNumber">(9)</div></div> <div id="Equ10" class="Equation EquationMathjax"><div class="EquationContent">$${\text{Entropy:}}\;E_{\text{nt}}
 = - \, \sum\nolimits_{i} {\sum\nolimits_{j} {C_{{\theta^{ \circ } d}} 
\left( {i, \, j} \right)\log C_{{\theta^{ \circ } d}} \left( {i, \, j} 
\right)} }$$</div> <div class="EquationNumber">(10)</div></div> <div id="Equ11" class="Equation EquationMathjax"><div class="EquationContent">$${\text{Contrast:}}\;C_{\text{on}}
 = \sum\nolimits_{i} {\sum\nolimits_{j} {\left( {i - j} \right)^{2} 
C_{{\theta^{ \circ } ,d}} \left( {i,\,j} \right)} } \,$$</div> <div class="EquationNumber">(11)</div></div> <div id="Equ12" class="Equation EquationMathjax"><div class="EquationContent">$${\text{Correlation}}:C_{\text{or}}
 = {{\left( {\sum\nolimits_{i} {\sum\nolimits_{j} {C_{{\theta^{ \circ } 
,d}} \left( {i, \, j} \right) - \mu_{x} \mu_{y} } } } \right)} 
\mathord{\left/ {\vphantom {{\left( {\sum\nolimits_{i} 
{\sum\nolimits_{j} {C_{{\theta^{ \circ } ,d}} \left( {i, \, j} \right) -
 \mu_{x} \mu_{y} } } } \right)} {\sigma_{x} \sigma_{y} }}} \right. 
\kern-0pt} {\sigma_{x} \sigma_{y} }}$$</div> <div class="EquationNumber">(12)</div></div> <div id="Equ13" class="Equation EquationMathjax"><div class="EquationContent">$${\text{Inverse
 difference moment}}:I_{\text{dm}} = \sum\nolimits_{i} 
{\sum\nolimits_{j} {(C_{{\theta^{ \circ } ,d}} \left( {i, \, j} 
\right)/|i - j|^{2} } )} ,\quad i \ne j$$</div> <div class="EquationNumber">(13)</div></div>where, <em class="EmphasisTypeItalic ">µ</em> <sub> <em class="EmphasisTypeItalic ">x</em> </sub> <em class="EmphasisTypeItalic ">µ</em> <sub> <em class="EmphasisTypeItalic ">y</em> </sub> represents the means and <em class="EmphasisTypeItalic ">σ</em> <sub> <em class="EmphasisTypeItalic ">x</em> </sub> <em class="EmphasisTypeItalic ">σ</em> <sub> <em class="EmphasisTypeItalic ">y</em> </sub> represents the standard deviations. They are defined as follows:<div id="Equ14" class="Equation EquationMathjax"><div class="EquationContent">$$\mu_{x} = \sum\nolimits_{i} i \sum\nolimits_{j} {C\left( {i, \, j} \right)}$$</div> <div class="EquationNumber">(14)</div></div> <div id="Equ15" class="Equation EquationMathjax"><div class="EquationContent">$$\mu_{y} = \sum\nolimits_{j} j \sum\nolimits_{i} {C\left( {i, \, j} \right)}$$</div> <div class="EquationNumber">(15)</div></div> <div id="Equ16" class="Equation EquationMathjax"><div class="EquationContent">$$\sigma_{x} = \sum\nolimits_{i} {\left( {i - \mu_{x} } \right)^{2} } \, \sum\nolimits_{j} {C\left( {i, \, j} \right)}$$</div> <div class="EquationNumber">(16)</div></div> <div id="Equ17" class="Equation EquationMathjax"><div class="EquationContent">$$\sigma_{x} = \sum\nolimits_{i} {\left( {j - \mu_{y} } \right)^{2} } \sum\nolimits_{i} {C\left( {i, \, j} \right)}$$</div> <div class="EquationNumber">(17)</div></div> </div></section><section id="Sec16" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">4.2.3 </span>Shape features</h4><div id="Par91" class="Para">Shape
 is an important and most powerful feature used for images’ indexing and
 retrieval. Shape features have been employed for image retrieval in 
many applications. A number of simple and statistical region shape 
descriptors are commonly used in our work, including, area, circularity,
 eccentricity, roundness, and convexity. The area-based descriptor is 
used in a number of works (Mezaris et al. <span class="CitationRef"><a href="#CR49" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2005</a></span>; Goh et al. <span class="CitationRef"><a href="#CR26" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2005</a></span>). Circularity is used in Song <span class="CitationRef"><a href="#CR69" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>; Stojmenovic et al. <span class="CitationRef"><a href="#CR71" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2013</a></span>. Circularity measures the ratio of area to boundary. In Stojmenovic and Nayak <span class="CitationRef"><a href="#CR70" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2008</a></span>,
 eccentricity or elongation is also used in addition to area. 
Eccentricity is the ratio of the length of the major axis to that of 
minor axis. Roundness and convexity are used in Song <span class="CitationRef"><a href="#CR69" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>.
 Roundness measures the ratio of area to the major axis, and convexity 
is the ratio of the area to the convex area. According to the study of 
Zhang et al. (<span class="CitationRef"><a href="#CR89" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>),
 Individual simple shape descriptors are not robust. Therefore, they are
 normally combined to create a more effective shape descriptor. These 
features can be calculated as follows:<div id="Equ18" class="Equation EquationMathjax"><div class="EquationContent">$${\text{Circularity}}\,\left({\text{compactness}} \right):C_{\text{irc}} = 4\pi \times \left( {\frac{A}{{p^{2} }}} \right)$$</div> <div class="EquationNumber">(18)</div></div> <div id="Equ19" class="Equation EquationMathjax"><div class="EquationContent">$${\text{Elongation}}\;\left( {\text{aspect ratio}} \right):E_{\text{lon}} = \frac{{M_{\text{A}} }}{{m_{\text{A}} }}$$</div> <div class="EquationNumber">(19)</div></div> <div id="Equ20" class="Equation EquationMathjax"><div class="EquationContent">$${\text{Roundness}}:R_{\text{oun}} = \frac{4A}{{\pi M_{\text{A}}^{2} }} .$$</div> <div class="EquationNumber">(20)</div></div> <div id="Equ21" class="Equation EquationMathjax"><div class="EquationContent">$${\text{Solidity}}\,\left( {\text{convexity}} \right):C_{\text{onv}} = \frac{A}{{C_{A} }}$$</div> <div class="EquationNumber">(21)</div></div>where <em class="EmphasisTypeItalic ">A</em> represents the area, <em class="EmphasisTypeItalic ">P</em> represents the perimeter, <em class="EmphasisTypeItalic ">M</em> <sub>A</sub> is the major axis, <em class="EmphasisTypeItalic ">m</em> <sub>A</sub> is the minor axis and <em class="EmphasisTypeItalic ">C</em> <sub> <em class="EmphasisTypeItalic ">A</em> </sub> is the convex.</div><div id="Par92" class="Para">After
 the calculation of these features (color, texture and shape features), 
these values are combined to get a feature vector. The feature vectors 
(FVs<sub>idb</sub>) of all the images are constructed and stored to create features database. The feature vector FV<sub>qi</sub>
 of the user query is also constructed in the same way and compared with
 the feature vectors of the database for similarity and searching of 
relevant images. The combination between features represents a solution 
to exceed limits of each single feature. The algorithm, describing the 
indexation phase of the proposed method with more specification about 
the extracted feature, is as follows:<figure class="Figure" id="Figb"><div class="MediaObject" id="MO24"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs13278-016-0362-9/MediaObjects/13278_2016_362_Figb_HTML.gif" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="13278_2016_362_Figb_HTML.gif" alt=""></a> </div></figure> </div></section></section><section id="Sec17" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">4.3 </span>Similarity measure</h3><div id="Par93" class="Para">Once features database (FV<sub>idb</sub>) of stored images is created, and the feature vector of the query image (FV<sub>qi</sub>) is computed, as shown in Fig.&nbsp;<span class="InternalRef"><a href="#Fig1">1</a></span>,
 similarity measurement presents the second issue. To measure the 
similarity, the difference is calculated by using one of distance 
metrics. We used the Euclidean distance. Euclidean Distance is used to 
measure the similarity between two images with N-dimensional feature 
vector. The vectors of the images with small distances are most similar 
to the query images (the top 10 retrieved images in our case). We 
suppose that FV<sub>qi</sub>&nbsp;=&nbsp;(<em class="EmphasisTypeItalic ">f</em> <sub> <em class="EmphasisTypeItalic ">q</em>0</sub>, <em class="EmphasisTypeItalic ">f</em> <sub> <em class="EmphasisTypeItalic ">q</em>1</sub>,…, <em class="EmphasisTypeItalic ">f</em> <sub> <em class="EmphasisTypeItalic ">qN</em>−1</sub>) and FV<sub>idb</sub>&nbsp;=&nbsp;(<em class="EmphasisTypeItalic ">f</em> <sub>db0</sub>, <em class="EmphasisTypeItalic ">f</em> <sub>db1</sub>,…, <em class="EmphasisTypeItalic ">f</em> <sub>db<em class="EmphasisTypeItalic ">N</em>−1</sub>), so the Euclidean distance is calculated by using the following formula (Fazal and Baharum <span class="CitationRef"><a href="#CR22" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2013</a></span>; Swarnambiga and Vasuki <span class="CitationRef"><a href="#CR73" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2013</a></span>):<div id="Equ22" class="Equation EquationMathjax"><div class="EquationContent">$${\text{Euclidiean}}\,{\text{distance}} = \sqrt {\sum \left( {f_{{{\text{qi}} }} - f_{{{\text{dbi}} }} } \right)^{2} } .$$</div> <div class="EquationNumber">(22)</div></div> </div><div id="Par94" class="Para">The
 algorithm, describing the retrieval phase of the proposed method with 
more specification about the metric used to calculate the similarity, is
 as follows:<figure class="Figure" id="Figc"><div class="MediaObject" id="MO26"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs13278-016-0362-9/MediaObjects/13278_2016_362_Figc_HTML.gif" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="13278_2016_362_Figc_HTML.gif" alt=""></a> </div></figure> </div></section><section id="Sec18" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">4.4 </span>Relevance feedback</h3><div id="Par95" class="Para">Basically,
 the relevance feedback technique is composed of three steps. In the 
first step, the system retrieves the most similar images according to 
the initial query. During the second step, the user (patient or 
physician) guides the search process, judging the returned images, and 
weighing the returned images based on a relevance degree (relevant or 
irrelevant). In the third step, the system captures the user’s 
expectation based on the performed feedback and automatically adjusts 
the further queries based on the user’s informed relevance. The second 
and third steps are repeated until the user is satisfied with the 
results. As the system captures the user’s intention when a new query is
 performed, the resulting set of images can be continually improved 
until the gain flattens, according to the iterative learning process 
(Liu et al. <span class="CitationRef"><a href="#CR45" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2007</a></span>). We performed the Rocchio algorithm in this experiment (Rocchio <span class="CitationRef"><a href="#CR59" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">1971</a></span>). We use the query point movement (QPM) (Doulamis and Doulamis <span class="CitationRef"><a href="#CR21" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2006</a></span>) technique to perform RF. Rocchio’s formula is the mostly used technique to iteratively improve this estimation:<div id="Equ23" class="Equation EquationMathjax"><div class="EquationContent">$$Q^{\prime
 } = \alpha Q + \beta \left( {\frac{1}{{N_{{R^{\prime } }} }}\mathop 
\sum \limits_{{i \in D_{R}^{\prime } }} D_{i} } \right) - \gamma \left( 
{\frac{1}{{N_{{N^{\prime } }} }}\mathop \sum \limits_{{i \in 
D_{N}^{\prime } }} D_{i} } \right)$$</div> <div class="EquationNumber">(23)</div></div>where <em class="EmphasisTypeItalic ">Q</em> is the original query (the initial feature vector), <em class="EmphasisTypeItalic ">Q′</em> is the updated query (the resultant feature vector), <span class="InlineEquation" id="IEq1">\(D_{R}^{\prime }\)</span> and <span class="InlineEquation" id="IEq2">\(D_{N}^{\prime }\)</span> are respectively the positive and negative examples given by the user, <em class="EmphasisTypeItalic ">N</em> <sub> <em class="EmphasisTypeItalic ">R</em>′</sub> and <em class="EmphasisTypeItalic ">N</em> <sub> <em class="EmphasisTypeItalic ">N</em>′</sub> are the number of relevant and irrelevant examples in <span class="InlineEquation" id="IEq3">\(D_{R}^{\prime }\)</span> and <span class="InlineEquation" id="IEq4">\(D_{N}^{\prime }\)</span>, and finally <em class="EmphasisTypeItalic ">α</em>, <em class="EmphasisTypeItalic ">β,</em> and <em class="EmphasisTypeItalic ">γ</em>
 are weighting factors, experimentally obtained, to tune the relevance 
feedback factors. The scenario for the relevance feedback steps in our 
methodology is as follows:<figure class="Figure" id="Figd"><div class="MediaObject" id="MO28"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs13278-016-0362-9/MediaObjects/13278_2016_362_Figd_HTML.gif" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="13278_2016_362_Figd_HTML.gif" alt=""></a> </div></figure> </div></section><section id="Sec19" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">4.5 </span>Experimental results</h3><p id="Par96" class="Para">Experiments
 have been carried out to validate the efficiency of the proposed model 
to execute similarity queries, through a graphical user interface (GUI) 
belongs to our social network. We explain the results, comment on them, 
and compare our system with other existing systems.</p><section id="Sec20" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">4.5.1 </span>Feature extraction tool</h4><p id="Par97" class="Para">Features
 extraction can be achieved with the help of certain image manipulation 
tools such as ImageJ. This tool helps us to efficiently extract various 
features to be used in our application for some specific searches. 
ImageJ<sup><a href="#Fn14" id="Fn14_source">14</a></sup> is a public 
domain Java image processing and analysis program inspired by NIH Image 
for the Macintosh. It runs, either as an online applet (used for the 
online features extraction), integrated by a link in our social network,
 or as a downloadable application (used for the offline features 
extraction). It can display, edit, analyze, process, and save images. It
 can calculate pixel value statistics of user-defined selections. ImageJ
 was designed with an open architecture that provides extensibility via 
Java plug-in. User-written plugins make it possible to solve almost any 
image processing or analysis problem. The various features, which are 
described in Sect.&nbsp;<span class="InternalRef"><a href="#Sec17">4.3</a></span>, are extracted by using ImageJ, and then the feature’s vector of each image is constructed and stored in the database.</p></section><section id="Sec21" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">4.5.2 </span>Data test and evaluation criteria</h4><div id="Par99" class="Para">The
 relevance of our approach was evaluated on preselected medical images, 
which are supplied by internal physicians and residents during their 
training, diagnostic and images analysis in relation to patients’ states
 and clinical cases, at Charles Nicolle Hospital in Tunis. This 
collection contains 500 medical images in the JPEG format. We perform 
experiments over images from this collection, classified in five 
categories in generally: Thorax, abdomen, lumbar spine, pelvis, skull, 
and others. Every database image is of size 865&nbsp;×&nbsp;705 or 
787&nbsp;×&nbsp;787 pixels. For evaluation, we use all the images in the
 database. Figure&nbsp;<span class="InternalRef"><a href="#Fig2">2</a></span> illustrates example of images in the database.<figure class="Figure" id="Fig2"><div class="MediaObject" id="MO29"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs13278-016-0362-9/MediaObjects/13278_2016_362_Fig2_HTML.jpg" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="13278_2016_362_Fig2_HTML.jpg" alt=""></a> </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig.&nbsp;2</span> <p class="SimplePara">Examples of images in the database</p> </div></figcaption></figure> </div><div id="Par100" class="Para">We
 evaluate the performance of this approach using the metric of 
precision, the recall, AP (average precision), and MAP (mean average 
precision) (Manning et al. <span class="CitationRef"><a href="#CR47" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2008</a></span>).
 The precision is defined as the ratio of the number of retrieved 
relevant images to the total number of retrieved images. We denote to 
the precision by <em class="EmphasisTypeItalic ">P</em>. The equation of the precision is:<div id="Equ24" class="Equation EquationMathjax"><div class="EquationContent">$$P
 = 
\frac{{{\text{Number}}\,{\text{of}}\,{\text{relevent}}\,{\text{images}}\,{\text{retreived}}}}{{{\text{Total}}\,{\text{number}}\,{\text{of}}\,{\text{images}}\,{\text{retreived}}
 }} .$$</div> <div class="EquationNumber">(24)</div></div> </div><div id="Par101" class="Para">The
 recall is defined as the ratio of the number of retrieved relevant 
images to the total number of relevant images in the database. We denote
 to the recall by <em class="EmphasisTypeItalic ">R</em>. The equation of the recall is:<div id="Equ25" class="Equation EquationMathjax"><div class="EquationContent">$$R
 = 
\frac{{{\text{Number}}\,{\text{of}}\,{\text{relevent}}\,{\text{images}}\,{\text{retreived}}}}{{{\text{Total}}\,{\text{number}}\,{\text{of}}\,{\text{relevent}}\,{\text{images}}\,{\text{in}}\,{\text{the}}\,{\text{DB
 }}}}$$</div> <div class="EquationNumber">(25)</div></div> </div><p id="Par102" class="Para">The
 MAP represents the quality of a system based on different levels of 
recall. If the precision score is 1.0, this means that every image 
retrieved by a search is relevant, but we do not know whether the search
 retrieves all the images relevant to the query. If the recall score is 
1.0, this means that all relevant images are retrieved by the search, 
but we do not know whether the number of irrelevant images were also 
retrieved (Afifi and Ashour <span class="CitationRef"><a href="#CR1" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>).</p></section><section id="Sec22" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">4.5.3 </span>Evaluation of the image retrieval scheme</h4><div id="Par103" class="Para">We
 perform some experiments to check the retrieval effectiveness of the 
proposed method without the relevance feedback process. The objective is
 to validate the efficiency of the retrieval model by the fusion between
 different features, which are described above, and compare its 
performance with other existing state-of-the-art methods in the 
literature. Some images were selected randomly from each category to 
test the system and retrieve 10 images similar to the query image. An 
example of a test is presented, as can be seen from Fig.&nbsp;<span class="InternalRef"><a href="#Fig3">3</a></span>, by selecting a random image from the pelvis class.<figure class="Figure" id="Fig3"><div class="MediaObject" id="MO32"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs13278-016-0362-9/MediaObjects/13278_2016_362_Fig3_HTML.jpg" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="13278_2016_362_Fig3_HTML.jpg" alt=""></a> </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig.&nbsp;3</span> <p class="SimplePara">Pelvis query and the top 10 retrieved images</p> </div></figcaption></figure> </div><p id="Par104" class="Para">The
 operation of the scheme in a precision–recall curve is depicted as a 
traditional graph, which delivers a significant outcome after the 
database is known and was practiced by some past systems. The 
application of the precision–recall curve enables the evaluation of the 
proposed scheme. The option of some images at random from each class in 
the database was to use as queries to compute the precision and recall 
data.</p><div id="Par105" class="Para">We note that our approach works 
well on each image in different classes. For every image, the precision 
of the retrieval solution is achieved by increasing the retrieved images
 amount. Figure&nbsp;<span class="InternalRef"><a href="#Fig4">4</a></span> presents the precision–recall graph for query images among different classes.<figure class="Figure" id="Fig4"><div class="MediaObject" id="MO33"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs13278-016-0362-9/MediaObjects/13278_2016_362_Fig4_HTML.gif" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="13278_2016_362_Fig4_HTML.gif" alt=""></a> </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig.&nbsp;4</span> <p class="SimplePara">Precision–recall curves</p> </div></figcaption></figure> </div><div id="Par106" class="Para">Furthermore,
 we investigated the influence of the proposed set of features on our 
integrated CBIR system performance. One of the testing processes is 
divided into four phases. In the first phase, the proposed method will 
use the randomly selected images to retrieve similar images from the 
database, using the color feature only. In the second phase, the 
proposed method will retrieve similar images from the database, using 
the texture feature only. In the third phase, the proposed method will 
retrieve similar images from the database, using the shape feature only.
 In the fourth phase, the proposed method will retrieve images similar 
to the input image according to the color, texture and shape features, 
according to our methodology. After many tests, it is clear from 
Table&nbsp;<span class="InternalRef"><a href="#Tab1">1</a></span> that 
proposed method works very well when we use both color, texture and 
shape feature to retrieve images similar to the input image. It shows 
the experimental results for each category, by calculating the average 
precision based on the computation of the precision for different tests.<div id="Tab1" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table&nbsp;1</span> <p class="SimplePara">Comparison of average precision obtained by different retrieval techniques</p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1 align-left"><col class="tcol2 align-char"><col class="tcol3 align-char"><col class="tcol4 align-char"><col class="tcol5 align-char"><col class="tcol6 align-char"></colgroup><thead><tr><th>&nbsp;</th><th> <p class="SimplePara">Pelvis</p> </th><th> <p class="SimplePara">Skull</p> </th><th> <p class="SimplePara">Abdomen</p> </th><th> <p class="SimplePara">Lumbar spine</p> </th><th> <p class="SimplePara">Thorax</p> </th></tr></thead><tbody><tr><td> <p class="SimplePara">Only color</p> </td><td> <p class="SimplePara">0.301</p> </td><td> <p class="SimplePara">0.328</p> </td><td> <p class="SimplePara">0.285</p> </td><td> <p class="SimplePara">0.450</p> </td><td> <p class="SimplePara">0.270</p> </td></tr><tr><td> <p class="SimplePara">Only texture</p> </td><td> <p class="SimplePara">0.500</p> </td><td> <p class="SimplePara">0.700</p> </td><td> <p class="SimplePara">0.350</p> </td><td> <p class="SimplePara">0.386</p> </td><td> <p class="SimplePara">0.500</p> </td></tr><tr><td> <p class="SimplePara">Only shape</p> </td><td> <p class="SimplePara">0.600</p> </td><td> <p class="SimplePara">0.730</p> </td><td> <p class="SimplePara">0.450</p> </td><td> <p class="SimplePara">0.550</p> </td><td> <p class="SimplePara">0.650</p> </td></tr><tr><td> <p class="SimplePara">Our method</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.950</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.890</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.838</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.730</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.698</em> </p> </td></tr></tbody></table></div></div> </div><div id="Par107" class="Para">Also, Fig.&nbsp;<span class="InternalRef"><a href="#Fig5">5</a></span>
 shows a graph to visualize which method has more accurate results. This
 figure demonstrates also that using the combination of color, texture 
and shape feature to represent the image and retrieve images similar to 
it, has more accuracy compared with only color feature or only texture 
feature or only shape feature. The results prove that the precision of 
the system based on all proposed integrated features is higher than for 
each feature separately.<figure class="Figure" id="Fig5"><div class="MediaObject" id="MO34"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs13278-016-0362-9/MediaObjects/13278_2016_362_Fig5_HTML.gif" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="13278_2016_362_Fig5_HTML.gif" alt=""></a> </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig.&nbsp;5</span> <p class="SimplePara">Average
 precision comparisons between color moments, texture features, shape 
features, and combination of color, texture and shape</p> </div></figcaption></figure> </div><div id="Par108" class="Para">Another
 testing process is divided into three phases. In the first phase, the 
proposed method will use the combination between color and texture 
features to retrieve similar images from the database. In the second 
phase, the proposed method will retrieve similar images from the 
database, using the combination between color and shape features. In the
 third phase, the proposed method will retrieve similar images from the 
database using the combination between texture and shape features. We 
compare these results with results performed by the combination of all 
features. It is clear, after many tests, from Table&nbsp;<span class="InternalRef"><a href="#Tab2">2</a></span>
 that proposed method works also very well when we use both color, 
texture and shape feature to retrieve images similar to the input image.
 Figure&nbsp;<span class="InternalRef"><a href="#Fig6">6</a></span> shows a graph to demonstrate which method has more accurate results.<div id="Tab2" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table&nbsp;2</span> <p class="SimplePara">Comparison of average precision obtained by different retrieval techniques</p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1 align-left"><col class="tcol2 align-char"><col class="tcol3 align-char"><col class="tcol4 align-char"><col class="tcol5 align-char"><col class="tcol6 align-char"></colgroup><thead><tr><th>&nbsp;</th><th> <p class="SimplePara">Pelvis</p> </th><th> <p class="SimplePara">Skull</p> </th><th> <p class="SimplePara">Abdomen</p> </th><th> <p class="SimplePara">Lumbar spine</p> </th><th> <p class="SimplePara">Thorax</p> </th></tr></thead><tbody><tr><td> <p class="SimplePara">Color&nbsp;+&nbsp;texture</p> </td><td> <p class="SimplePara">0.800</p> </td><td> <p class="SimplePara">0.550</p> </td><td> <p class="SimplePara">0.400</p> </td><td> <p class="SimplePara">0.550</p> </td><td> <p class="SimplePara">0.480</p> </td></tr><tr><td> <p class="SimplePara">Color&nbsp;+&nbsp;shape</p> </td><td> <p class="SimplePara">0.750</p> </td><td> <p class="SimplePara">0.750</p> </td><td> <p class="SimplePara">0.600</p> </td><td> <p class="SimplePara">0.600</p> </td><td> <p class="SimplePara">0.600</p> </td></tr><tr><td> <p class="SimplePara">Shape&nbsp;+&nbsp;texture</p> </td><td> <p class="SimplePara">0.680</p> </td><td> <p class="SimplePara">0.780</p> </td><td> <p class="SimplePara">0.650</p> </td><td> <p class="SimplePara">0.650</p> </td><td> <p class="SimplePara">0.700</p> </td></tr><tr><td> <p class="SimplePara">Our method</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.950</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.890</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.838</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.730</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.698</em> </p> </td></tr></tbody></table></div></div> <figure class="Figure" id="Fig6"><div class="MediaObject" id="MO35"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs13278-016-0362-9/MediaObjects/13278_2016_362_Fig6_HTML.gif" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="13278_2016_362_Fig6_HTML.gif" alt=""></a> </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig.&nbsp;6</span> <p class="SimplePara">Features comparison chart with the proposed method</p> </div></figcaption></figure> </div><div id="Par109" class="Para">Moreover
 tests were performed on unchanged (original) images, as well as on 
modified images, from the database. In these experiments, all query 
images (changed and original) were used to search for similar images 
stored in the databases. To investigate the retrieval of changed images,
 some query images were changed in terms of scale and rotation. The 
images were rotated by 10°, 45°, and 90°. The scale change factors were 
0.9 and 1.1. In these experiments, the precision decreased by 
approximately 6&nbsp;%. The precision–recall curves for experiments on 
changed images are shown in Fig.&nbsp;<span class="InternalRef"><a href="#Fig7">7</a></span>.<figure class="Figure" id="Fig7"><div class="MediaObject" id="MO36"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs13278-016-0362-9/MediaObjects/13278_2016_362_Fig7_HTML.gif" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="13278_2016_362_Fig7_HTML.gif" alt=""></a> </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig.&nbsp;7</span> <p class="SimplePara">Precision versus recall curve for original and modified images</p> </div></figcaption></figure> </div></section><section id="Sec23" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">4.5.4 </span>Comparative performance evaluation</h4><div id="Par110" class="Para">We
 report experimental results that show the feasibility and utility of 
the proposed algorithm and compare its performance with four 
state-of-the-art methods. Ramamurthy et al. (<span class="CitationRef"><a href="#CR58" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>), Wu and Tai (<span class="CitationRef"><a href="#CR78" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2009</a></span>), Ramamurthy and Chandran (<span class="CitationRef"><a href="#CR57" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>) and Bueno et al. (<span class="CitationRef"><a href="#CR12" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2010</a></span>).
 To simulate the practical situation, the sequence of query images used 
in all the experiments is always, as for the other experiments, 
generated at random. We use the average precision (AP) and the mean 
average precision (MAP), as mentioned in Sect.&nbsp;<span class="InternalRef"><a href="#Sec20">4.5.1</a></span>, as our retrieval performance metrics for the comparison between methods. Table&nbsp;<span class="InternalRef"><a href="#Tab3">3</a></span> shows that our proposed system performance is better than the other systems (Ramamurthy et al. (<span class="CitationRef"><a href="#CR58" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>) and Ramamurthy and Chandran (<span class="CitationRef"><a href="#CR57" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2012</a></span>)) for all classes. Figure&nbsp;<span class="InternalRef"><a href="#Fig8">8</a></span>
 shows the comparison of the proposed system with other systems. It 
shows the average precision value of each system for each class via the 
value from Table&nbsp;<span class="InternalRef"><a href="#Tab3">3</a></span> by a vertical bar.<div id="Tab3" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table&nbsp;3</span> <p class="SimplePara">Comparison among the average precision of the proposed method, Ramamurthy et al. and Ramamurthy and Chandran</p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1 align-left"><col class="tcol2 align-char"><col class="tcol3 align-char"><col class="tcol4 align-char"><col class="tcol5 align-char"><col class="tcol6 align-char"></colgroup><thead><tr><th>&nbsp;</th><th> <p class="SimplePara">Pelvis</p> </th><th> <p class="SimplePara">Skull</p> </th><th> <p class="SimplePara">Abdomen</p> </th><th> <p class="SimplePara">Lumbar spine</p> </th><th> <p class="SimplePara">Thorax</p> </th></tr></thead><tbody><tr><td> <p class="SimplePara">Ramamurthy et al.</p> </td><td> <p class="SimplePara">0.770</p> </td><td> <p class="SimplePara">0.770</p> </td><td> <p class="SimplePara">0.520</p> </td><td> <p class="SimplePara">0.470</p> </td><td> <p class="SimplePara">0.220</p> </td></tr><tr><td> <p class="SimplePara">Ramamurthy and Chandran</p> </td><td> <p class="SimplePara">0.700</p> </td><td> <p class="SimplePara">0.690</p> </td><td> <p class="SimplePara">0.550</p> </td><td> <p class="SimplePara">0.530</p> </td><td> <p class="SimplePara">0.400</p> </td></tr><tr><td> <p class="SimplePara">Proposed method</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.950</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.890</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.838</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.730</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.698</em> </p> </td></tr></tbody></table></div></div> <figure class="Figure" id="Fig8"><div class="MediaObject" id="MO37"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs13278-016-0362-9/MediaObjects/13278_2016_362_Fig8_HTML.gif" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="13278_2016_362_Fig8_HTML.gif" alt=""></a> </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig.&nbsp;8</span> <p class="SimplePara">Average precision comparisons between the proposed system and existing systems</p> </div></figcaption></figure> </div><div id="Par111" class="Para">According to results in Table&nbsp;<span class="InternalRef"><a href="#Tab4">4</a></span>,
 we see also that the image retrieval performance by the proposed method
 is competitive with the all other tested methods, in terms of mean 
average precision (MAP).<div id="Tab4" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table&nbsp;4</span> <p class="SimplePara">Average precision comparison between the proposed system and existing systems</p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1 align-left"><col class="tcol2 align-char"></colgroup><thead><tr><th>&nbsp;</th><th> <p class="SimplePara">MAP</p> </th></tr></thead><tbody><tr><td> <p class="SimplePara">Ramamurthy et al.</p> </td><td> <p class="SimplePara">0.55</p> </td></tr><tr><td> <p class="SimplePara">Wu and Tai</p> </td><td> <p class="SimplePara">0.51</p> </td></tr><tr><td> <p class="SimplePara">Bueno et al.</p> </td><td> <p class="SimplePara">0.75</p> </td></tr><tr><td> <p class="SimplePara">Ramamurthy and Chandran</p> </td><td> <p class="SimplePara">0.57</p> </td></tr><tr><td> <p class="SimplePara">Our method</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.82</em> </p> </td></tr></tbody></table></div></div> </div><p id="Par112" class="Para">The
 comparison is carried out by an implementation of each schema and 
method (systems to compare with our schema) by using MATLAB 7.0 and 
records different results from tests, performed on our image dataset. We
 can be sure that there is no difference between founded results and 
published results. So, we can be sure, now, that our proposed system is 
superior compared to other existing systems.</p></section><section id="Sec24" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">4.5.5 </span>Evaluation of the image retrieval scheme with relevance feedback</h4><div id="Par113" class="Para">As mentioned above, the relevance feedback method works in an iterative fashion. At every iteration, a set of <em class="EmphasisTypeItalic ">n</em>
 images are shown. The user is then given the chance to classify them as
 relevant or non-relevant, and his/her selections are given as an input 
to the search algorithm. Finally, the algorithm computes a new set of <em class="EmphasisTypeItalic ">k</em>
 images which are used as the input for the next iteration. The feedback
 processes were performed 5 times, and number of returned images is 10. 
For completeness, a final experiment aims to evaluate the effectiveness 
and the performance of our method when it is applied in all iterations 
of the relevance feedback mechanism. In Fig.&nbsp;<span class="InternalRef"><a href="#Fig9">9</a></span>,
 we present an example of a scenario of judgment and labeling 
(relevant/irrelevant) performed by a user related to an input query 
image. Different results for this experiment are shown in Figs.&nbsp;<span class="InternalRef"><a href="#Fig10">10</a></span> and <span class="InternalRef"><a href="#Fig11">11</a></span>.<figure class="Figure" id="Fig9"><div class="MediaObject" id="MO38"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs13278-016-0362-9/MediaObjects/13278_2016_362_Fig9_HTML.jpg" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="13278_2016_362_Fig9_HTML.jpg" alt=""></a> </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig.&nbsp;9</span> <p class="SimplePara">An example of labeling performed by a user</p> </div></figcaption></figure> <figure class="Figure" id="Fig10"><div class="MediaObject" id="MO39"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs13278-016-0362-9/MediaObjects/13278_2016_362_Fig10_HTML.gif" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="13278_2016_362_Fig10_HTML.gif" alt=""></a> </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig.&nbsp;10</span> <p class="SimplePara">Average precision comparison before and after relevance feedback</p> </div></figcaption></figure> <figure class="Figure" id="Fig11"><div class="MediaObject" id="MO40"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs13278-016-0362-9/MediaObjects/13278_2016_362_Fig11_HTML.gif" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="13278_2016_362_Fig11_HTML.gif" alt=""></a> </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig.&nbsp;11</span> <p class="SimplePara">Relationship between average precision and number of feedback iterations</p> </div></figcaption></figure> </div><p id="Par114" class="Para">Figures&nbsp;<span class="InternalRef"><a href="#Fig10">10</a></span> and <span class="InternalRef"><a href="#Fig11">11</a></span>
 shows the retrieval results, in terms of the AP, using the relevance 
feedback algorithm after the five feedback iterations. It is not 
difficult to see that, compared with the retrieval results without 
relevance feedback, the performance of our relevance feedback retrieval 
system is improved. However, as the number of the feedback iteration 
increases, the performance of our relevance feedback retrieval system 
becomes better and better, and it is more effective.</p><div id="Par115" class="Para">Figure&nbsp;<span class="InternalRef"><a href="#Fig11">11</a></span>
 describes also detailed comparison of the average retrieval precision 
for different classes. From this figure, we observed that there is a 
rapid increase in retrieval performance with each feedback iteration. 
Retrieval performance is improved from 69.8 to 82&nbsp;% from first 
iteration to the fifth iteration for the thorax class as an example. 
Results are also tabulated in Table&nbsp;<span class="InternalRef"><a href="#Tab5">5</a></span>.
 Users (physicians or patients) could use as many relevance feedback 
cycles as needed to improve the accuracy and the certainty of their 
diagnosis.<div id="Tab5" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Table&nbsp;5</span> <p class="SimplePara">Average precision value on each feedback iteration</p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1 align-left"><col class="tcol2 align-char"><col class="tcol3 align-char"><col class="tcol4 align-char"><col class="tcol5 align-char"><col class="tcol6 align-char"><col class="tcol7 align-char"></colgroup><thead><tr><th>&nbsp;</th><th> <p class="SimplePara">Before RF</p> </th><th> <p class="SimplePara">After 1st iteration</p> </th><th> <p class="SimplePara">After 2nd iteration</p> </th><th> <p class="SimplePara">After 3rd iteration</p> </th><th> <p class="SimplePara">After 4th iteration</p> </th><th> <p class="SimplePara">After 5th iteration</p> </th></tr></thead><tbody><tr><td> <p class="SimplePara">Pelvis</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.950</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.955</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.960</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.964</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.970</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.980</em> </p> </td></tr><tr><td> <p class="SimplePara">Skull</p> </td><td> <p class="SimplePara"> <strong class="EmphasisTypeBold ">0.890</strong> </p> </td><td> <p class="SimplePara"> <strong class="EmphasisTypeBold ">0.910</strong> </p> </td><td> <p class="SimplePara"> <strong class="EmphasisTypeBold ">0.923</strong> </p> </td><td> <p class="SimplePara"> <strong class="EmphasisTypeBold ">0.937</strong> </p> </td><td> <p class="SimplePara"> <strong class="EmphasisTypeBold ">0.942</strong> </p> </td><td> <p class="SimplePara"> <strong class="EmphasisTypeBold ">0.950</strong> </p> </td></tr><tr><td> <p class="SimplePara">Abdomen</p> </td><td> <p class="SimplePara"> <em><strong class="EmphasisTypeBoldItalic ">0.838</strong></em> </p> </td><td> <p class="SimplePara"> <em><strong class="EmphasisTypeBoldItalic ">0.860</strong></em> </p> </td><td> <p class="SimplePara"> <em><strong class="EmphasisTypeBoldItalic ">0.880</strong></em> </p> </td><td> <p class="SimplePara"> <em><strong class="EmphasisTypeBoldItalic ">0.900</strong></em> </p> </td><td> <p class="SimplePara"> <em><strong class="EmphasisTypeBoldItalic ">0.915</strong></em> </p> </td><td> <p class="SimplePara"> <em><strong class="EmphasisTypeBoldItalic ">0.922</strong></em> </p> </td></tr><tr><td> <p class="SimplePara">Lumbar spine</p> </td><td> <p class="SimplePara"> <strong class="EmphasisTypeBold ">0.730</strong> </p> </td><td> <p class="SimplePara"> <strong class="EmphasisTypeBold ">0.754</strong> </p> </td><td> <p class="SimplePara"> <strong class="EmphasisTypeBold ">0.780</strong> </p> </td><td> <p class="SimplePara"> <strong class="EmphasisTypeBold ">0.800</strong> </p> </td><td> <p class="SimplePara"> <strong class="EmphasisTypeBold ">0.830</strong> </p> </td><td> <p class="SimplePara"> <strong class="EmphasisTypeBold ">0.870</strong> </p> </td></tr><tr><td> <p class="SimplePara">Thorax</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.698</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.710</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.723</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.754</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.790</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">0.820</em> </p> </td></tr></tbody></table></div></div> </div></section></section></div></section><section id="Sec25" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading" data-role="collapsible-handle" tabindex="0"><span class="HeadingNumber">5 </span>The evaluation of our methodology’s impact in the decision-making process<span class="section-icon"></span></h2><div class="content"><p id="Par116" class="Para">After
 the navigation through our social network site and using the integrated
 CBMIR system, the specialists can judge the impact of our social 
network site in generally, and the integrated CBMIR system in 
particular.</p><section id="Sec26" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">5.1 </span>Evaluation methodology’s description</h3><p id="Par117" class="Para">Specialists
 are asked to fill a questionnaire. Its aim was to identify relevant 
information on their actual needs. Analyses were performed to evaluate, 
specially, the satisfaction of the specialists when using the integrated
 CBMIR system in clinical practice. We consider that the judgment of 
specialists as a reference for patient’s judgement. Basically, the 
questionnaire, as a semi-structured one, was composed of five personal 
questions, and 18 regarding the system assessment. The latter were 
elaborated based on usability and functional suitability features, which
 allow the evaluation how easy it was to use the medical social network 
and the CBMIR system (user interface esthetics, learn ability, 
operability). Functional suitability features evidenced the set of 
functions that respond to the internal and external demands of the 
system (subdivided into functional adequacy and functional correctness),
 as listed in ISO/IEC 25010 (<span class="CitationRef"><a href="#CR36" title="View reference" role="button" aria-pressed="false" aria-haspopup="true" aria-controls="popup-references">2011</a></span>).
 Moreover, a set of features was prepared with the aim of complementing 
ISO/IEC 25010 features according to this study approach (general views 
and opinions, new recommendations and requirements for system 
improvement and/or to solve limitations). In order to perform the 
analysis of our methodology, we conducted experiments with the support 
of 10 specialists, including radiologists and resident physicians from 
Charles Nicolle Hospital in Tunis. Twenty (20) images were used as query
 centers in such process. Each specialist analyzed five images on 
average, and each image was analyzed by two different specialists on 
average.</p></section><section id="Sec27" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">5.2 </span>Results and interpretation</h3><div id="Par118" class="Para">Finally, the specialists answered the questionnaire described above. The questionnaire showed that:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li> <p id="Par119" class="Para">100&nbsp;% found it easy to navigate through the medical social network site;</p> </li><li> <p id="Par120" class="Para">100&nbsp;% had some previous knowledge of these sites;</p> </li><li> <p id="Par121" class="Para">100&nbsp;% found that it is important to use a social network site dedicated to health care community (physicians and patients);</p> </li><li> <p id="Par122" class="Para">90&nbsp;% believes in the need of our site in the clinical routine;</p> </li><li> <p id="Par123" class="Para">90&nbsp;% found that it is important to integrate a CBMIR system through the medical social network site;</p> </li><li> <p id="Par124" class="Para">80&nbsp;% found it easy to operate the CBMIR system;</p> </li><li> <p id="Par125" class="Para">40&nbsp;% had some previous knowledge of this type of system;</p> </li><li> <p id="Par126" class="Para">90&nbsp;% believes that the CBMIR system helps in the diagnostic;</p> </li><li> <p id="Par127" class="Para">90&nbsp;%
 are satisfied about the interaction with the system to identify the 
relevant among the irrelevant images (about the RF method);</p> </li><li> <p id="Par128" class="Para">90&nbsp;% indicates that our system can help to train medical students and radiologists at Medical School;</p> </li><li> <p id="Par129" class="Para">90&nbsp;% believes in the need of our system in the clinical routine.</p> </li></ul></div> </div><p id="Par130" class="Para">Based
 on these findings, we can say that specialists have high expectations 
when using the medical social network site. We can say, also, that 
specialists have high expectations when using the integrated CBMIR 
system in clinical practice as a tool that makes relevant information 
and helps them in the decision making when they use the social network 
site. They need this system in the diagnostic aid, in order to establish
 the diagnosis exactly from existing comments and posts, so our image 
retrieval scheme can be directly used in the diagnosis process. They 
need it also for medical teaching by comparing similar cases and their 
particularities or comparing similar cases with different diagnosis and 
for medical research by finding, for example, certain types of images to
 be included in a study. With the results obtained, interesting facts 
can be analyzed regarding the application of the CBMIR system in 
clinical practice and the importance of calibrating the system 
correctly.</p></section></div></section><section id="Sec28" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading" data-role="collapsible-handle" tabindex="0"><span class="HeadingNumber">6 </span>Conclusion and future work<span class="section-icon"></span></h2><div class="content"><p id="Par131" class="Para">Social
 image indexing and retrieval in the large databases of the social 
networks advanced the challenges to form a new problem that needs 
special handling. Content-based image retrieval presents an interesting 
research topic to manipulate medical images. Further, medical image 
indexing and retrieval is a challenging task on its own, and thus 
different solutions have been proposed, trying to address different 
angles of the problem. For this reason, the integration of CBIR systems 
through social networks becomes important to handle the huge volume of 
uploaded images by different users. So, we proposed in this paper a 
medical social network destined to patients and physicians, in order to 
minimize medical errors by fostering collaboration between them. We, 
also, proposed the integration of a CBMIR system through our medical 
social network site. We presented an effective image retrieval system 
based on the combination and the fusion between color (gray level), 
texture and shape features. Therefore, color, shape and texture features
 are the most important features that were considered when developing a 
CBIR method. For the color content extraction, a well-known and powerful
 technique like the gray-level histogram and gray-level moment can be 
used, the co-occurrence matrix for texture content extraction was used, 
and statistical region descriptors for shape content extraction was 
used. Experimentations were performed by a medical image database from 
Charles Nicolle Hospital, in Tunis. The comparability of our proposed 
scheme with the other existing CBIR systems demonstrates the usage of 
the same database for system evaluation. Furthermore, it also proves 
that our system is better off than the other systems while the outcomes 
are satisfactory. We implemented, in our work, a relevance feedback 
approach to improve the CMBIR method quality, dealing with the “semantic
 gap problem.” By using the relevance feedback technique, we maximized 
the accuracy of the integrated CBMIR method, guided by the user’s 
interaction. The experiments showed that the proposed method is 
effective in improving the query precision, contributing to bridge the 
semantic gap and achieving improvement in the query results of up to 
20&nbsp;%. We presented, also, a study about the evaluation of our 
social network site in generally, and the integrated CBMIR system in 
particular in order to analyze the impact of our methodology in a real 
clinical environment. Different results obtained by the questionnaire 
show us that a CBMIR method is useful in real environments to assist 
specialists, during the decision-making process, and the integration of 
these systems through medical social network site is very successful to 
improve diagnostic. Finally, different analysis presented, also, 
interesting results about the acceptance and viability of using our 
medical social network site.</p><p id="Par132" class="Para">Future work 
will focus on presenting an analysis approach for existing comments. The
 objective is to extract keywords from comments must be used to give an 
overview, a summary, of what exist on comments and to annotate images in
 order to facilitate the search task. The indexation is used to improve 
the image search from textual queries.</p></div></section><section id="Footnotes" class="FootnoteSection Section1 RenderAsSection1"><h2 class="Heading" data-role="collapsible-handle" tabindex="0">Footnotes<span class="section-icon"></span></h2><div class="content"><ol><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn1_source">1</a>.</span><div class="FootnoteContent" id="Fn1"><p id="Par3" class="Para"> <span class="ExternalRef"><a target="_blank" rel="noopener" href="http://iom.nationalacademies.org/"><span class="RefSource">http://iom.nationalacademies.org/</span></a></span>.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn2_source">2</a>.</span><div class="FootnoteContent" id="Fn2"><p id="Par9" class="Para"> <span class="ExternalRef"><a target="_blank" rel="noopener" href="http://corp.yougov.com/healthcare/consumers-use-preference-expectations-hospital"><span class="RefSource">http://corp.yougov.com/healthcare/consumers-use-preference-expectations-hospital</span></a></span> social-media.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn3_source">3</a>.</span><div class="FootnoteContent" id="Fn3"><p id="Par11" class="Para"> <span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.sobercircle.com/"><span class="RefSource">http://www.sobercircle.com</span></a></span>.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn4_source">4</a>.</span><div class="FootnoteContent" id="Fn4"><p id="Par12" class="Para"> <span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.sparkpeople.com/"><span class="RefSource">http://www.sparkpeople.com/</span></a></span>.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn5_source">5</a>.</span><div class="FootnoteContent" id="Fn5"><p id="Par13" class="Para"> <span class="ExternalRef"><a target="_blank" rel="noopener" href="https://www.fitocracy.com/"><span class="RefSource">https://www.fitocracy.com/</span></a></span>.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn6_source">6</a>.</span><div class="FootnoteContent" id="Fn6"><p id="Par14" class="Para"> <span class="ExternalRef"><a target="_blank" rel="noopener" href="https://www.dacadoo.com/"><span class="RefSource">https://www.dacadoo.com/</span></a></span>.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn7_source">7</a>.</span><div class="FootnoteContent" id="Fn7"><p id="Par15" class="Para"> <span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.asklepios.com/"><span class="RefSource">http://www.asklepios.com/</span></a></span>.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn8_source">8</a>.</span><div class="FootnoteContent" id="Fn8"><p id="Par16" class="Para"> <span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.acc.org/"><span class="RefSource">http://www.acc.org/</span></a></span>.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn9_source">9</a>.</span><div class="FootnoteContent" id="Fn9"><p id="Par17" class="Para"> <span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.diabspace.com/"><span class="RefSource">http://www.diabspace.com/</span></a></span>.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn10_source">10</a>.</span><div class="FootnoteContent" id="Fn10"><p id="Par18" class="Para"> <span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.parlonscancer.ca/"><span class="RefSource">http://www.parlonscancer.ca/</span></a></span>.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn11_source">11</a>.</span><div class="FootnoteContent" id="Fn11"><p id="Par19" class="Para"> <span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.renaloo.com/"><span class="RefSource">http://www.renaloo.com/</span></a></span>.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn12_source">12</a>.</span><div class="FootnoteContent" id="Fn12"><p id="Par20" class="Para"> <span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.rxspace.com/"><span class="RefSource">http://www.rxspace.com/</span></a></span>.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn13_source">13</a>.</span><div class="FootnoteContent" id="Fn13"><p id="Par55" class="Para">IRMA Homepage (English): <span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.irma-project.org/index_en.php"><span class="RefSource">http://www.irma-project.org/index_en.php</span></a></span>.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn14_source">14</a>.</span><div class="FootnoteContent" id="Fn14"><p id="Par98" class="Para"> <span class="ExternalRef"><a target="_blank" rel="noopener" href="http://imagej.nih.gov/ij/"><span class="RefSource">http://imagej.nih.gov/ij/</span></a></span>.</p></div></li></ol></div></section></div><section class="Section1 RenderAsSection1" id="Bib1" tabindex="-1"><h2 class="Heading" data-role="collapsible-handle" tabindex="0">References<span class="section-icon"></span></h2><div class="content"><ol class="BibliographyWrapper"><li class="Citation"><div class="CitationContent" id="CR1">Afifi
 AJ, Ashour WM (2012) Content-based image retrieval using invariant 
color and texture features. In: International conference on digital 
image computing techniques and applications (DICTA). IEEE, Fremantle, 
WA, pp 1–6<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Afifi%20AJ%2C%20Ashour%20WM%20%282012%29%20Content-based%20image%20retrieval%20using%20invariant%20color%20and%20texture%20features.%20In%3A%20International%20conference%20on%20digital%20image%20computing%20techniques%20and%20applications%20%28DICTA%29.%20IEEE%2C%20Fremantle%2C%20WA%2C%20pp%201%E2%80%936"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR2">Agma
 J, Traina M, André G, Balan R, Bortolotti LM, Traina C Jr (2007) 
Content-based image retrieval using approximate shape of objects. In: 
The 17th IEEE symposium on computer-based medical systems (CBMS’07), pp 
91–96<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Agma%20J%2C%20Traina%20M%2C%20Andr%C3%A9%20G%2C%20Balan%20R%2C%20Bortolotti%20LM%2C%20Traina%20C%20Jr%20%282007%29%20Content-based%20image%20retrieval%20using%20approximate%20shape%20of%20objects.%20In%3A%20The%2017th%20IEEE%20symposium%20on%20computer-based%20medical%20systems%20%28CBMS%E2%80%9907%29%2C%20pp%2091%E2%80%9396"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR3">Aisen
 AM, Broderick LS, Winer-Muram H, Brodley CE, Kak AC, Pavlopoulou C et 
al (2003) Automated storage and retrieval of thin section CT images to 
assist diagnosis: system description and preliminary assessment. 
Radiology 228(1):265–270<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1148/radiol.2281020126"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Automated%20storage%20and%20retrieval%20of%20thin%20section%20CT%20images%20to%20assist%20diagnosis%3A%20system%20description%20and%20preliminary%20assessment&amp;author=AM.%20Aisen&amp;author=LS.%20Broderick&amp;author=H.%20Winer-Muram&amp;author=CE.%20Brodley&amp;author=AC.%20Kak&amp;author=C.%20Pavlopoulou&amp;journal=Radiology&amp;volume=228&amp;issue=1&amp;pages=265-270&amp;publication_year=2003"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR4">Akgül
 C, Rubin D, Napel S, Beaulieu C, Greenspan H, Acar B (2011) 
Content-based image retrieval in radiology: current status and future 
directions. J Digit Imaging 24:208–222<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1007/s10278-010-9290-9"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Content-based%20image%20retrieval%20in%20radiology%3A%20current%20status%20and%20future%20directions&amp;author=C.%20Akg%C3%BCl&amp;author=D.%20Rubin&amp;author=S.%20Napel&amp;author=C.%20Beaulieu&amp;author=H.%20Greenspan&amp;author=B.%20Acar&amp;journal=J%20Digit%20Imaging&amp;volume=24&amp;pages=208-222&amp;publication_year=2011"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR5">Almansoori
 W, Zarour O, Jarada TN, Karampales P, Rokne J, Alhajj R (2011) 
Applications of social network construction and analysis in the medical 
referral process. In: Proceedings of the 2011 IEEE ninth international 
conference on dependable, autonomic and secure computing (DASC ‘11)<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Almansoori%20W%2C%20Zarour%20O%2C%20Jarada%20TN%2C%20Karampales%20P%2C%20Rokne%20J%2C%20Alhajj%20R%20%282011%29%20Applications%20of%20social%20network%20construction%20and%20analysis%20in%20the%20medical%20referral%20process.%20In%3A%20Proceedings%20of%20the%202011%20IEEE%20ninth%20international%20conference%20on%20dependable%2C%20autonomic%20and%20secure%20computing%20%28DASC%20%E2%80%9811%29"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR6">Antani
 S, Long LR, Thoma GR, Lee DJ (2003) Evaluation of shape indexing 
methods for content-based retrieval of X-ray images. In: Yeung MM, 
Lienhart RW, Li CS (eds) Proceedings of SPIE 5021, pp 405–416<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Antani%20S%2C%20Long%20LR%2C%20Thoma%20GR%2C%20Lee%20DJ%20%282003%29%20Evaluation%20of%20shape%20indexing%20methods%20for%20content-based%20retrieval%20of%20X-ray%20images.%20In%3A%20Yeung%20MM%2C%20Lienhart%20RW%2C%20Li%20CS%20%28eds%29%20Proceedings%20of%20SPIE%205021%2C%20pp%20405%E2%80%93416"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR7">Antani
 S, Lee D, Long LR, Thoma GR (2004) Evaluation of shape similarity 
measurement methods for spine X-ray images. J Vis Commun Image Represent
 15(3):285–302<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1016/j.jvcir.2004.04.005"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Evaluation%20of%20shape%20similarity%20measurement%20methods%20for%20spine%20X-ray%20images&amp;author=S.%20Antani&amp;author=D.%20Lee&amp;author=LR.%20Long&amp;author=GR.%20Thoma&amp;journal=J%20Vis%20Commun%20Image%20Represent&amp;volume=15&amp;issue=3&amp;pages=285-302&amp;publication_year=2004"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR8">Arevalillo-Herráez
 M, Ferri FJ, Moreno-Picot S (2013) A hybrid multi-objective 
optimization algorithm for content based image retrieval. Appl Soft 
Comput 13:4358–4369<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1016/j.asoc.2013.06.016"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=A%20hybrid%20multi-objective%20optimization%20algorithm%20for%20content%20based%20image%20retrieval&amp;author=M.%20Arevalillo-Herr%C3%A1ez&amp;author=FJ.%20Ferri&amp;author=S.%20Moreno-Picot&amp;journal=Appl%20Soft%20Comput&amp;volume=13&amp;pages=4358-4369&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR9">Ashish
 O, Manpreet S (2012) Content based image retrieval system for medical 
databases (CBIR-MD)—lucratively tested on endoscopy, dental and skull 
images. IJCSI Int J Comput Sci Issues 9(1):300–306<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Content%20based%20image%20retrieval%20system%20for%20medical%20databases%20%28CBIR-MD%29%E2%80%94lucratively%20tested%20on%20endoscopy%2C%20dental%20and%20skull%20images&amp;author=O.%20Ashish&amp;author=S.%20Manpreet&amp;journal=IJCSI%20Int%20J%20Comput%20Sci%20Issues&amp;volume=9&amp;issue=1&amp;pages=300-306&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR10">Bach
 JR, Fuller C, Gupta A, Hampapur A, Horowitz B, Humphrey R et al (1996) 
Virage image search engine: an open framework for image management. In: 
Sethi IK, Jain RC (eds) Proceedings of SPIE, vol 2670, no 1, pp 76–87<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Bach%20JR%2C%20Fuller%20C%2C%20Gupta%20A%2C%20Hampapur%20A%2C%20Horowitz%20B%2C%20Humphrey%20R%20et%20al%20%281996%29%20Virage%20image%20search%20engine%3A%20an%20open%20framework%20for%20image%20management.%20In%3A%20Sethi%20IK%2C%20Jain%20RC%20%28eds%29%20Proceedings%20of%20SPIE%2C%20vol%202670%2C%20no%201%2C%20pp%2076%E2%80%9387"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR11">Bhattacharjee
 N, Parekh R (2011) Skin texture analysis for medical diagnosis. In: The
 international conference on communication, computing and security. New 
York, pp 301–306<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Bhattacharjee%20N%2C%20Parekh%20R%20%282011%29%20Skin%20texture%20analysis%20for%20medical%20diagnosis.%20In%3A%20The%20international%20conference%20on%20communication%2C%20computing%20and%20security.%20New%20York%2C%20pp%20301%E2%80%93306"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR12">Bueno
 R, Ribeiro MX, Traina AJM (2010) Improving medical image retrieval 
through multi-descriptor similarity functions and association rules. In:
 IEEE 23rd international symposium on computer-based medical systems 
(CBMS), pp 309–314<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Bueno%20R%2C%20Ribeiro%20MX%2C%20Traina%20AJM%20%282010%29%20Improving%20medical%20image%20retrieval%20through%20multi-descriptor%20similarity%20functions%20and%20association%20rules.%20In%3A%20IEEE%2023rd%20international%20symposium%20on%20computer-based%20medical%20systems%20%28CBMS%29%2C%20pp%20309%E2%80%93314"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR13">Bugatti
 PH, Ponciano-Silva M, Agma J, Traina M, Traina C Jr, Marques P (2009) 
Content-based retrieval of medical images: from context to perception. 
In: 22nd IEEE international symposium on computer-based medical systems 
(CBMS), pp 1–8<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Bugatti%20PH%2C%20Ponciano-Silva%20M%2C%20Agma%20J%2C%20Traina%20M%2C%20Traina%20C%20Jr%2C%20Marques%20P%20%282009%29%20Content-based%20retrieval%20of%20medical%20images%3A%20from%20context%20to%20perception.%20In%3A%2022nd%20IEEE%20international%20symposium%20on%20computer-based%20medical%20systems%20%28CBMS%29%2C%20pp%201%E2%80%938"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR14">Bugatti
 PH, Ribeiro MX, Traina JM, Traina C Jr (2011) Feature selection guided 
by perception in medical CBIR systems. In: First IEEE international 
conference on healthcare informatics, imaging and systems biology, pp 
323–330<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Bugatti%20PH%2C%20Ribeiro%20MX%2C%20Traina%20JM%2C%20Traina%20C%20Jr%20%282011%29%20Feature%20selection%20guided%20by%20perception%20in%20medical%20CBIR%20systems.%20In%3A%20First%20IEEE%20international%20conference%20on%20healthcare%20informatics%2C%20imaging%20and%20systems%20biology%2C%20pp%20323%E2%80%93330"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR15">Chechik
 G, Sharma V, Shalit U, Bengio S (2010) Large scale online learning of 
image similarity through ranking. J Mach Learn Res 11:1109–1135<span class="Occurrences"><span class="Occurrence OccurrenceAMSID"><a class="gtm-reference" data-reference-type="MathSciNet" target="_blank" rel="noopener" href="http://www.ams.org/mathscinet-getitem?mr=2629826"><span><span>MathSciNet</span></span></a></span><span class="Occurrence OccurrenceZLBID"><a class="gtm-reference" data-reference-type="MATH" target="_blank" rel="noopener" href="http://www.emis.de/MATH-item?1242.68212"><span><span>MATH</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Large%20scale%20online%20learning%20of%20image%20similarity%20through%20ranking&amp;author=G.%20Chechik&amp;author=V.%20Sharma&amp;author=U.%20Shalit&amp;author=S.%20Bengio&amp;journal=J%20Mach%20Learn%20Res&amp;volume=11&amp;pages=1109-1135&amp;publication_year=2010"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR16">Cox
 IJ, Miller ML, Minka TP, Papathomas TV, Yianilos PN (2000) The Bayesian
 image retieval system, PicHunter: theory, implementation and 
psychophysical experiments. IEEE Tran Image Process 9(1):20–37<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/83.817596"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=The%20Bayesian%20image%20retieval%20system%2C%20PicHunter%3A%20theory%2C%20implementation%20and%20psychophysical%20experiments&amp;author=IJ.%20Cox&amp;author=ML.%20Miller&amp;author=TP.%20Minka&amp;author=TV.%20Papathomas&amp;author=PN.%20Yianilos&amp;journal=IEEE%20Tran%20Image%20Process&amp;volume=9&amp;issue=1&amp;pages=20-37&amp;publication_year=2000"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR17">Daniel RG, Liza SR, Jennifer LK (2013) Dangers and opportunities for social media in medicine. Clin Obstet Gynecol. doi:<span class="ExternalRef">&nbsp;<a target="_blank" rel="noopener" href="https://doi.org/10.1097/GRF.0b013e318297dc38"><span class="RefSource">10.1097/GRF.0b013e318297dc38</span></a></span>
                        <span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Dangers%20and%20opportunities%20for%20social%20media%20in%20medicine&amp;author=RG.%20Daniel&amp;author=SR.%20Liza&amp;author=LK.%20Jennifer&amp;journal=Clin%20Obstet%20Gynecol&amp;publication_year=2013&amp;doi=10.1097%2FGRF.0b013e318297dc38"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR18">Datta
 R, Joshi D, Li J, Wang JZ (2008) Image retrieval: ideas, influences, 
and trends of the new age. ACM Comput Surv 40(2):5:1–5:60<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1145/1348246.1348248"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Image%20retrieval%3A%20ideas%2C%20influences%2C%20and%20trends%20of%20the%20new%20age&amp;author=R.%20Datta&amp;author=D.%20Joshi&amp;author=J.%20Li&amp;author=JZ.%20Wang&amp;journal=ACM%20Comput%20Surv&amp;volume=40&amp;issue=2&amp;pages=5%3A1-5%3A60&amp;publication_year=2008"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR19">De
 Oliveira JEE, Machado AMC, Chavez GC, Lopes APB, Deserno TM, De Araujo 
AA (2010) Mammosys: a content-based image retrieval system using breast 
density patterns. Comput Methods Programs Biomed 99(3):289–297<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1016/j.cmpb.2010.01.005"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Mammosys%3A%20a%20content-based%20image%20retrieval%20system%20using%20breast%20density%20patterns&amp;author=JEE.%20Oliveira&amp;author=AMC.%20Machado&amp;author=GC.%20Chavez&amp;author=APB.%20Lopes&amp;author=TM.%20Deserno&amp;author=AA.%20Araujo&amp;journal=Comput%20Methods%20Programs%20Biomed&amp;volume=99&amp;issue=3&amp;pages=289-297&amp;publication_year=2010"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR20">Doganay S (2014) Healthcare social networks: new choices for doctors, patients. <span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.informationweek.com/healthcare/patient-tools/healthcare-social-networks-new-choices-for-doctors-patients/d/d-id/1234884"><span class="RefSource">http://www.informationweek.com/healthcare/patient-tools/healthcare-social-networks-new-choices-for-doctors-patients/d/d-id/1234884</span></a></span>
                        <span class="Occurrences"></span></div></li><li class="Citation"><div class="CitationContent" id="CR21">Doulamis
 N, Doulamis A (2006) Evaluation of relevance feedback schemes in 
content-based in retrieval systems. Signal Process Image Commun 
21(4):334–357<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1016/j.image.2005.11.006"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceZLBID"><a class="gtm-reference" data-reference-type="MATH" target="_blank" rel="noopener" href="http://www.emis.de/MATH-item?1148.94309"><span><span>MATH</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Evaluation%20of%20relevance%20feedback%20schemes%20in%20content-based%20in%20retrieval%20systems&amp;author=N.%20Doulamis&amp;author=A.%20Doulamis&amp;journal=Signal%20Process%20Image%20Commun&amp;volume=21&amp;issue=4&amp;pages=334-357&amp;publication_year=2006"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR22">Fazal
 M, Baharum B (2013) Analysis of distance metrics in content-based image
 retrieval using statistical quantized histogram texture features in the
 DCT domain. J King Saud Univ Comput Inf Sci 25:207–218<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Analysis%20of%20distance%20metrics%20in%20content-based%20image%20retrieval%20using%20statistical%20quantized%20histogram%20texture%20features%20in%20the%20DCT%20domain&amp;author=M.%20Fazal&amp;author=B.%20Baharum&amp;journal=J%20King%20Saud%20Univ%20Comput%20Inf%20Sci&amp;volume=25&amp;pages=207-218&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR23">Feldman
 DL (2012) Medical social media networks: communicating across the 
virtual highway. Q J Health Care Pract Risk Manag Infocus 18(1):2–5<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Medical%20social%20media%20networks%3A%20communicating%20across%20the%20virtual%20highway&amp;author=DL.%20Feldman&amp;journal=Q%20J%20Health%20Care%20Pract%20Risk%20Manag%20Infocus&amp;volume=18&amp;issue=1&amp;pages=2-5&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR24">Flickner
 M, Sawhney H, Niblack W, Ashley J, Huang Q, Dom B et al (1995) Query by
 image and video content: the QBIC system. Computer 28(9):23–32<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/2.410146"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Query%20by%20image%20and%20video%20content%3A%20the%20QBIC%20system&amp;author=M.%20Flickner&amp;author=H.%20Sawhney&amp;author=W.%20Niblack&amp;author=J.%20Ashley&amp;author=Q.%20Huang&amp;author=B.%20Dom&amp;journal=Computer&amp;volume=28&amp;issue=9&amp;pages=23-32&amp;publication_year=1995"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR25">Franklin V, Greene S (2007) Sweet talk: a text messaging support system. J Diabetes Nurs 11(1):22–26<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Sweet%20talk%3A%20a%20text%20messaging%20support%20system&amp;author=V.%20Franklin&amp;author=S.%20Greene&amp;journal=J%20Diabetes%20Nurs&amp;volume=11&amp;issue=1&amp;pages=22-26&amp;publication_year=2007"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR26">Goh
 K-S, Chang EY, Li B (2005) Using one-class and two-class SVMs for 
multiclass image annotation. IEEE Trans Knowl Data Eng 17(10):1333–1346<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/TKDE.2005.170"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Using%20one-class%20and%20two-class%20SVMs%20for%20multiclass%20image%20annotation&amp;author=K-S.%20Goh&amp;author=EY.%20Chang&amp;author=B.%20Li&amp;journal=IEEE%20Trans%20Knowl%20Data%20Eng&amp;volume=17&amp;issue=10&amp;pages=1333-1346&amp;publication_year=2005"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR27">Gong
 J, Sun S (2011) Individual doctor recommendation model on medical 
social network. In: Proceedings of the 7th international conference on 
advanced data mining and applications (ADMA’11)<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Gong%20J%2C%20Sun%20S%20%282011%29%20Individual%20doctor%20recommendation%20model%20on%20medical%20social%20network.%20In%3A%20Proceedings%20of%20the%207th%20international%20conference%20on%20advanced%20data%20mining%20and%20applications%20%28ADMA%E2%80%9911%29"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR28">Greenspan
 H (2007) Medical image categorization and retrieval. For PACS using the
 GMM-KL framework. IEEE Trans Inf Technol BioMed 11:190–202<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/TITB.2006.874191"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Medical%20image%20categorization%20and%20retrieval.%20For%20PACS%20using%20the%20GMM-KL%20framework&amp;author=H.%20Greenspan&amp;journal=IEEE%20Trans%20Inf%20Technol%20BioMed&amp;volume=11&amp;pages=190-202&amp;publication_year=2007"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR29">Grenier
 C (2003) The role of intermediate subject to understand the structuring
 of an organizational network of actors and technology—case of a care 
network. In: Proceedings of the 9th conference of the association 
information and management, Grenoble<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Grenier%20C%20%282003%29%20The%20role%20of%20intermediate%20subject%20to%20understand%20the%20structuring%20of%20an%20organizational%20network%20of%20actors%20and%20technology%E2%80%94case%20of%20a%20care%20network.%20In%3A%20Proceedings%20of%20the%209th%20conference%20of%20the%20association%20information%20and%20management%2C%20Grenoble"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR30">Güld
 MO, Thies C, Fischer B, Lehmann TM (2007) A generic concept for the 
implementation of medical image retrieval systems. Int Med Inform 
76(2–3):252–259<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1016/j.ijmedinf.2006.02.011"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=A%20generic%20concept%20for%20the%20implementation%20of%20medical%20image%20retrieval%20systems&amp;author=MO.%20G%C3%BCld&amp;author=C.%20Thies&amp;author=B.%20Fischer&amp;author=TM.%20Lehmann&amp;journal=Int%20Med%20Inform&amp;volume=76&amp;issue=2%E2%80%933&amp;pages=252-259&amp;publication_year=2007"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR31">Harishchandra
 H, Mushigeri S, Niranjan UC (2014) Medical image retrieval–performance 
comparison using texture features. Int J Eng Res Dev 9(9):30–34<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Medical%20image%20retrieval%E2%80%93performance%20comparison%20using%20texture%20features&amp;author=H.%20Harishchandra&amp;author=S.%20Mushigeri&amp;author=UC.%20Niranjan&amp;journal=Int%20J%20Eng%20Res%20Dev&amp;volume=9&amp;issue=9&amp;pages=30-34&amp;publication_year=2014"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR32">Hoi
 SCHH, Jin R, Zhu JK, Lyu MR (2009) Semi-supervised SVM batch mode 
active learning and its applications to image retrieval. ACM Trans Inf 
Syst 27(3):1–29<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1145/1508850.1508854"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Semi-supervised%20SVM%20batch%20mode%20active%20learning%20and%20its%20applications%20to%20image%20retrieval&amp;author=SCHH.%20Hoi&amp;author=R.%20Jin&amp;author=JK.%20Zhu&amp;author=MR.%20Lyu&amp;journal=ACM%20Trans%20Inf%20Syst&amp;volume=27&amp;issue=3&amp;pages=1-29&amp;publication_year=2009"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR33">Hsu
 W, Antani S, Long LR, Neve L, Thoma GR (2009) SPIRS: a web based image 
retrieval system for large biomedical databases. Int J Med Inform 
78(1):13–24<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1016/j.ijmedinf.2008.09.006"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=SPIRS%3A%20a%20web%20based%20image%20retrieval%20system%20for%20large%20biomedical%20databases&amp;author=W.%20Hsu&amp;author=S.%20Antani&amp;author=LR.%20Long&amp;author=L.%20Neve&amp;author=GR.%20Thoma&amp;journal=Int%20J%20Med%20Inform&amp;volume=78&amp;issue=1&amp;pages=13-24&amp;publication_year=2009"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR34">Hu
 W, Xie N, Li L, Zeng X (2011) A survey on visual content-based video 
indexing and retrieval. IEEE Trans Syst Man Cybern C Appl Rev 
41(6):797–819<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/TSMCC.2011.2109710"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=A%20survey%20on%20visual%20content-based%20video%20indexing%20and%20retrieval&amp;author=W.%20Hu&amp;author=N.%20Xie&amp;author=L.%20Li&amp;author=X.%20Zeng&amp;journal=IEEE%20Trans%20Syst%20Man%20Cybern%20C%20Appl%20Rev&amp;volume=41&amp;issue=6&amp;pages=797-819&amp;publication_year=2011"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR35">Iakovidis
 D, Pelekis N, Kotsifakos E, Kopanakis I, Karanikas H, Theodoridis Y 
(2009) A pattern similarity scheme for medical imag retrieval. IEEE 
Trans Inf Technol Biomed 13(4):442–509<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/TITB.2008.923144"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=A%20pattern%20similarity%20scheme%20for%20medical%20imag%20retrieval&amp;author=D.%20Iakovidis&amp;author=N.%20Pelekis&amp;author=E.%20Kotsifakos&amp;author=I.%20Kopanakis&amp;author=H.%20Karanikas&amp;author=Y.%20Theodoridis&amp;journal=IEEE%20Trans%20Inf%20Technol%20Biomed&amp;volume=13&amp;issue=4&amp;pages=442-509&amp;publication_year=2009"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR36">ISO,
 IEC 25010 (2011) Systems and software engineering-Systems and software 
Quality Requirements and Evaluation (SQuaRE)-System and software quality
 models. International Standards Organization, Geneva<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Systems%20and%20software%20engineering-Systems%20and%20software%20Quality%20Requirements%20and%20Evaluation%20%28SQuaRE%29-System%20and%20software%20quality%20models&amp;publication_year=2011"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR37">John C, Kazunori O (2012) A comparative study of similarity measures for content-based medical image retrieval. <span class="ExternalRef"><a target="_blank" rel="noopener" href="http://ceur-ws.org/Vol-1178/CLEF2012wn-ImageCLEF-CollinsEt2012.pdf2012"><span class="RefSource">http://ceur-ws.org/Vol-1178/CLEF2012wn-ImageCLEF-CollinsEt2012.pdf2012</span></a></span>
                        <span class="Occurrences"></span></div></li><li class="Citation"><div class="CitationContent" id="CR38">Keysers
 D, Dahmen J, Ney H, Wein BB, Lehmann TM (2003) Statistica framework for
 model-based image retrieval in medical applications. J Electron Imaging
 12(1):59–68<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1117/1.1525790"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Statistica%20framework%20for%20model-based%20image%20retrieval%20in%20medical%20applications&amp;author=D.%20Keysers&amp;author=J.%20Dahmen&amp;author=H.%20Ney&amp;author=BB.%20Wein&amp;author=TM.%20Lehmann&amp;journal=J%20Electron%20Imaging&amp;volume=12&amp;issue=1&amp;pages=59-68&amp;publication_year=2003"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR39">Komali A et al (2012) 3D color feature extraction in content-based image retrieval. Int J Soft Comput Eng (IJSCE) 2(3):560–563<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=3D%20color%20feature%20extraction%20in%20content-based%20image%20retrieval&amp;author=A.%20Komali&amp;journal=Int%20J%20Soft%20Comput%20Eng%20%28IJSCE%29&amp;volume=2&amp;issue=3&amp;pages=560-563&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR40">Kumar
 A, Kim J, Cai W, Fulham M, Feng D (2013) Content-based medical image 
retrieval: a survey of applications to multidimensional and 
multimodality data. J Digit Imaging. doi:<span class="ExternalRef">&nbsp;<a target="_blank" rel="noopener" href="https://doi.org/10.1007/s10278-013-9619-2"><span class="RefSource">10.1007/s10278-013-9619-2</span></a></span>
                        <span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Content-based%20medical%20image%20retrieval%3A%20a%20survey%20of%20applications%20to%20multidimensional%20and%20multimodality%20data&amp;author=A.%20Kumar&amp;author=J.%20Kim&amp;author=W.%20Cai&amp;author=M.%20Fulham&amp;author=D.%20Feng&amp;journal=J%20Digit%20Imaging&amp;publication_year=2013&amp;doi=10.1007%2Fs10278-013-9619-2"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR41">Lee
 DJ, Antani S, Long LR (2003) Similarity measurement using polygon curve
 representation and Fourier descriptors for shape-based vertebral image 
retrieval. In: Sonka M, Fitzpatrick JM (eds) Proceedings of SPIE, vol 
5032, pp 1283–1291<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Lee%20DJ%2C%20Antani%20S%2C%20Long%20LR%20%282003%29%20Similarity%20measurement%20using%20polygon%20curve%20representation%20and%20Fourier%20descriptors%20for%20shape-based%20vertebral%20image%20retrieval.%20In%3A%20Sonka%20M%2C%20Fitzpatrick%20JM%20%28eds%29%20Proceedings%20of%20SPIE%2C%20vol%205032%2C%20pp%201283%E2%80%931291"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR42">Lee
 DJ, Antani S, Chang Y, Gledhill K, Long LR, Christensen P (2009) CBIR 
of spine X-ray images on intervertebral disc space and shape profiles 
using feature ranking and voting consensus. Data Knowl Eng 
68(12):1359–1369<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1016/j.datak.2009.07.008"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=CBIR%20of%20spine%20X-ray%20images%20on%20intervertebral%20disc%20space%20and%20shape%20profiles%20using%20feature%20ranking%20and%20voting%20consensus&amp;author=DJ.%20Lee&amp;author=S.%20Antani&amp;author=Y.%20Chang&amp;author=K.%20Gledhill&amp;author=LR.%20Long&amp;author=P.%20Christensen&amp;journal=Data%20Knowl%20Eng&amp;volume=68&amp;issue=12&amp;pages=1359-1369&amp;publication_year=2009"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR43">Lew
 MS, Sebe N, Djeraba C, Jain R (2006) Content-based multimedia 
information retrieval: state of the art and challenges. ACM Trans 
Multimed Comput Commun Appl 2(1):1–19<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1145/1126004.1126005"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Content-based%20multimedia%20information%20retrieval%3A%20state%20of%20the%20art%20and%20challenges&amp;author=MS.%20Lew&amp;author=N.%20Sebe&amp;author=C.%20Djeraba&amp;author=R.%20Jain&amp;journal=ACM%20Trans%20Multimed%20Comput%20Commun%20Appl&amp;volume=2&amp;issue=1&amp;pages=1-19&amp;publication_year=2006"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR44">Li J (2014) Data protection in healthcare social networks. J IEEE Softw 31(1):46–53<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/MS.2013.99"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Data%20protection%20in%20healthcare%20social%20networks&amp;author=J.%20Li&amp;journal=J%20IEEE%20Softw&amp;volume=31&amp;issue=1&amp;pages=46-53&amp;publication_year=2014"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR45">Liu
 Y, Zhang D, Lu G, Ma W-Y (2007) A survey of content based image 
retrieval with high-level semantics. Pattern Recognit Lett 40(1):262–282<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1016/j.patcog.2006.04.045"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceZLBID"><a class="gtm-reference" data-reference-type="MATH" target="_blank" rel="noopener" href="http://www.emis.de/MATH-item?1103.68503"><span><span>MATH</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=A%20survey%20of%20content%20based%20image%20retrieval%20with%20high-level%20semantics&amp;author=Y.%20Liu&amp;author=D.%20Zhang&amp;author=G.%20Lu&amp;author=W-Y.%20Ma&amp;journal=Pattern%20Recognit%20Lett&amp;volume=40&amp;issue=1&amp;pages=262-282&amp;publication_year=2007"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR46">MacArthur
 SD, Brodley CE, Shyu CR (2000) Relevance feedback decision trees in 
content-based image retrieval. In: Proceedings of the IEEE work-shop 
content-based access of image and video libraries, pp 68–72<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=MacArthur%20SD%2C%20Brodley%20CE%2C%20Shyu%20CR%20%282000%29%20Relevance%20feedback%20decision%20trees%20in%20content-based%20image%20retrieval.%20In%3A%20Proceedings%20of%20the%20IEEE%20work-shop%20content-based%20access%20of%20image%20and%20video%20libraries%2C%20pp%2068%E2%80%9372"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR47">Manning CD, Raghavan P, Schutze H (2008) Introduction to information retrieval. Cambridge University Press, New York<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1017/CBO9780511809071"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceZLBID"><a class="gtm-reference" data-reference-type="MATH" target="_blank" rel="noopener" href="http://www.emis.de/MATH-item?1160.68008"><span><span>MATH</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Introduction%20to%20information%20retrieval&amp;author=CD.%20Manning&amp;author=P.%20Raghavan&amp;author=H.%20Schutze&amp;publication_year=2008"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR48">Messaoudi
 A, Bouslimi R, Akaichi J (2013) Indexing medical images based on 
collaborative experts reports. Int J Comput Appl 70(5):1–9<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Indexing%20medical%20images%20based%20on%20collaborative%20experts%20reports&amp;author=A.%20Messaoudi&amp;author=R.%20Bouslimi&amp;author=J.%20Akaichi&amp;journal=Int%20J%20Comput%20Appl&amp;volume=70&amp;issue=5&amp;pages=1-9&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR49">Mezaris
 V, Kompatsiaris I, Strintzis MG (2005) An ontology approach to object 
based image retrieval. In: Proceedings of the international conference 
on image processing, pp 511–514<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Mezaris%20V%2C%20Kompatsiaris%20I%2C%20Strintzis%20MG%20%282005%29%20An%20ontology%20approach%20to%20object%20based%20image%20retrieval.%20In%3A%20Proceedings%20of%20the%20international%20conference%20on%20image%20processing%2C%20pp%20511%E2%80%93514"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR50">Müller
 H, Rosset A, Garcia A, Vallée JP, Geissbuhler A (2005) Benefits of 
content-based visual data access in radiology. Radiographics 
25(3):849–858<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1148/rg.253045071"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Benefits%20of%20content-based%20visual%20data%20access%20in%20radiology&amp;author=H.%20M%C3%BCller&amp;author=A.%20Rosset&amp;author=A.%20Garcia&amp;author=JP.%20Vall%C3%A9e&amp;author=A.%20Geissbuhler&amp;journal=Radiographics&amp;volume=25&amp;issue=3&amp;pages=849-858&amp;publication_year=2005"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR51">Nandagopalan
 S, Adiga BS, Deepak N (2008) A universal model for content-based image 
retrieval. World Acad Sci Eng Technol 46:644–647<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=A%20universal%20model%20for%20content-based%20image%20retrieval&amp;author=S.%20Nandagopalan&amp;author=BS.%20Adiga&amp;author=N.%20Deepak&amp;journal=World%20Acad%20Sci%20Eng%20Technol&amp;volume=46&amp;pages=644-647&amp;publication_year=2008"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR52">Napel
 SA, Beaulieu CF, Rodriguez C, Cui J, Xu J, Gupta A et al (2010) 
Automated retrieval of CT images of liver lesions on the basis of image 
similarity: method and preliminary results. Radiology 256(1):243–252<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1148/radiol.10091694"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Automated%20retrieval%20of%20CT%20images%20of%20liver%20lesions%20on%20the%20basis%20of%20image%20similarity%3A%20method%20and%20preliminary%20results&amp;author=SA.%20Napel&amp;author=CF.%20Beaulieu&amp;author=C.%20Rodriguez&amp;author=J.%20Cui&amp;author=J.%20Xu&amp;author=A.%20Gupta&amp;journal=Radiology&amp;volume=256&amp;issue=1&amp;pages=243-252&amp;publication_year=2010"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR53">Patil PB, Kokare MB (2011) Relevance feedback in content based image retrieval: a review. J Appli Comput Sci Math 10:41–47<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Relevance%20feedback%20in%20content%20based%20image%20retrieval%3A%20a%20review&amp;author=PB.%20Patil&amp;author=MB.%20Kokare&amp;journal=J%20Appli%20Comput%20Sci%20Math&amp;volume=10&amp;pages=41-47&amp;publication_year=2011"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR54">Pentland A, Picard RW, Sclaroff S (1996) Photobook: content-based manipulation of image databases. Int J Comput Vis 18:233–254<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1007/BF00123143"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Photobook%3A%20content-based%20manipulation%20of%20image%20databases&amp;author=A.%20Pentland&amp;author=RW.%20Picard&amp;author=S.%20Sclaroff&amp;journal=Int%20J%20Comput%20Vis&amp;volume=18&amp;pages=233-254&amp;publication_year=1996"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR55">Qian
 X, Tagare HD, Fulbright RK, Long R, Antani S (2010) Optimal embedding 
for shape indexing in medical image databases. Med Image Anal 
14(3):243–254<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1016/j.media.2010.01.001"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Optimal%20embedding%20for%20shape%20indexing%20in%20medical%20image%20databases&amp;author=X.%20Qian&amp;author=HD.%20Tagare&amp;author=RK.%20Fulbright&amp;author=R.%20Long&amp;author=S.%20Antani&amp;journal=Med%20Image%20Anal&amp;volume=14&amp;issue=3&amp;pages=243-254&amp;publication_year=2010"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR56">Rajakumar
 K, Muttan S (2013) MRI image retrieval using Wavelet with Mahalanobis 
distance measurement. J Electr Eng Technol 8(5):1188–1193<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.5370/JEET.2013.8.5.1188"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=MRI%20image%20retrieval%20using%20Wavelet%20with%20Mahalanobis%20distance%20measurement&amp;author=K.%20Rajakumar&amp;author=S.%20Muttan&amp;journal=J%20Electr%20Eng%20Technol&amp;volume=8&amp;issue=5&amp;pages=1188-1193&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR57">Ramamurthy
 B, Chandran KR (2012) Content based medical image retrieval with 
texture content using gray level co-occurrence matrix and k-means 
clustering algorithms. J Comput Sci 8(7):1070–1076<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.3844/jcssp.2012.1070.1076"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Content%20based%20medical%20image%20retrieval%20with%20texture%20content%20using%20gray%20level%20co-occurrence%20matrix%20and%20k-means%20clustering%20algorithms&amp;author=B.%20Ramamurthy&amp;author=KR.%20Chandran&amp;journal=J%20Comput%20Sci&amp;volume=8&amp;issue=7&amp;pages=1070-1076&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR58">Ramamurthy
 B, Chandran KR, Meenakshi VR, Shilpa V (2012) CBMIR: content based 
medical image retrieval system using texture and intensity for dental 
images. In: International conference eco-friendly computing and 
communication systems, ICECCS, vol 305, pp 125–134<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Ramamurthy%20B%2C%20Chandran%20KR%2C%20Meenakshi%20VR%2C%20Shilpa%20V%20%282012%29%20CBMIR%3A%20content%20based%20medical%20image%20retrieval%20system%20using%20texture%20and%20intensity%20for%20dental%20images.%20In%3A%20International%20conference%20eco-friendly%20computing%20and%20communication%20systems%2C%20ICECCS%2C%20vol%20305%2C%20pp%20125%E2%80%93134"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR59">Rocchio
 JJ (1971) Relevance feedback in information retrieval: SMART retrieval 
system, 1st edn. Prentice Hall, Upper Saddle River, pp 323–341<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Relevance%20feedback%20in%20information%20retrieval%3A%20SMART%20retrieval%20system&amp;author=JJ.%20Rocchio&amp;publication_year=1971"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR60">Rui
 Y, Huang TS, Chang SF (1999) Image retrieval: current techniques, 
promising directions, and open issues. J Vis Commun Image Represent 
10(1):39–62<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1006/jvci.1999.0413"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Image%20retrieval%3A%20current%20techniques%2C%20promising%20directions%2C%20and%20open%20issues&amp;author=Y.%20Rui&amp;author=TS.%20Huang&amp;author=SF.%20Chang&amp;journal=J%20Vis%20Commun%20Image%20Represent&amp;volume=10&amp;issue=1&amp;pages=39-62&amp;publication_year=1999"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR61">Selvarani
 G, Annadurai S (2007) Medical image retrieval by combining low level 
features and DICOM features. In: International IEEE conference on 
computational intelligence and multimedia applications, pp 587–589<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Selvarani%20G%2C%20Annadurai%20S%20%282007%29%20Medical%20image%20retrieval%20by%20combining%20low%20level%20features%20and%20DICOM%20features.%20In%3A%20International%20IEEE%20conference%20on%20computational%20intelligence%20and%20multimedia%20applications%2C%20pp%20587%E2%80%93589"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR62">Seng
 WC, Mirisaee SH (2009) A content-based retrieval system for blood cells
 images. In: International IEEE conference on future computer and 
communication, pp 412–415<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Seng%20WC%2C%20Mirisaee%20SH%20%282009%29%20A%20content-based%20retrieval%20system%20for%20blood%20cells%20images.%20In%3A%20International%20IEEE%20conference%20on%20future%20computer%20and%20communication%2C%20pp%20412%E2%80%93415"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR63">Shanmugapriya
 N, Nallusamy R (2014) A new content based image retrieval system using 
GMM and relevance feedback. J Comput Sci 10(2):330–340<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.3844/jcssp.2014.330.340"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=A%20new%20content%20based%20image%20retrieval%20system%20using%20GMM%20and%20relevance%20feedback&amp;author=N.%20Shanmugapriya&amp;author=R.%20Nallusamy&amp;journal=J%20Comput%20Sci&amp;volume=10&amp;issue=2&amp;pages=330-340&amp;publication_year=2014"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR64">Shyu
 CR, Brodley CE, Kak AC, Kosaka A, Aisen AM, Broderick LS (1999) ASSERT:
 a physician-in-the-loop content-based retrieval system for HRCT image 
databases. Comput Vision Image Underst 75(1–2):111–132<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1006/cviu.1999.0768"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=ASSERT%3A%20a%20physician-in-the-loop%20content-based%20retrieval%20system%20for%20HRCT%20image%20databases&amp;author=CR.%20Shyu&amp;author=CE.%20Brodley&amp;author=AC.%20Kak&amp;author=A.%20Kosaka&amp;author=AM.%20Aisen&amp;author=LS.%20Broderick&amp;journal=Comput%20Vision%20Image%20Underst&amp;volume=75&amp;issue=1%E2%80%932&amp;pages=111-132&amp;publication_year=1999"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR65">Sidong
 L, Lei J, Weidong C, Lingfeng W, Eberl S, Fulham MJ, Dagan F (2010) 
Localized multiscale texture based retrieval of neurological image. In: 
IEEE 23rd international symposium on computer-based medical systems 
(CBMS), pp 243–248<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Sidong%20L%2C%20Lei%20J%2C%20Weidong%20C%2C%20Lingfeng%20W%2C%20Eberl%20S%2C%20Fulham%20MJ%2C%20Dagan%20F%20%282010%29%20Localized%20multiscale%20texture%20based%20retrieval%20of%20neurological%20image.%20In%3A%20IEEE%2023rd%20international%20symposium%20on%20computer-based%20medical%20systems%20%28CBMS%29%2C%20pp%20243%E2%80%93248"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR66">Singh
 P, Singh S, Kaur G (2009) Efficient techniques used in CBMIR for 
medical image retrievals. Proc World Acad Sci Eng Technol 38:434–437<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Efficient%20techniques%20used%20in%20CBMIR%20for%20medical%20image%20retrievals&amp;author=P.%20Singh&amp;author=S.%20Singh&amp;author=G.%20Kaur&amp;journal=Proc%20World%20Acad%20Sci%20Eng%20Technol&amp;volume=38&amp;pages=434-437&amp;publication_year=2009"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR67">Singh J, Kaleka JS, Sharma R (2012) Different approaches of CBIR techniques. Int J Comput Distributed Syst 1(2):76–78<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Different%20approaches%20of%20CBIR%20techniques&amp;author=J.%20Singh&amp;author=JS.%20Kaleka&amp;author=R.%20Sharma&amp;journal=Int%20J%20Comput%20Distributed%20Syst&amp;volume=1&amp;issue=2&amp;pages=76-78&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR68">Smeulders
 A, Worring M, Santini S, Gupta A, Jain R (2000) Content based image 
retrieval at the end of the early years. IEEE TransPattern Anal Mach 
Intell 22(12):1349–1380<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/34.895972"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Content%20based%20image%20retrieval%20at%20the%20end%20of%20the%20early%20years&amp;author=A.%20Smeulders&amp;author=M.%20Worring&amp;author=S.%20Santini&amp;author=A.%20Gupta&amp;author=R.%20Jain&amp;journal=IEEE%20TransPattern%20Anal%20Mach%20Intell&amp;volume=22&amp;issue=12&amp;pages=1349-1380&amp;publication_year=2000"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR69">Song Y (2012) Image Analysis for Automatic Phenotyping Measurements. Biomathematics and Statistics Scotland (BioSS). Wageningen<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Song%20Y%20%282012%29%20Image%20Analysis%20for%20Automatic%20Phenotyping%20Measurements.%20Biomathematics%20and%20Statistics%20Scotland%20%28BioSS%29.%20Wageningen"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR70">Stojmenovic
 M, Nayak A (2008) Measuring the related properties of linearity and 
elongation of point sets. In: 13th Iberoamerican congress on pattern 
recognition, CIARP 2008. Havana, Cuba, pp 102–111<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Stojmenovic%20M%2C%20Nayak%20A%20%282008%29%20Measuring%20the%20related%20properties%20of%20linearity%20and%20elongation%20of%20point%20sets.%20In%3A%2013th%20Iberoamerican%20congress%20on%20pattern%20recognition%2C%20CIARP%202008.%20Havana%2C%20Cuba%2C%20pp%20102%E2%80%93111"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR71">Stojmenovic
 M, Jevremovic A, Nayak A (2013) Fast iris detection via shape based 
circularity. In: 8th IEEE conference on industrial electronics and 
applications (ICIEA), pp 747–752<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Stojmenovic%20M%2C%20Jevremovic%20A%2C%20Nayak%20A%20%282013%29%20Fast%20iris%20detection%20via%20shape%20based%20circularity.%20In%3A%208th%20IEEE%20conference%20on%20industrial%20electronics%20and%20applications%20%28ICIEA%29%2C%20pp%20747%E2%80%93752"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR72">Su
 Z, Zhang H, Li S, Ma S (2003) Relevance feedback in content-based image
 retrieval: Bayesian framework, feature subspaces, and progressive 
learning. IEEE Trans Image Process 12(8):924–936<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/TIP.2003.815254"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Relevance%20feedback%20in%20content-based%20image%20retrieval%3A%20Bayesian%20framework%2C%20feature%20subspaces%2C%20and%20progressive%20learning&amp;author=Z.%20Su&amp;author=H.%20Zhang&amp;author=S.%20Li&amp;author=S.%20Ma&amp;journal=IEEE%20Trans%20Image%20Process&amp;volume=12&amp;issue=8&amp;pages=924-936&amp;publication_year=2003"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR73">Swarnambiga A, Vasuki SM (2013) Distance measures for medical image retrieval. Int J Imaging Syst Technol 23:9–21<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1002/ima.22031"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Distance%20measures%20for%20medical%20image%20retrieval&amp;author=A.%20Swarnambiga&amp;author=SM.%20Vasuki&amp;journal=Int%20J%20Imaging%20Syst%20Technol&amp;volume=23&amp;pages=9-21&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR74">Tieu
 K, Viola P (2003) Boosting image retrieval. In: Proceedings of the IEEE
 conference on computer vision pattern recognition, pp 228–235<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Tieu%20K%2C%20Viola%20P%20%282003%29%20Boosting%20image%20retrieval.%20In%3A%20Proceedings%20of%20the%20IEEE%20conference%20on%20computer%20vision%20pattern%20recognition%2C%20pp%20228%E2%80%93235"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR75">Wang M, Hua XS (2011) Active learning in multimedia annotation and retrieval: a survey. ACM Trans Intell Syst Technol 2(2):10–31<span class="Occurrences"><span class="Occurrence OccurrenceAMSID"><a class="gtm-reference" data-reference-type="MathSciNet" target="_blank" rel="noopener" href="http://www.ams.org/mathscinet-getitem?mr=2825107"><span><span>MathSciNet</span></span></a></span><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1145/1899412.1899414"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Active%20learning%20in%20multimedia%20annotation%20and%20retrieval%3A%20a%20survey&amp;author=M.%20Wang&amp;author=XS.%20Hua&amp;journal=ACM%20Trans%20Intell%20Syst%20Technol&amp;volume=2&amp;issue=2&amp;pages=10-31&amp;publication_year=2011"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR76">Wang
 M, Li H, Tao D, Lu K, Wu X (2012) Multimodal graph-based reranking for 
web image search. IEEE Trans Image Process 21(11):4649–4661<span class="Occurrences"><span class="Occurrence OccurrenceAMSID"><a class="gtm-reference" data-reference-type="MathSciNet" target="_blank" rel="noopener" href="http://www.ams.org/mathscinet-getitem?mr=2991643"><span><span>MathSciNet</span></span></a></span><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/TIP.2012.2207397"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Multimodal%20graph-based%20reranking%20for%20web%20image%20search&amp;author=M.%20Wang&amp;author=H.%20Li&amp;author=D.%20Tao&amp;author=K.%20Lu&amp;author=X.%20Wu&amp;journal=IEEE%20Trans%20Image%20Process&amp;volume=21&amp;issue=11&amp;pages=4649-4661&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR77">Wang
 X-Y, Li Y-W, Yang H-Y, Chen J-W (2014) An image retrieval scheme with 
relevance feedback using feature reconstruction and SVM 
reclassification. Neurocomputing 127:214–230<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1016/j.neucom.2013.08.007"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=An%20image%20retrieval%20scheme%20with%20relevance%20feedback%20using%20feature%20reconstruction%20and%20SVM%20reclassification&amp;author=X-Y.%20Wang&amp;author=Y-W.%20Li&amp;author=H-Y.%20Yang&amp;author=J-W.%20Chen&amp;journal=Neurocomputing&amp;volume=127&amp;pages=214-230&amp;publication_year=2014"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR78">Wu
 C, Tai X (2009) Application of gray level variation statistic in 
gastroscopic image retrieval. In: Eighth IEEE/ACIS international 
conference on computer and information science, pp 342–346<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Wu%20C%2C%20Tai%20X%20%282009%29%20Application%20of%20gray%20level%20variation%20statistic%20in%20gastroscopic%20image%20retrieval.%20In%3A%20Eighth%20IEEE%2FACIS%20international%20conference%20on%20computer%20and%20information%20science%2C%20pp%20342%E2%80%93346"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR79">Xiang-Yang
 W, Bei-Bei Z, Hong-Ying Y (2012) Content-based image retrieval by 
integrating color and texture features. Multimed Tools Appl 
68(3):545–569<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Content-based%20image%20retrieval%20by%20integrating%20color%20and%20texture%20features&amp;author=W.%20Xiang-Yang&amp;author=Z.%20Bei-Bei&amp;author=Y.%20Hong-Ying&amp;journal=Multimed%20Tools%20Appl&amp;volume=68&amp;issue=3&amp;pages=545-569&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR80">Xie
 Y, Chen Z, Cheng Y, Zhang K, Agrawal A, Liao WK, Choudhary A (2013) 
Detecting and tracking disease outbreaks by mining social media data. 
In: Proceedings of the twenty-third international joint conference on 
artificial intelligence (IJCAI’13)<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Xie%20Y%2C%20Chen%20Z%2C%20Cheng%20Y%2C%20Zhang%20K%2C%20Agrawal%20A%2C%20Liao%20WK%2C%20Choudhary%20A%20%282013%29%20Detecting%20and%20tracking%20disease%20outbreaks%20by%20mining%20social%20media%20data.%20In%3A%20Proceedings%20of%20the%20twenty-third%20international%20joint%20conference%20on%20artificial%20intelligence%20%28IJCAI%E2%80%9913%29"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR81">Xin
 J, Jin JS (2004) Relevance feedback for content-based image retrieval 
using Bayesian network. In: Proceedings of the pan-sydney area workshop 
on visual information processing (VIP ‘05). Australian Computer Society,
 Inc Darlinghurst Australia, pp 91–94<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Xin%20J%2C%20Jin%20JS%20%282004%29%20Relevance%20feedback%20for%20content-based%20image%20retrieval%20using%20Bayesian%20network.%20In%3A%20Proceedings%20of%20the%20pan-sydney%20area%20workshop%20on%20visual%20information%20processing%20%28VIP%20%E2%80%9805%29.%20Australian%20Computer%20Society%2C%20Inc%20Darlinghurst%20Australia%2C%20pp%2091%E2%80%9394"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR82">Xu
 X, Lee DJ, Antani S, Long L (2008) A spine X-ray image retrieval system
 using partial shape matching. IEEE Trans Inf Technol Biomed 
12(1):100–108<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/TITB.2007.904149"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=A%20spine%20X-ray%20image%20retrieval%20system%20using%20partial%20shape%20matching&amp;author=X.%20Xu&amp;author=DJ.%20Lee&amp;author=S.%20Antani&amp;author=L.%20Long&amp;journal=IEEE%20Trans%20Inf%20Technol%20Biomed&amp;volume=12&amp;issue=1&amp;pages=100-108&amp;publication_year=2008"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR83">Yogapriya
 J, Ila V (2013) An integrated framework based on texture features, 
cuckoo search and relevance vector machine for medical image retrieval 
system. Am J Appl Sci 10(11):1398–1412<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.3844/ajassp.2013.1398.1412"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=An%20integrated%20framework%20based%20on%20texture%20features%2C%20cuckoo%20search%20and%20relevance%20vector%20machine%20for%20medical%20image%20retrieval%20system&amp;author=J.%20Yogapriya&amp;author=V.%20Ila&amp;journal=Am%20J%20Appl%20Sci&amp;volume=10&amp;issue=11&amp;pages=1398-1412&amp;publication_year=2013"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR84">Yue
 J, Li Z, Liu L, Fu Z (2010) Content-based image retrieval using color 
and texture fused features. Math Comput Model 54:1121–1127<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1016/j.mcm.2010.11.044"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Content-based%20image%20retrieval%20using%20color%20and%20texture%20fused%20features&amp;author=J.%20Yue&amp;author=Z.%20Li&amp;author=L.%20Liu&amp;author=Z.%20Fu&amp;journal=Math%20Comput%20Model&amp;volume=54&amp;pages=1121-1127&amp;publication_year=2010"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR85">Zeyad
 SY, Dzulkifli M, Tanzila S, Mohammed HA, Amjad R, Al-R Mznah, Al-D 
Abdullah (2014) Content-based image retrieval using PSO and k-means 
clustering algorithm. Arab J Geosci 8(8):6211–6224. doi:<span class="ExternalRef">&nbsp;<a target="_blank" rel="noopener" href="https://doi.org/10.1007/s12517-014-1584-7"><span class="RefSource">10.1007/s12517-014-1584-7</span></a></span>
                        <span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Content-based%20image%20retrieval%20using%20PSO%20and%20k-means%20clustering%20algorithm&amp;author=SY.%20Zeyad&amp;author=M.%20Dzulkifli&amp;author=S.%20Tanzila&amp;author=HA.%20Mohammed&amp;author=R.%20Amjad&amp;author=Mznah.%20Al-R&amp;author=Abdullah.%20Al-D&amp;journal=Arab%20J%20Geosci&amp;volume=8&amp;issue=8&amp;pages=6211-6224&amp;publication_year=2014&amp;doi=10.1007%2Fs12517-014-1584-7"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR86">Zhang D, Lu G (2004) Review of shape representation and description techniques. Pattern Recognit 37(1):1–19<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1016/j.patcog.2003.07.008"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Review%20of%20shape%20representation%20and%20description%20techniques&amp;author=D.%20Zhang&amp;author=G.%20Lu&amp;journal=Pattern%20Recognit&amp;volume=37&amp;issue=1&amp;pages=1-19&amp;publication_year=2004"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR87">Zhang J, Ye L (2010) Series feature aggregation for content-based image retrieval. Comput Electr Eng 36(4):691–701<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1016/j.compeleceng.2008.11.001"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceZLBID"><a class="gtm-reference" data-reference-type="MATH" target="_blank" rel="noopener" href="http://www.emis.de/MATH-item?1214.68148"><span><span>MATH</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Series%20feature%20aggregation%20for%20content-based%20image%20retrieval&amp;author=J.%20Zhang&amp;author=L.%20Ye&amp;journal=Comput%20Electr%20Eng&amp;volume=36&amp;issue=4&amp;pages=691-701&amp;publication_year=2010"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR88">Zhang
 G, Ma ZM, He Y, Zhao T (2008) Texture characteristic extraction for 
dominant directions in content-based medical image retrieval. In: IEEE 
international conference on biomedical engineering and informatics 
(BMEI), pp 253–257<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Zhang%20G%2C%20Ma%20ZM%2C%20He%20Y%2C%20Zhao%20T%20%282008%29%20Texture%20characteristic%20extraction%20for%20dominant%20directions%20in%20content-based%20medical%20image%20retrieval.%20In%3A%20IEEE%20international%20conference%20on%20biomedical%20engineering%20and%20informatics%20%28BMEI%29%2C%20pp%20253%E2%80%93257"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR89">Zhang D, Islam MdM, Lu G (2012) A review on automatic image annotation techniques. Pattern Recognit 45:346–362<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1016/j.patcog.2011.05.013"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=A%20review%20on%20automatic%20image%20annotation%20techniques&amp;author=D.%20Zhang&amp;author=MdM.%20Islam&amp;author=G.%20Lu&amp;journal=Pattern%20Recognit&amp;volume=45&amp;pages=346-362&amp;publication_year=2012"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationContent" id="CR90">Zhi W, Wenwu Z, Peng C, Lifeng S, Shiqiang Y (2013) Social media recommendation. Soc Media Retr Comput Commun Netw. doi:<span class="ExternalRef">&nbsp;<a target="_blank" rel="noopener" href="https://doi.org/10.1007/978-1-4471-4555-43"><span class="RefSource">10.1007/978-1-4471-4555-43</span></a></span>
                        <span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Social%20media%20recommendation&amp;author=W.%20Zhi&amp;author=Z.%20Wenwu&amp;author=C.%20Peng&amp;author=S.%20Lifeng&amp;author=Y.%20Shiqiang&amp;journal=Soc%20Media%20Retr%20Comput%20Commun%20Netw&amp;publication_year=2013&amp;doi=10.1007%2F978-1-4471-4555-43"><span><span>Google Scholar</span></span></a></span></span></div></li></ol></div></section><section class="Section1 RenderAsSection1"><h2 class="Heading" id="copyrightInformation" data-role="collapsible-handle" tabindex="0">Copyright information<span class="section-icon"></span></h2><div class="ArticleCopyright content"><div class="ArticleCopyright">©&nbsp;Springer-Verlag Wien&nbsp;2016</div></div></section></div>
                        </article>
                        <aside class="section section--collapsible" id="AboutThisContent">
    <h2 class="section__heading" id="aboutcontent" data-role="collapsible-handle" tabindex="0">About this article<span class="section-icon"></span></h2>
    <div class="section__content bibliographic-information">
                <div id="crossMark" class="crossmark">
            <a data-crossmark="10.1007%2Fs13278-016-0362-9" class="gtm-crossmark" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007%2Fs13278-016-0362-9" title="Verify currency and authenticity via CrossMark">
                <span class="u-screenreader-only">CrossMark</span>
                <svg class="CrossMark" id="crossmark-icon" width="57" height="81">
                    <image width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="/springerlink-static/481091012/images/png/crossmark.png" xlink="http://www.w3.org/1999/xlink" xlink:href="/springerlink-static/481091012/images/svg/crossmark.svg"></image>
                </svg>
            </a>
        </div>

        <div class="crossmark__adjacent">
            <dl class="citation-info u-highlight-target u-mb-16" id="citeas">
    <dt class="test-cite-heading">
        Cite this article as:
    </dt>
    <dd id="citethis-text">Ayadi, M.G., Bouslimi, R. &amp; Akaichi, J. Soc. Netw. Anal. Min. (2016) 6: 53. https://doi.org/10.1007/s13278-016-0362-9</dd>
</dl>
                <ul class="bibliographic-information__list bibliographic-information__list--inline">
        <li class="bibliographic-information__item">
            <span class="bibliographic-information__title"><abbr title="Digital Object Identifier">DOI</abbr></span>
            <span class="bibliographic-information__value u-overflow-wrap" id="doi-url">https://doi.org/10.1007/s13278-016-0362-9</span>
        </li>
            <li class="bibliographic-information__item">
                <span class="bibliographic-information__title">Publisher Name</span>
                <span class="bibliographic-information__value" id="publisher-name">Springer Vienna</span>
            </li>
            <li class="bibliographic-information__item">
                <span class="bibliographic-information__title">Print ISSN</span>
                <span class="bibliographic-information__value" id="print-issn">1869-5450</span>
            </li>
            <li class="bibliographic-information__item ">
                <span class="bibliographic-information__title">Online ISSN</span>
                <span class="bibliographic-information__value" id="electronic-issn">1869-5469</span>
            </li>

        
    </ul>

            <ul class="bibliographic-information__list">
        <li class="bibliographic-information__item">
            <a id="about-journal" class="bibliographic-information__misc-links gtm-about-this" title="Visit Springer.com for information about this article's journal" href="https://www.springer.com/journal/13278/about">About this journal</a>
        </li>
                    <li class="bibliographic-information__item">
                        <a id="reprintsandpermissions-link" class="u-external" target="_blank" rel="noopener" href="https://s100.copyright.com/AppDispatchServlet?publisherName=SpringerNature&amp;orderBeanReset=true&amp;orderSource=SpringerLink&amp;copyright=Springer-Verlag+Wien&amp;author=Mouhamed+Gaith+Ayadi%2C+Riadh+Bouslimi%2C+Jalel+Akaichi&amp;issueNum=1&amp;contentID=10.1007%2Fs13278-016-0362-9&amp;openAccess=false&amp;publicationDate=2016&amp;startPage=53&amp;volumeNum=6&amp;title=A+medical+image+retrieval+scheme+with+relevance+feedback+through+a+medical+social+network&amp;imprint=Springer-Verlag+Wien&amp;publication=1869-5450" title="Visit RightsLink for information about reusing this article">Reprints and Permissions</a>
                    </li>
</ul>

        </div>
      
      
          
    </div>
</aside>

                        <div class="section section--collapsible uptodate-recommendations gtm-recommendations">
    <h2 class="uptodate-recommendations__title section__heading gtm-recommendations__title" id="uptodaterecommendations" data-role="collapsible-handle" tabindex="0">Personalised recommendations<span class="section-icon"></span></h2>
    <div class="section__content">
        <div class="uptodate-recommendations__container">
             <link rel="uptodate-inline" href="https://link.springer.com/springerlink-static/481091012/css/recommendations.css">
        </div>
    </div>
</div>
                                    <div class="sticky-banner u-interface u-js-screenreader-only" aria-hidden="true" data-component="SpringerLink.StickyBanner" data-namespace="hasButton">
                <div class="sticky-banner__container"><span class="sticky-banner__title sticky-banner__title--short u-overflow-ellipsis" title="A medical image retrieval scheme with relevance feedback through a medical social network">A medical image retrieval scheme with relevance feedback through a medical social network</span>
                        <div class="citations c-button-dropdown" data-component="SV.Dropdown" data-namespace="citationsSticky" aria-label="button with dropdown options" style="">
        <button type="button" class="c-button-dropdown__button" data-role="button-dropdown__control" aria-pressed="false" aria-expanded="false" aria-controls="Dropdown.citationsSticky-dropdown"><span class="u-overflow-ellipsis c-button-dropdown__button-title">
    <span>Cite</span>
    <span class="hide-text-small">article</span>
</span><span class="c-button-dropdown__icon"></span></button>
<div class="u-composite-layer c-button-dropdown__container" aria-hidden="true" aria-label="dropdown" id="Dropdown.citationsSticky-dropdown"><ul class="citations__content" data-role="button-dropdown__content">
    <li>
        <a href="#citeas" class="gtm-cite-dropdown">How to cite?</a>
    </li>
        <li>
            <a href="https://citation-needed.springer.com/v2/references/10.1007/s13278-016-0362-9?format=refman&amp;flavour=citation" title="Download this article's citation as a .RIS file" class="gtm-export-citation" data-gtmlabel="RIS">
                <span class="citations__extension" data-gtmlabel="RIS">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"></path></svg>
                    .RIS
                </span>
                <span class="citations__types">
                        <span>
                            Papers
                        </span>
                        <span>
                            Reference Manager
                        </span>
                        <span>
                            RefWorks
                        </span>
                        <span>
                            Zotero
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="https://citation-needed.springer.com/v2/references/10.1007/s13278-016-0362-9?format=endnote&amp;flavour=citation" title="Download this article's citation as a .ENW file" class="gtm-export-citation" data-gtmlabel="ENW">
                <span class="citations__extension" data-gtmlabel="ENW">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"></path></svg>
                    .ENW
                </span>
                <span class="citations__types">
                        <span>
                            EndNote
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="https://citation-needed.springer.com/v2/references/10.1007/s13278-016-0362-9?format=bibtex&amp;flavour=citation" title="Download this article's citation as a .BIB file" class="gtm-export-citation" data-gtmlabel="BIB">
                <span class="citations__extension" data-gtmlabel="BIB">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"></path></svg>
                    .BIB
                </span>
                <span class="citations__types">
                        <span>
                            BibTeX
                        </span>
                        <span>
                            JabRef
                        </span>
                        <span>
                            Mendeley
                        </span>
                </span>
            </a>
        </li>
</ul></div>
    </div>

                            <a class="c-button share-this gtm-shareby-sharelink-link test-shareby-sharelink-link" data-test="shareable-link" target="_blank" rel="noopener" href="https://link.springer.com/sharelink/10.1007/s13278-016-0362-9"><span>Share</span> <span class="hide-text-small">article</span></a>




                                    <a href="https://link.springer.com/content/pdf/10.1007%2Fs13278-016-0362-9.pdf" target="_blank" class="c-button c-button--blue c-button__icon-right gtm-pdf-link" title="Download this article in PDF format" rel="noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" version="1.1"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill="#fff"><g transform="translate(12.000000, 5.000000)"><path d="M7 7.3L7 1C7 0.4 6.6 0 6 0 5.4 0 5 0.4 5 1L5 7.3 3.5 5.7C3.1 5.3 2.5 5.3 2.1 5.7L2.1 5.7C1.7 6.1 1.7 6.7 2.1 7.1L5.3 10.3C5.7 10.7 6.3 10.7 6.7 10.3L9.9 7.1C10.3 6.7 10.3 6.1 9.9 5.7L9.9 5.7C9.5 5.3 8.9 5.3 8.5 5.7L7 7.3 7 7.3ZM0 13C0 12.4 0.5 12 1 12L11 12C11.6 12 12 12.4 12 13 12 13.6 11.5 14 11 14L1 14C0.4 14 0 13.6 0 13L0 13Z"></path></g></g></g></svg>
            <span class="hide-text-small">Download</span>
            <span>PDF</span>
        </a>

                </div>
            </div>




                    </div>
                    <aside class="main-sidebar-right u-interface">
                        <div data-role="sticky-wrapper">
                            <div class="main-sidebar-right__content u-composite-layer" data-component="SpringerLink.StickySidebar">
                                <div class="article-actions" id="article-actions">
                                    <h2 class="u-screenreader-only">Actions</h2>


                                    <div class="u-js-hide u-js-show-two-col">
                                        

                                                <div class="download-article test-pdf-link">
                                                            <a href="https://link.springer.com/content/pdf/10.1007%2Fs13278-016-0362-9.pdf" target="_blank" class="c-button c-button--blue c-button__icon-right gtm-pdf-link" title="Download this article in PDF format" rel="noopener">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" version="1.1"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill="#fff"><g transform="translate(12.000000, 5.000000)"><path d="M7 7.3L7 1C7 0.4 6.6 0 6 0 5.4 0 5 0.4 5 1L5 7.3 3.5 5.7C3.1 5.3 2.5 5.3 2.1 5.7L2.1 5.7C1.7 6.1 1.7 6.7 2.1 7.1L5.3 10.3C5.7 10.7 6.3 10.7 6.7 10.3L9.9 7.1C10.3 6.7 10.3 6.1 9.9 5.7L9.9 5.7C9.5 5.3 8.9 5.3 8.5 5.7L7 7.3 7 7.3ZM0 13C0 12.4 0.5 12 1 12L11 12C11.6 12 12 12.4 12 13 12 13.6 11.5 14 11 14L1 14C0.4 14 0 13.6 0 13L0 13Z"></path></g></g></g></svg>
            <span class="hide-text-small">Download</span>
            <span>PDF</span>
        </a>

                                                </div>


                                            <div class="citations c-button-dropdown" data-component="SV.Dropdown" data-namespace="citations" aria-label="button with dropdown options">
        <button type="button" class="c-button-dropdown__button" data-role="button-dropdown__control" aria-pressed="false" aria-expanded="false" aria-controls="Dropdown.citations-dropdown"><span class="u-overflow-ellipsis c-button-dropdown__button-title">
    <span>Cite</span>
    <span class="hide-text-small">article</span>
</span><span class="c-button-dropdown__icon"></span></button>
<div class="u-composite-layer c-button-dropdown__container" aria-hidden="true" aria-label="dropdown" id="Dropdown.citations-dropdown"><ul class="citations__content" data-role="button-dropdown__content">
    <li>
        <a href="#citeas" class="gtm-cite-dropdown">How to cite?</a>
    </li>
        <li>
            <a href="https://citation-needed.springer.com/v2/references/10.1007/s13278-016-0362-9?format=refman&amp;flavour=citation" title="Download this article's citation as a .RIS file" class="gtm-export-citation" data-gtmlabel="RIS">
                <span class="citations__extension" data-gtmlabel="RIS">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"></path></svg>
                    .RIS
                </span>
                <span class="citations__types">
                        <span>
                            Papers
                        </span>
                        <span>
                            Reference Manager
                        </span>
                        <span>
                            RefWorks
                        </span>
                        <span>
                            Zotero
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="https://citation-needed.springer.com/v2/references/10.1007/s13278-016-0362-9?format=endnote&amp;flavour=citation" title="Download this article's citation as a .ENW file" class="gtm-export-citation" data-gtmlabel="ENW">
                <span class="citations__extension" data-gtmlabel="ENW">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"></path></svg>
                    .ENW
                </span>
                <span class="citations__types">
                        <span>
                            EndNote
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="https://citation-needed.springer.com/v2/references/10.1007/s13278-016-0362-9?format=bibtex&amp;flavour=citation" title="Download this article's citation as a .BIB file" class="gtm-export-citation" data-gtmlabel="BIB">
                <span class="citations__extension" data-gtmlabel="BIB">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#0176C3"></path></svg>
                    .BIB
                </span>
                <span class="citations__types">
                        <span>
                            BibTeX
                        </span>
                        <span>
                            JabRef
                        </span>
                        <span>
                            Mendeley
                        </span>
                </span>
            </a>
        </li>
</ul></div>
    </div>

                                                <a class="c-button share-this gtm-shareby-sharelink-link test-shareby-sharelink-link" data-test="shareable-link" target="_blank" rel="noopener" href="https://link.springer.com/sharelink/10.1007/s13278-016-0362-9"><span>Share</span> <span class="hide-text-small">article</span></a>




                                    </div>
                                </div>
                                <nav class="toc" aria-label="article contents">
    <h2 class="u-h4 u-screenreader-only">Table of contents</h2>
    <ul id="article-contents" class="article-contents" role="menu">
            <li role="menuitem">
                <a title="Article" href="#enumeration"><span class="u-overflow-ellipsis">Article</span></a>
            </li>
            <li role="menuitem">
                <a title="Abstract" href="#Abs1"><span class="u-overflow-ellipsis">Abstract</span></a>
            </li>
            <li role="menuitem">
                <a title="1 Introduction" href="#Sec1"><span class="u-overflow-ellipsis">1 Introduction</span></a>
            </li>
            <li role="menuitem">
                <a title="2 Related work" href="#Sec2"><span class="u-overflow-ellipsis">2 Related work</span></a>
            </li>
            <li role="menuitem">
                <a title="3 Social network description and implementation" href="#Sec8"><span class="u-overflow-ellipsis">3 Social network description and implementation</span></a>
            </li>
            <li role="menuitem">
                <a title="4 The proposed methodology" href="#Sec9"><span class="u-overflow-ellipsis">4 The proposed methodology</span></a>
            </li>
            <li role="menuitem">
                <a title="5 The evaluation of our methodology’s impact in the decision-making process" href="#Sec25"><span class="u-overflow-ellipsis">5 The evaluation of our methodology’s impact in the decision-making process</span></a>
            </li>
            <li role="menuitem">
                <a title="6 Conclusion and future work" href="#Sec28"><span class="u-overflow-ellipsis">6 Conclusion and future work</span></a>
            </li>
            <li role="menuitem">
                <a title="Footnotes" href="#Footnotes"><span class="u-overflow-ellipsis">Footnotes</span></a>
            </li>
            <li role="menuitem">
                <a title="References" href="#Bib1"><span class="u-overflow-ellipsis">References</span></a>
            </li>
            <li role="menuitem">
                <a title="Copyright information" href="#copyrightInformation"><span class="u-overflow-ellipsis">Copyright information</span></a>
            </li>
            
            <li role="menuitem">
                <a title="About this article" href="#aboutcontent"><span class="u-overflow-ellipsis">About this article</span></a>
            </li>
    </ul>
</nav>

                            </div>
                                <div class="skyscraper-ad u-hide" data-component="SpringerLink.GoogleAds" data-namespace="skyscraper"></div>

                        </div>
                    </aside>
                </div>
            </main>
                <footer class="footer u-interface">
        <div class="footer__aside-wrapper">
            <div class="footer__content">
                <div class="footer__aside">
                    <p class="footer__strapline">Over 10 million scientific documents at your fingertips</p>
                                <div class="footer__edition c-button-dropdown c-button-dropdown--ghost" data-component="SpringerLink.EditionSwitcher" aria-label="button with dropdown options">
                                    <button type="button" title="Switch between Academic &amp; Corporate Edition" class="c-button-dropdown__button" data-role="button-dropdown__control" aria-pressed="false" aria-expanded="false" aria-controls="EditionSwitcher.0111119311-dropdown"><span class="u-overflow-ellipsis c-button-dropdown__button-title">Academic Edition</span><span class="c-button-dropdown__icon"></span></button>
                                    <div class="u-composite-layer c-button-dropdown__container" aria-hidden="true" aria-label="dropdown" id="EditionSwitcher.0111119311-dropdown"><ul data-role="button-dropdown__content">
                                        <li class="selected"><a href="https://link.springer.com/siteEdition/link" id="siteedition-academic-link">Academic Edition</a></li>
                                        <li><a href="https://link.springer.com/siteEdition/rd" id="siteedition-corporate-link" tabindex="-1">Corporate Edition</a></li>
                                    </ul></div>
                                </div>
                </div>
            </div>
        </div>
        <div class="footer__content">
            <ul class="footer__nav">
                <li>
                    <a href="https://link.springer.com/">Home</a>
                </li>
                <li>
                    <a href="https://link.springer.com/impressum">Impressum</a>
                </li>
                <li>
                    <a href="https://link.springer.com/termsandconditions">Legal information</a>
                </li>
                <li>
                    <a href="https://link.springer.com/privacystatement">Privacy statement</a>
                </li>
                <li>
                    <a href="https://link.springer.com/cookiepolicy">How we use cookies</a>
                </li>
                <li>
                    <a href="https://link.springer.com/accessibility" class="gtm-footer-accessibility">Accessibility</a>
                </li>
                <li>
                    <a id="contactus-footer-link" href="https://link.springer.com/contactus">Contact us</a>
                </li>
            </ul>
            <a class="parent-logo" target="_blank" rel="noopener" href="https://www.springernature.com/" title="Go to Springer Nature">
                <span class="u-screenreader-only">Springer Nature</span>
                <svg width="125" height="12">
                    <image width="125" height="12" alt="Springer Nature logo" src="/springerlink-static/481091012/images/png/springernature.png" xlink="http://www.w3.org/1999/xlink" xlink:href="/springerlink-static/481091012/images/svg/springernature.svg">
                    </image>
                </svg>
            </a>

            <p class="footer__copyright">© 2017 Springer International Publishing AG. Part of <a target="_blank" rel="noopener" href="https://www.springernature.com/">Springer Nature</a>.</p>

                <p class="footer__user-access-info">
                    <span>Not logged in</span>
                    <span>CAPES MEC (3000197460) - Universidade Tecnologica Federal do Parana (3000201946)</span>
                    <span>168.181.51.234</span>
                </p>
        </div>
    </footer>

        </div>
          <script type="text/javascript">
    (function() {
        document.addEventListener('readystatechange', function () {
            if (document.readyState === 'complete') {
                var linkEl = document.querySelector('.js-ctm');
                if (window.matchMedia && window.matchMedia(linkEl.media).matches) {
                    var scriptMathJax = document.createElement('script');
                    scriptMathJax.async = true;
                    scriptMathJax.src = '/springerlink-static/481091012/js/mathJax.js';
                    var s0 = document.getElementsByTagName('script')[0];
                    s0.parentNode.insertBefore(scriptMathJax, s0);
                }
            }
        });
    })();
</script>

<script type="text/javascript">
    (function() {
        var linkEl = document.querySelector('.js-ctm');
        var scriptsList = [];
        var polyfillFeatures = '';

        if (window.matchMedia && window.matchMedia(linkEl.media).matches) {
            (function(h){h.className = h.className.replace('no-js', 'js')})(document.documentElement);
            
            window.SpringerLink = window.SpringerLink || {};
            window.SpringerLink.staticLocation = '/springerlink-static/481091012';
            
            polyfillFeatures = 'default,fetch,Promise,Object.setPrototypeOf,Object.entries,Number.isInteger,MutationObserver,startsWith,Array.prototype.includes,Array.from';
            
            scriptsList = [
                // 'https://cdnjs.cloudflare.com/ajax/libs/airbrake-js/1.0.7/client.min.js',
                window.SpringerLink.staticLocation + '/js/jquery-3.3.1.min.js',
                'https://cdn.polyfill.io/v2/polyfill.js?features=' + polyfillFeatures,
                window.SpringerLink.staticLocation + '/js/main.js',
                        'https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js'
            ];

            scriptsList.forEach(function(script) {
                var tag = document.createElement('script');
                
                tag.async = false;
                tag.src = script;

                document.body.appendChild(tag);
            });
        }
    })();
</script><script src="jquery-3.js"></script><script src="polyfill.js"></script><script src="main.js"></script><script src="6b2ec9cd-5ace-4387-96d2-963e596401c6.js"></script>

<script type="text/javascript">
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>

<script type="text/javascript" class="optanon-category-2">

   function viewport() {
        if (document.documentElement.clientWidth < 620) {
            size = 'small';
        }
        else if(document.documentElement.clientWidth < 1075 ) {
            size = 'medium';
        }
        else {
            size = 'wide';
        }
        return size;
    };

    function reportForMouseEvents(linkCssSelectors, nolardUrl, experiment, abgroup) {
        var counter = 0;
        linkCssSelectors.forEach(function(cssSelector) {
            $('body').delegate(cssSelector, 'click', function() {
                if(counter == 0) {
                    reportConversion(nolardUrl, experiment, abgroup);
                    counter++;
                }
            });
        });
    };

    function reportConversion(nolardUrl, experiment, abgroup) {
        $.ajax({ url: nolardUrl + '/convert/' + experiment + '/' + abgroup });
    };

    function reportForMouseEvent(linkCssSelector, nolardUrl, experiment, abgroup) {
        var counter = 0;
        var elem = document.querySelector(linkCssSelector)

        if (elem.addEventListener) {
            elem.addEventListener("click", function(e) {
                if(counter == 0) {
                    reportConversion(nolardUrl, experiment, abgroup);
                    counter++;
                }
            });
        } else  {
            elem.attachEvent("click", function(e) {
                if(counter == 0) {
                    reportConversion(nolardUrl, experiment, abgroup);
                    counter++;
                }
            });
        }
    };

    function reportParticipation(nolardUrl, experiment, abgroup, participations) {
        if (participations)
            participations.push(experiment + '/' + abgroup)

        var xhr = new XMLHttpRequest()
        xhr.open('GET', nolardUrl + '/participate/' + experiment + '/' + abgroup);
        xhr.send();
    };

        function recordCurrentExperiment() {
            var participations = [];
            if (document.querySelector('#reprintsandpermissions-bottom-link') !== null) {
                reportParticipation('https://ab-reporting.live.cf.public.springer.com', 'bunsen-article-reprints-permissions-location', 'featureoff', participations);
                reportForMouseEvent('#reprintsandpermissions-bottom-link', 'https://ab-reporting.live.cf.public.springer.com', 'bunsen-article-reprints-permissions-location', 'featureoff');
            } else if(document.querySelector('#reprintsandpermissions-top-link') !== null) {
                reportParticipation('https://ab-reporting.live.cf.public.springer.com', 'bunsen-article-reprints-permissions-location', 'featureon', participations);
                reportForMouseEvent('#reprintsandpermissions-top-link', 'https://ab-reporting.live.cf.public.springer.com', 'bunsen-article-reprints-permissions-location', 'featureon');
            }
            return participations;
        };

</script>

<script type="text/javascript" class="optanon-category-2">
    window.onload = function() {
    var linkEl = document.querySelector('.js-ctm');


        if (window.matchMedia && window.matchMedia(linkEl.media).matches) {
          var bookProductType = dataLayer[0].content && dataLayer[0].content.book && dataLayer[0].content.book.bookProductType
          var hasBody = dataLayer[0]['Has Body'] == 'Y';
          var userHasAccess = dataLayer[0]['HasAccess'] == 'Y';
          var liveOrStatic = dataLayer[0].content && dataLayer[0].content.version

          // Baseline reference work page
          var isReferenceWork = window.location.pathname.startsWith('/referencework/');
          var isReferenceWorkEntry = window.location.pathname.startsWith('/referenceworkentry/');
          var isEncOrDict = bookProductType == 'Encyclop(a)edia' || bookProductType == 'Dictionary';

          // Baseline static reference work entry page pdf download
          if (isReferenceWorkEntry && isEncOrDict && $('.test-rwepdf-link').length) {
            reportParticipation('https://ab-reporting.live.cf.public.springer.com', 'bunsen-'+liveOrStatic+'-rwepage-enc-pdf-download', 'baseline');
            reportForMouseEvents(['.test-rwepdf-link'], 'https://ab-reporting.live.cf.public.springer.com', 'bunsen-'+liveOrStatic+'-rwepage-enc-pdf-download', 'baseline');
          }
          if (isReferenceWorkEntry && !isEncOrDict && $('.gtm-pdf-link').length) {
            reportParticipation('https://ab-reporting.live.cf.public.springer.com', 'bunsen-'+liveOrStatic+'-rwepage-nonenc-pdf-download', 'baseline');
            reportForMouseEvents(['.gtm-pdf-link'], 'https://ab-reporting.live.cf.public.springer.com', 'bunsen-'+liveOrStatic+'-rwepage-nonenc-pdf-download', 'baseline');

            if (($('.test-bookpdf-link').length || $('.test-bookepub-link').length)) {
                reportParticipation('https://ab-reporting.live.cf.public.springer.com', 'bunsen-'+liveOrStatic+'-rwpage-nonenc-book-download', 'baseline');
                reportForMouseEvents(['.test-bookpdf-link', 'test-bookepub-link'], 'https://ab-reporting.live.cf.public.springer.com', 'bunsen-'+liveOrStatic+'-rwpage-nonenc-book-download', 'baseline');
            }
          }

          // Baseline static reference work page pdf download
          if (isReferenceWork && isEncOrDict && ($('.test-bookpdf-link').length || $('.test-bookepub-link').length)) {
            reportParticipation('https://ab-reporting.live.cf.public.springer.com', 'bunsen-'+liveOrStatic+'-rwpage-enc-book-download', 'baseline');
              reportForMouseEvents(['.test-bookpdf-link', 'test-bookepub-link'], 'https://ab-reporting.live.cf.public.springer.com', 'bunsen-'+liveOrStatic+'-rwpage-enc-book-download', 'baseline');
          }

          if (isReferenceWork && !isEncOrDict) {
            if (($('.test-bookpdf-link').length || $('.test-bookepub-link').length)) {
                reportParticipation('https://ab-reporting.live.cf.public.springer.com', 'bunsen-'+liveOrStatic+'-rwpage-nonenc-book-download', 'baseline');
                reportForMouseEvents(['.test-bookpdf-link', 'test-bookepub-link'], 'https://ab-reporting.live.cf.public.springer.com', 'bunsen-'+liveOrStatic+'-rwpage-nonenc-book-download', 'baseline');
            }

            if ($('.test-book-toc-download-link').length) {
                reportParticipation('https://ab-reporting.live.cf.public.springer.com', 'bunsen-'+liveOrStatic+'-rwpage-nonenc-pdf-download', 'baseline');
                reportForMouseEvents(['.test-book-toc-download-link'], 'https://ab-reporting.live.cf.public.springer.com', 'bunsen-'+liveOrStatic+'-rwpage-nonenc-pdf-download', 'baseline');
            }
          }

          // Baseline static rwe page fulltext html view with access
          if (isReferenceWorkEntry && isEncOrDict && hasBody) {
            reportParticipation('https://ab-reporting.live.cf.public.springer.com', 'bunsen-'+liveOrStatic+'-rwepage-enc-html-download', 'baseline');
            if (userHasAccess)
                reportConversion('https://ab-reporting.live.cf.public.springer.com', 'bunsen-'+liveOrStatic+'-rwepage-enc-html-download', 'baseline');
          }
          if (isReferenceWorkEntry && !isEncOrDict && hasBody) {
            reportParticipation('https://ab-reporting.live.cf.public.springer.com', 'bunsen-'+liveOrStatic+'-rwepage-nonenc-html-download', 'baseline');
            if (userHasAccess)
                reportConversion('https://ab-reporting.live.cf.public.springer.com', 'bunsen-'+liveOrStatic+'-rwepage-nonenc-html-download', 'baseline');
          }
      }

            if (window.matchMedia && window.matchMedia(linkEl.media).matches) {
                recordCurrentExperiment()
            }
   };

</script>

    <script type="text/javascript" id="googletag-push">
        
            var adSlot = '270604982/springerlink/13278/article';
        

        var definedSlots = [
                {slot: [728, 90], containerName: 'doubleclick-leaderboard-ad'},
                {slot: [160, 600], containerName: 'doubleclick-ad'},
        ];
    </script>


        
        <span id="chat-widget" class="u-hide"></span>
                    <noscript>
                <img aria-hidden="true" role="presentation" src="https://ssl-springer.met.vgwort.de/na/pw-vgzm.415900-10.1007-s13278-016-0362-9" width='1' height='1' alt='' />
            </noscript>

    

<div style="display: none; visibility: hidden;"><script id="gpt-control-script" src="gpt.js"></script>

<script id="gpt-control-setup">var googletag=googletag||{};googletag.cmd=googletag.cmd||[];</script></div><button href="javascript:;" title="focus catcher" class="js-focus-catcher u-screenreader-only" tabindex="-1"></button><script type="text/javascript" id="">window.Krux||((Krux=function(){Krux.q.push(arguments)}).q=[]);
googletag.cmd.push(function(){googletag.pubads().setTargeting("doi",google_tag_manager["GTM-WCF9Z9"].macro(8));googletag.pubads().setTargeting("kwrd",google_tag_manager["GTM-WCF9Z9"].macro(9));googletag.pubads().setTargeting("pmc",google_tag_manager["GTM-WCF9Z9"].macro(10));googletag.pubads().setTargeting("BPID",google_tag_manager["GTM-WCF9Z9"].macro(11));googletag.pubads().setTargeting("edition",google_tag_manager["GTM-WCF9Z9"].macro(12));googletag.pubads().setTargeting("sucode",google_tag_manager["GTM-WCF9Z9"].macro(13));googletag.pubads().setTargeting("eissn",google_tag_manager["GTM-WCF9Z9"].macro(14));googletag.pubads().setTargeting("pissn",
google_tag_manager["GTM-WCF9Z9"].macro(15));googletag.pubads().setTargeting("eisbn",google_tag_manager["GTM-WCF9Z9"].macro(16));googletag.pubads().setTargeting("pisbn",google_tag_manager["GTM-WCF9Z9"].macro(17));googletag.pubads().setTargeting("logged",google_tag_manager["GTM-WCF9Z9"].macro(18));googletag.pubads().setTargeting("ksg",Krux.segments);googletag.pubads().setTargeting("kuid",Krux.uid);googletag.pubads().setRequestNonPersonalizedAds(google_tag_manager["GTM-WCF9Z9"].macro(22)?0:1);googletag.pubads().enableSingleRequest();googletag.pubads().enableAsyncRendering();googletag.enableServices()});</script><div id="optanon" class="modern"><div id="optanon-popup-bg"></div><div id="optanon-popup-wrapper" role="dialog" aria-modal="true" tabindex="-1" lang="en-GB"><div id="optanon-popup-top"><a href="#" onclick="Optanon.TriggerGoogleAnalyticsEvent('OneTrust Cookie Consent', 'Preferences Close Button');" class="optanon-close-link optanon-close optanon-close-ui" title="Close Preference Centre"><div id="optanon-close" style="background: url(https://optanon.blob.core.windows.net/skins/default_flat_bottom_two_button_white/v2/images/optanon-pop-up-close.png);width:34px;height:34px;"></div></a></div><div id="optanon-popup-body"><div id="optanon-popup-body-left"><div id="optanon-popup-body-left-shading"></div><div id="optanon-branding-top-logo" style="background-image: url(https://optanon.blob.core.windows.net/logos/5138/5138:link.springer.com/springer.png) !important;"></div><ul id="optanon-menu"><li class="menu-item-on menu-item-about" title="Your Privacy"><p><a href="#">Your Privacy</a></p></li><li class="menu-item-necessary menu-item-on" title="Strictly Necessary Cookies"><p><a href="#">Strictly Necessary Cookies</a></p></li><li class="menu-item-on menu-item-performance" title="Performance Cookies"><p><a href="#">Performance Cookies</a></p></li><li class="menu-item-on menu-item-functional" title="Functional Cookies"><p><a href="#">Functional Cookies</a></p></li><li class="menu-item-on menu-item-advertising" title="Targeting Cookies"><p><a href="#">Targeting Cookies</a></p></li><li class="menu-item-moreinfo menu-item-off" title="More Information"><p><a target="_blank" href="https://link.springer.com/cookiepolicy" onclick="Optanon.TriggerGoogleAnalyticsEvent('OneTrust Cookie Consent', 'Preferences Cookie Policy');">More Information</a></p></li></ul></div><div id="optanon-popup-body-right"><h2 aria-label="true">Privacy Preference Centre</h2><h3></h3><div id="optanon-popup-more-info-bar"><div class="optanon-status"><div class="optanon-status-editable"><form><fieldset><p><input value="check" id="chkMain" checked="checked" class="optanon-status-checkbox" type="checkbox"><label for="chkMain">Active</label></p></fieldset></form></div><div class="optanon-status-always-active optanon-status-on"><p>Always Active</p></div></div></div><div id="optanon-main-info-text"></div></div><div class="optanon-bottom-spacer"></div></div><div id="optanon-popup-bottom"> <a href="https://onetrust.com/poweredbyonetrust" target="_blank"><div id="optanon-popup-bottom-logo" style="background: url(https://optanon.blob.core.windows.net/skins/default_flat_bottom_two_button_white/v2/images/cookie-collective-top-bottom.png);width:155px;height:35px;" title="powered by OneTrust"></div></a><div class="optanon-button-wrapper optanon-save-settings-button optanon-close optanon-close-consent"><div class="optanon-white-button-left"></div><div class="optanon-white-button-middle"><a href="#" onclick="Optanon.TriggerGoogleAnalyticsEvent('OneTrust Cookie Consent', 'Preferences Save Settings');">Save Settings</a></div><div class="optanon-white-button-right"></div></div><div class="optanon-button-wrapper optanon-allow-all-button optanon-allow-all" style="display: none;"><div class="optanon-white-button-left"></div><div class="optanon-white-button-middle"><a href="#" onclick="Optanon.TriggerGoogleAnalyticsEvent('OneTrust Cookie Consent', 'Preferences Allow All');">Allow All</a></div><div class="optanon-white-button-right"></div></div></div></div></div><div class="optanon-alert-box-wrapper  " style="bottom: 0px;"><div class="optanon-alert-box-bottom-top"><div class="optanon-alert-box-corner-close"><a class="optanon-alert-box-close" href="#" title="Close Banner" onclick="Optanon.TriggerGoogleAnalyticsEvent('OneTrust Cookie Consent', 'Banner Close Button');"></a></div></div><div class="optanon-alert-box-bg"><div class="optanon-alert-box-logo"> </div><div class="optanon-alert-box-body"><p>We
 use cookies to personalise content and ads, to provide social media 
features and to analyse our traffic. We also share information about 
your use of our site with our social media, advertising and analytics 
partners in accordance with our <a href="https://link.springer.com/privacystatement">Privacy Statement</a>. You can manage your preferences in Manage Cookies.</p></div><div class="optanon-clearfix"></div><div class="optanon-alert-box-button-container"><div class="optanon-alert-box-button optanon-button-close"><div class="optanon-alert-box-button-middle"><a class="optanon-alert-box-close" href="#">Close</a></div></div><div class="optanon-alert-box-button optanon-button-allow"><div class="optanon-alert-box-button-middle"><a class="optanon-allow-all" href="#" onclick="Optanon.TriggerGoogleAnalyticsEvent('OneTrust Cookie Consent', 'Banner Accept Cookies');">OK</a></div></div><div class="optanon-alert-box-button optanon-button-more"><div class="optanon-alert-box-button-middle"><a class="optanon-toggle-display" href="#" onclick="Optanon.TriggerGoogleAnalyticsEvent('OneTrust Cookie Consent', 'Banner Open Preferences');">Manage Cookies</a></div></div></div><div class="optanon-clearfix optanon-alert-box-bottom-padding"></div></div></div><script type="text/javascript" id="">(function(d,z){function n(){for(var a in dataLayer)if(dataLayer.hasOwnProperty(a)&&dataLayer[a]["Event Category"])return dataLayer[a]["Event Category"];return"warning: no event category"}function v(a,b,c,h){"undefined"!==typeof dataLayer?dataLayer.push({event:"Scroll To Section",eventCategory:n(),eventAction:a,eventLabel:b,eventValue:1,eventNonInteraction:h}):("undefined"!==typeof ga&&ga("send","event",n(),a,b,1,{nonInteraction:1}),"undefined"!==typeof _gaq&&_gaq.push(["_trackEvent",n(),a,b,1,!0]))}
function k(b,g,c,h){if(-1===a[c].cache.indexOf(c+"-"+h)&&document.querySelectorAll(b).length){var f="viewed"===h?document.querySelectorAll(b)[0].getBoundingClientRect().height:0,x=document.querySelectorAll(b)[0].getBoundingClientRect().top+d.pageYOffset;g>=x+f&&(v(p(c)+" "+p(h),b,c,!1),a[c].cache.push(c+"-"+h),q++)}}function r(a){a=document.querySelectorAll(a);return a.length?a[0].offsetHeight||0:0}function m(b,g,c){var f=d.pageYOffset+d.innerHeight;if(q>=w)"handleClick"===g&&d.removeEventListener(b,
t),"throttle"===g&&d.removeEventListener(b,u);else if(c)setTimeout(function(){0<r(a[c].content)&&(a[c].reached&&k(a[c].content,f,c,"reached"),a[c].viewed&&k(a[c].content,f,c,"viewed"))},10);else for(var e in a)a.hasOwnProperty(e)&&0<r(a[e].content)&&(a[e].reached&&k(a[e].content,f,e,"reached"),a[e].viewed&&k(a[e].content,f,e,"viewed"))}function y(b){b=document.querySelectorAll(a[b].content).length;return 0<b?!0:!1}function p(a){return a.charAt(0).toUpperCase()+a.slice(1)}function u(a,b){var c,f,e,
g=null,d=0,k=function(){d=new Date;g=null;e=a.apply(c,f)};return function(){var h=new Date;d||(d=h);var l=b-(h-d);c=this;f=arguments;0>=l?(clearTimeout(g),g=null,d=h,e=a.apply(c,f)):g||(g=setTimeout(k,l));return e}}function t(a,b){return function(c){a(c,b)}}var w=0,q=0,a={recommendations:{content:".gtm-recommendations iframe",clickable:".gtm-recommendations .gtm-recommendations__title",exists:!1,reached:!1,viewed:!0,size:0,cache:[]},"abstract":{content:"Section.Abstract",clickable:null,exists:!0,
reached:!1,viewed:!0,size:0,cache:[]},references:{content:".Bibliography \x3e .content, [id^\x3dBib1] \x3e .content",clickable:".Bibliography \x3e .Heading, [id^\x3dBib1] \x3e .Heading",exists:!0,reached:!0,viewed:!0,size:0,cache:[]},about:{content:"#AboutThisContent",clickable:"#AboutThisContent \x3e #aboutcontent",exists:!1,reached:!1,viewed:!0,size:0,cache:[]}};d.addEventListener("scroll",u(function(){m("scroll","throttle")},500));d.addEventListener("orientationchange",u(function(){m("orientationchange",
"throttle")},500));for(var b in a)if(a.hasOwnProperty(b)&&(a[b].exists&&a[b].size++,a[b].reached&&a[b].size++,a[b].viewed&&a[b].size++,w+=a[b].size,a[b].exists&&y(b)||!a[b].exists)){a[b].exists&&-1===a[b].cache.indexOf(b+"-exists")&&(v(p(b)+" Exists",a[b].content,b,!0),a[b].cache.push(b+"-exists"),q++);if(0<r(a[b].content)){var l=d.pageYOffset+d.innerHeight;a[b].reached&&k(a[b].content,l,b,"reached");a[b].viewed&&k(a[b].content,l,b,"viewed")}a[b].clickable&&(l=document.querySelectorAll(a[b].clickable)[0])&&
(l.addEventListener("click",t(function(a,b){m("click","handleClick",b)},b)),l.addEventListener("click",t(function(a,b){13==a.keyCode&&m("click","handleClick",b)},b)))}})(window);</script><script type="text/javascript" id="gtm-recommendations-script" src="entry-point"></script><script type="text/javascript" id="">window.Krux||((Krux=function(){Krux.q.push(arguments)}).q=[]);(function(){var a=document.createElement("script");a.type="text/javascript";a.async=!0;a.src=("https:"===location.protocol?"https:":"http:")+"//cdn.krxd.net/controltag/KDqyaFZ_.js";var b=document.getElementsByTagName("script")[0];b.parentNode.insertBefore(a,b)})();</script><script type="text/javascript" id="">window.Krux||((Krux=function(){Krux.q.push(arguments)}).q=[]);(function(){function b(a){a="kx"+a;return window.localStorage?window.localStorage[a]||"":navigator.cookieEnabled?(a=document.cookie.match(a+"\x3d([^;]*)"))&&unescape(a[1])||"":""}Krux.user=b("user");Krux.segments=b("segs")&&b("segs").split(",")||[]})();</script><script type="text/javascript" id="">var allowed=google_tag_manager["GTM-WCF9Z9"].macro(38);Krux("consent:set",{dc:allowed,al:allowed,tg:allowed,cd:!1,sh:!1,re:!1},function(a,b){a?console.error(a):console.log("consent flags set ",b)});</script><script type="text/javascript" id="">window.dataLayer.push({ksg:Krux.segments,kuid:Krux.uid});</script><img class="pub_300x250 pub_300x250m pub_728x90 text-ad textAd text_ad text_ads text-ads text-ad-links" style="width: 1px !important; height: 1px !important; position: absolute !important; left: -10000px !important; top: -1000px !important;"></body><iframe id="kx-proxy-KDqyaFZ_" src="https://cdn.krxd.net/partnerjs/xdi/proxy.3d2100fd7107262ecb55ce6847f01fa5.html#%21kxcid=KDqyaFZ_&amp;kxt=https%3A%2F%2Flink.springer.com&amp;kxcl=cdn&amp;kxp=" style="display: none; visibility: hidden; height: 0; width: 0;"></iframe></html>