% METODOLOGIA------------------------------------------------------------------

\chapter{METODOLOGIA}
\label{chap:metodologia}
Metodologia, ferramenta e base utilizada na pesquisa.

SVM:

Inventado originalmente por Vladimir Vapnik e Alexey Chervonenkis em 1963 com modificações para permitir classificações não-lineares em 1992 por Vapnik, os SVM são modelos de aprendizagem supervisionada utilizados nas tarefas de classificação e regressão.
Na SVM, os dados são representados por pontos num espaço n-dimensional, sendo n o número de atributos dos seus dados. SVM são classificadores binários lineares pois tenta buscar um hiperplano que separe linearmente os dados em dois grupos distintos. Podem existir diversos hiperplanos de separação, porém a melhor escolha é o plano que possui maior margem, ou seja, o plano que contém a maior separação entre os dados mais próximos, dessa forma diminuindo o erro de generalização do classificador. Novos elementos que forem classificados serão julgados de acordo com qual lado do hiperplano eles se encontram.
Casos que não podem ser linearmente separados podem ser modificados através das funções de kernel: funções que convertem os dados em espaços de maiores dimensões onde a análise pode ser feita e os dados separados linearmente por um hiperplano. Este hiperplano encontrado pode não ser uma reta nos planos originais dos dados.
São algoritmos que funcionam bem em espaços de muitas dimensões, até mesmo maior do que o número de exemplos da base de dados, porém não são eficientes quando os dados possuem muito ruído.
Em problemas de regressão as técnicas principais do SVM ainda podem ser usadas. Um hiperplano pode ser encontrado que divide o conjunto de dados em 2, porém a margem de separação passa a ser uma margem de erro aceitável, sendo que os dados que caiam dentro dessa margem de erro podem ser calculados seus valores a partir do hiperplano de separação e os dados que caem fora das margens são descartados como pontos fora da curva.

Dados:

A base de dados contava originalmente com 790 instâncias, porém apenas 500 foram disponibilizadas de forma pública devido a confidencialidades da página do Facebook em questão. Todas as instâncias contam com 19 atributos, após a criação de um modelo de previsão de performance da postagem, 7 desses atributos foram considerados como entrada e os outros 12 como medidas de performance.
Os 7 atributos de entrada são: a quantidade de likes totais da página no momento da postagem em questão, o tipo de postagem (foto, vídeo, link ou status), a categoria (atributo interno da empresa adicionado manualmente), mês, dia da semana e hora da postagem, cada um em um atributo separado e se o post foi pago para ter mais publicidade. Os outros 12 atributos de performance são: quantas pessoas o post atingiu, quantas vezes a postagem foi vista, quantos usuários interagiram com a postagem ao todo, quantidade de pessoas que interagiram com a postagem sem “compartilhar com outras pessoas” através de histórias, quando vezes o conteúdo da postagem foi clicado sem “compartilhar com outras pessoas” através de histórias, quantas vezes a postagem foi vista por pessoas que curtem a página em questão, quantas pessoas que curtem a página viram a postagem, quantas pessoas que curtem a página e interagiram com a postagem, quantidade de comentários, curtidas, compartilhamentos e o total desses últimos 3 atributos. Todos esses atributos, com exceção da categoria que foi adicionada manualmente por um funcionário da empresa, foram retirados das estatísticas da página da empresa de cosméticos diretamente do Facebook.
Após o processo de mineração utilizando SVM, foi gerado um modelo para cada um dos 12 atributos de performance utilizando os 7 atributos de entrada. Os resultados dos modelos foram comparados com os valores reais retirados dos dados através de diferença absoluta e percentual, além da média de erro absoluto percentual para cada um dos 12 atributos. Para facilitar a análise dos atributos de saída, estes foram divididos em visualização e interação, sendo que os de interação tiveram como melhor resultado 27\% de erro e o melhor de visualização com 37\% de erro. Especulasse que os erros maiores com relação a previsão de visualização se devem a maiores fatores fora do controle dos dados e dos modelos, como a aleatoriedade do algoritmo de feed do Facebook.

SOM:

\cite{ultsch1995self} SOFM são usados para classificar dados de altas dimensões mapeando os dados para dimensões menores e quanto combinado com o método de U-Matrix ajuda a identificar clusteres de dados de forma mais fácil que métodos tradicionais de clusterização. Apesar de ser comparado com o método k-means por causa do seu método de particionamento, o SOFM não é idêntico e em alguns casos tem melhor performance que o seu comparativo. Kohonen's self organizing map representa um modelo de rede neural utilizado em aprendizagem não supervisionada, DEFINIÇÃO DE NÃO SUPERVISIONADO: o que significa que os resultados são provenientes das propriedades inerentes dos dados em si, não tendo um mestre que diz o que é certo ou errado e que não é preciso conhecer a estrutura dos dados anteriormente à aplicação do algoritmo. O algoritmo adapta dados em altas dimensões numa grade de duas dimensões e preservando a vizinhança na topologia do mapa as estruturas dos dados podem ser descobertas explorando o mapa, e se a propriedade de preservação de vizinhança estiver correta então dados próximos terão uma correspondência na mesma latitude no mapa de Kohonen, ou seja, os clusteres de dados de altas dimensões estaram também em clusteres na projeção em menor dimensão. Como as distâncias entre os pontos do mapa estão igualmente distribuídos, somente o método de SOFM não nos indica a presença ou não de clusteres. O método de U-Matrix tem como ideia visualizar a topologia dos mapas de atributos. Ao analisar os pesos em cada ponto da grade com seu vizinho e mostrando a distância entre os dois vizinhos como altura, temos uma figura em tres dimensões do mapa de Kohonen. U-Matrix, curto para unified distance matrix, contém uma aproximação geométrica do vetor distribuição da rede de Kohonen. Os vales representam dados que estão próximos enquanto colinas representam uma separação e maior distância entre os vizinhos.

\cite{kolehmainen2004data} seja \textbf{X} uma matriz de dados com \textit{p} atributos e \textit{n} amostras, um mapa auto-organizado consiste de M neurônios organizados numa grade de duas dimensões, cada uma das células contendo seu vetor de peso para cada uma das variáveis. FORMULA w\_m =( w\_m1,..w\_mp) , (m=1...M). Os pesos são inicializados para valores aleatórios. Para cada vetor \textbf{x}\_i é encontrado o melhor neurônio (BEST MATCHING UNIT), ou seja o neurônio com a maior proximidade com o vetor de entrada, ou seja o BMU é o neurônio que tenha a menor distância Euclidiana do vetor de entrada e dos pesos da matriz. Os pesos dos vizinhos (que são escolhidos de acordo com uma função) são corrigidos junto com o peso do próprio BMU se utilizando do contador de interações, o index do BMU e o index do neurônio. Processo básico do SOM: 1. econtre o BMU para um vetor de entrada de acordo com a menor distância euclidiana, 2. mova o vetor peso do BMU em direção ao vetor de entrada de acordo com a formula apropriada, 3. mova o vetor de peso dos neurônios vizinhos em direção ao vetor de entrada, 4. repetir até que todosos vetores de entrada tenham sido utilizados, reptia tudo até que o mapa seja totalmente coberto, 6. encontre o BMU final para cada uma das entradas de acordo com a distância euclidiana, ou seja, o neurônio a qual o vetor pertence. FÓRMULAS NESTE ARTIGOS DE COMO OS CÁLCULOS SÃO FEITOS.

\cite{wendel2010formalizing} segundo a pesquisa, não foi encontrado nenhuma fonte única de regras e boas práticas na criação de SOMs efetivas, então este trabalho tenta estabelecer \textit{guidelines} para a criação de SOMs através de teste empíricos, variando os parâmetros de inicialização, os níveis de treinamento, o tamanho da rede e dos nós, comparando saída, medidas de incerteza e interpretações, tudo isso em conjunto com \''boas práticas\'' da área. As guidelines foram agrupadas em 6 categorias: inicialização (linear ou aleatória, sem necessidade de normalização de dados binários, porém Skupin 2008 recomenda a inicialização aleatória para preservar o processo como completamente auto-governado), tamanho do mapa (as recomendações iniciais se referem ao propósito da construção e objetivos do mapa a ser gerado seungo Ultsch e Simon 1990, Vesanto 2005 recomenda uma fórmula de tamanho ótimo RETIRAR FÓRMULA DE TAMANHO. Buscar um tamanho ótimo evita que a computação do mapa fique muito pesada por lidar com muitos espaços vazios, mas um pouco precisa ser preservado a fim de facilitar na interpretação dos clusteres), forma do mapa (as recomendações por Kohonen sugeram uma geometria assimétrica a fim de evitar efeito de bordas, sendo que o lado mais curto deveria ser no máximo metade do lado mais comprido, isso é necessário pois clusteres localizados no centro do mapa são mais fáceis de interpretar como separados de grupos de clusteres que se encontram em bordas), geometria e tamanho da vizinhança (esse fator influencia na hora do recalculo dos pesos pois é necessário levar em conta e atualizar os pesos dos vizinhos do dado a ser conferido na interação, para estabelecer o melhor tamanho a primeira interação o tamanho da vizinhança abragia o mapa todo e a medida que as interações iam passando o tamanho foi sendo reduzido pela metade), duração de treinamento e ajuste de matrix (training lenght = 10000 e matrix tunning rate = 2000), qualificação da incerteza (utiliza o erro de quantização e U-Matix, erro de quantização é dado por uma fórmula que retorna valores entre 0 e 1 de acordo com o quanto o mapa se adequa aos dados, um retorno de 0 pode indicar overfitting e essa medida permite a comparação quantitativa entre mapas diferentes, já a U-Matrix indica qualitativamente e a clareza com que os clusteres são divididos, contém 2 dobro de espaços da SOM e é definida pela medida de similaridade entre uma célula e suas vizinhas, bordas entre clusteres muito forte podem indicar overfitting dos dados também. ISSO É UM RESUMO DE UMA PALESTRA DE CONFERÊNCIA, VERIFICAR A VALIDAD DE USAR ESSAS INFORMAÇÕES.

\cite{ultsch1990kohonen} desde sua introdução em 1982, SOMs vem sendo usados em diversas áreas por sua característica de a estrutura de dados em altas dimensões, porém a aplicação exclusiva dos mapas auto-organizáveis não necessariamente indica alguma informação a respeito de clusteres e análises de dados devem ser conduzidos sobre os mesmos para que sejam tiradas conclusões a respeito de clusterização. Explicação de como o algoritmo de Kohonen para SOM funciona e como U-Matrix funcionam em detalhes e fórmulas. 

\section{PRÉ-PROCESSAMENTO}
\label{sec:metodologiaPreProc}
Tudo que foi aplicado na base de dados.

\section{MINERAÇÃO DE DADOS}
\label{sec:metodologiaMine}
Tudo que foi aplicado para mineração, svm e regressão.

\section{PÓS-PROCESSAMENTO}
\label{sec:metodologiaPosProc}
Tudo que foi feito após o proc de datamining.